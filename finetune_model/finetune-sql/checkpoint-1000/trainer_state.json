{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.17343797424446084,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0017343797424446083,
      "grad_norm": 2.5550849437713623,
      "learning_rate": 0.00019968777103209019,
      "loss": 3.0523,
      "step": 10
    },
    {
      "epoch": 0.0034687594848892165,
      "grad_norm": 2.0123374462127686,
      "learning_rate": 0.00019934084995663486,
      "loss": 2.3094,
      "step": 20
    },
    {
      "epoch": 0.005203139227333824,
      "grad_norm": 2.694957733154297,
      "learning_rate": 0.00019899392888117953,
      "loss": 1.7411,
      "step": 30
    },
    {
      "epoch": 0.006937518969778433,
      "grad_norm": 1.9005944728851318,
      "learning_rate": 0.0001986470078057242,
      "loss": 1.1989,
      "step": 40
    },
    {
      "epoch": 0.008671898712223042,
      "grad_norm": 1.4874471426010132,
      "learning_rate": 0.00019830008673026888,
      "loss": 1.0164,
      "step": 50
    },
    {
      "epoch": 0.010406278454667649,
      "grad_norm": 2.6214494705200195,
      "learning_rate": 0.00019795316565481355,
      "loss": 0.9067,
      "step": 60
    },
    {
      "epoch": 0.012140658197112257,
      "grad_norm": 1.266445279121399,
      "learning_rate": 0.00019760624457935822,
      "loss": 0.9224,
      "step": 70
    },
    {
      "epoch": 0.013875037939556866,
      "grad_norm": 1.247423529624939,
      "learning_rate": 0.0001972593235039029,
      "loss": 0.8334,
      "step": 80
    },
    {
      "epoch": 0.015609417682001475,
      "grad_norm": 1.0444533824920654,
      "learning_rate": 0.00019691240242844754,
      "loss": 0.8003,
      "step": 90
    },
    {
      "epoch": 0.017343797424446084,
      "grad_norm": 1.0636088848114014,
      "learning_rate": 0.0001965654813529922,
      "loss": 0.8069,
      "step": 100
    },
    {
      "epoch": 0.01907817716689069,
      "grad_norm": 1.1975089311599731,
      "learning_rate": 0.00019621856027753686,
      "loss": 0.8468,
      "step": 110
    },
    {
      "epoch": 0.020812556909335297,
      "grad_norm": 1.125996470451355,
      "learning_rate": 0.00019587163920208153,
      "loss": 0.7739,
      "step": 120
    },
    {
      "epoch": 0.022546936651779908,
      "grad_norm": 1.0888392925262451,
      "learning_rate": 0.0001955247181266262,
      "loss": 0.7945,
      "step": 130
    },
    {
      "epoch": 0.024281316394224515,
      "grad_norm": 1.4818414449691772,
      "learning_rate": 0.00019517779705117088,
      "loss": 0.7209,
      "step": 140
    },
    {
      "epoch": 0.026015696136669125,
      "grad_norm": 1.7991796731948853,
      "learning_rate": 0.00019483087597571555,
      "loss": 0.74,
      "step": 150
    },
    {
      "epoch": 0.027750075879113732,
      "grad_norm": 1.8334739208221436,
      "learning_rate": 0.0001944839549002602,
      "loss": 0.7401,
      "step": 160
    },
    {
      "epoch": 0.02948445562155834,
      "grad_norm": 1.1871280670166016,
      "learning_rate": 0.00019413703382480487,
      "loss": 0.6621,
      "step": 170
    },
    {
      "epoch": 0.03121883536400295,
      "grad_norm": 1.0494414567947388,
      "learning_rate": 0.00019379011274934952,
      "loss": 0.5911,
      "step": 180
    },
    {
      "epoch": 0.03295321510644756,
      "grad_norm": 0.973773181438446,
      "learning_rate": 0.0001934431916738942,
      "loss": 0.6231,
      "step": 190
    },
    {
      "epoch": 0.03468759484889217,
      "grad_norm": 1.3078233003616333,
      "learning_rate": 0.00019309627059843886,
      "loss": 0.5732,
      "step": 200
    },
    {
      "epoch": 0.036421974591336774,
      "grad_norm": 1.2161680459976196,
      "learning_rate": 0.00019274934952298353,
      "loss": 0.6369,
      "step": 210
    },
    {
      "epoch": 0.03815635433378138,
      "grad_norm": 1.6396287679672241,
      "learning_rate": 0.0001924024284475282,
      "loss": 0.625,
      "step": 220
    },
    {
      "epoch": 0.03989073407622599,
      "grad_norm": 1.4116144180297852,
      "learning_rate": 0.00019205550737207288,
      "loss": 0.647,
      "step": 230
    },
    {
      "epoch": 0.041625113818670595,
      "grad_norm": 1.0980229377746582,
      "learning_rate": 0.00019170858629661753,
      "loss": 0.5614,
      "step": 240
    },
    {
      "epoch": 0.04335949356111521,
      "grad_norm": 1.1962525844573975,
      "learning_rate": 0.0001913616652211622,
      "loss": 0.6344,
      "step": 250
    },
    {
      "epoch": 0.045093873303559816,
      "grad_norm": 1.2604451179504395,
      "learning_rate": 0.00019101474414570687,
      "loss": 0.5919,
      "step": 260
    },
    {
      "epoch": 0.04682825304600442,
      "grad_norm": 0.9544632434844971,
      "learning_rate": 0.00019066782307025152,
      "loss": 0.5926,
      "step": 270
    },
    {
      "epoch": 0.04856263278844903,
      "grad_norm": 1.094160795211792,
      "learning_rate": 0.0001903209019947962,
      "loss": 0.5788,
      "step": 280
    },
    {
      "epoch": 0.05029701253089364,
      "grad_norm": 1.0095820426940918,
      "learning_rate": 0.00018997398091934086,
      "loss": 0.6399,
      "step": 290
    },
    {
      "epoch": 0.05203139227333825,
      "grad_norm": 1.3148770332336426,
      "learning_rate": 0.00018962705984388554,
      "loss": 0.5493,
      "step": 300
    },
    {
      "epoch": 0.05376577201578286,
      "grad_norm": 1.2883634567260742,
      "learning_rate": 0.00018928013876843018,
      "loss": 0.5717,
      "step": 310
    },
    {
      "epoch": 0.055500151758227464,
      "grad_norm": 0.8932894468307495,
      "learning_rate": 0.00018893321769297485,
      "loss": 0.5656,
      "step": 320
    },
    {
      "epoch": 0.05723453150067207,
      "grad_norm": 1.0753910541534424,
      "learning_rate": 0.00018858629661751953,
      "loss": 0.594,
      "step": 330
    },
    {
      "epoch": 0.05896891124311668,
      "grad_norm": 0.753275454044342,
      "learning_rate": 0.0001882393755420642,
      "loss": 0.5478,
      "step": 340
    },
    {
      "epoch": 0.060703290985561285,
      "grad_norm": 1.0140023231506348,
      "learning_rate": 0.00018789245446660885,
      "loss": 0.5268,
      "step": 350
    },
    {
      "epoch": 0.0624376707280059,
      "grad_norm": 0.818980872631073,
      "learning_rate": 0.00018754553339115352,
      "loss": 0.566,
      "step": 360
    },
    {
      "epoch": 0.0641720504704505,
      "grad_norm": 1.1010839939117432,
      "learning_rate": 0.0001871986123156982,
      "loss": 0.6807,
      "step": 370
    },
    {
      "epoch": 0.06590643021289512,
      "grad_norm": 1.2425607442855835,
      "learning_rate": 0.00018685169124024284,
      "loss": 0.6306,
      "step": 380
    },
    {
      "epoch": 0.06764080995533972,
      "grad_norm": 1.155747413635254,
      "learning_rate": 0.0001865047701647875,
      "loss": 0.5633,
      "step": 390
    },
    {
      "epoch": 0.06937518969778433,
      "grad_norm": 1.2084158658981323,
      "learning_rate": 0.00018615784908933218,
      "loss": 0.5758,
      "step": 400
    },
    {
      "epoch": 0.07110956944022893,
      "grad_norm": 0.8114025592803955,
      "learning_rate": 0.00018581092801387686,
      "loss": 0.532,
      "step": 410
    },
    {
      "epoch": 0.07284394918267355,
      "grad_norm": 0.8228296637535095,
      "learning_rate": 0.00018546400693842153,
      "loss": 0.5447,
      "step": 420
    },
    {
      "epoch": 0.07457832892511815,
      "grad_norm": 0.9397292733192444,
      "learning_rate": 0.0001851170858629662,
      "loss": 0.5517,
      "step": 430
    },
    {
      "epoch": 0.07631270866756276,
      "grad_norm": 0.8269107937812805,
      "learning_rate": 0.00018477016478751085,
      "loss": 0.5935,
      "step": 440
    },
    {
      "epoch": 0.07804708841000738,
      "grad_norm": 0.958692729473114,
      "learning_rate": 0.00018442324371205552,
      "loss": 0.5456,
      "step": 450
    },
    {
      "epoch": 0.07978146815245198,
      "grad_norm": 0.9679394960403442,
      "learning_rate": 0.00018407632263660017,
      "loss": 0.5506,
      "step": 460
    },
    {
      "epoch": 0.08151584789489659,
      "grad_norm": 0.8142249584197998,
      "learning_rate": 0.00018372940156114484,
      "loss": 0.5826,
      "step": 470
    },
    {
      "epoch": 0.08325022763734119,
      "grad_norm": 1.1221299171447754,
      "learning_rate": 0.0001833824804856895,
      "loss": 0.599,
      "step": 480
    },
    {
      "epoch": 0.0849846073797858,
      "grad_norm": 0.8575493693351746,
      "learning_rate": 0.00018303555941023418,
      "loss": 0.6185,
      "step": 490
    },
    {
      "epoch": 0.08671898712223042,
      "grad_norm": 1.043362021446228,
      "learning_rate": 0.00018268863833477886,
      "loss": 0.5496,
      "step": 500
    },
    {
      "epoch": 0.08845336686467502,
      "grad_norm": 0.776316225528717,
      "learning_rate": 0.00018234171725932353,
      "loss": 0.5821,
      "step": 510
    },
    {
      "epoch": 0.09018774660711963,
      "grad_norm": 0.7581605315208435,
      "learning_rate": 0.00018199479618386818,
      "loss": 0.4986,
      "step": 520
    },
    {
      "epoch": 0.09192212634956423,
      "grad_norm": 1.2291282415390015,
      "learning_rate": 0.00018164787510841282,
      "loss": 0.5699,
      "step": 530
    },
    {
      "epoch": 0.09365650609200885,
      "grad_norm": 0.8632634878158569,
      "learning_rate": 0.0001813009540329575,
      "loss": 0.5643,
      "step": 540
    },
    {
      "epoch": 0.09539088583445346,
      "grad_norm": 1.301264762878418,
      "learning_rate": 0.00018095403295750217,
      "loss": 0.5133,
      "step": 550
    },
    {
      "epoch": 0.09712526557689806,
      "grad_norm": 1.0773627758026123,
      "learning_rate": 0.00018060711188204684,
      "loss": 0.5576,
      "step": 560
    },
    {
      "epoch": 0.09885964531934267,
      "grad_norm": 1.1195361614227295,
      "learning_rate": 0.0001802601908065915,
      "loss": 0.512,
      "step": 570
    },
    {
      "epoch": 0.10059402506178727,
      "grad_norm": 1.0149531364440918,
      "learning_rate": 0.00017991326973113619,
      "loss": 0.5615,
      "step": 580
    },
    {
      "epoch": 0.10232840480423189,
      "grad_norm": 0.7162327170372009,
      "learning_rate": 0.00017956634865568086,
      "loss": 0.5361,
      "step": 590
    },
    {
      "epoch": 0.1040627845466765,
      "grad_norm": 0.9201861023902893,
      "learning_rate": 0.0001792194275802255,
      "loss": 0.5411,
      "step": 600
    },
    {
      "epoch": 0.1057971642891211,
      "grad_norm": 0.9617904424667358,
      "learning_rate": 0.00017887250650477018,
      "loss": 0.5089,
      "step": 610
    },
    {
      "epoch": 0.10753154403156572,
      "grad_norm": 1.022149682044983,
      "learning_rate": 0.00017852558542931482,
      "loss": 0.6215,
      "step": 620
    },
    {
      "epoch": 0.10926592377401032,
      "grad_norm": 0.8238493204116821,
      "learning_rate": 0.0001781786643538595,
      "loss": 0.5451,
      "step": 630
    },
    {
      "epoch": 0.11100030351645493,
      "grad_norm": 0.8939248919487,
      "learning_rate": 0.00017783174327840417,
      "loss": 0.5524,
      "step": 640
    },
    {
      "epoch": 0.11273468325889954,
      "grad_norm": 0.8883143663406372,
      "learning_rate": 0.00017748482220294884,
      "loss": 0.5246,
      "step": 650
    },
    {
      "epoch": 0.11446906300134414,
      "grad_norm": 0.7424055933952332,
      "learning_rate": 0.00017713790112749352,
      "loss": 0.5263,
      "step": 660
    },
    {
      "epoch": 0.11620344274378876,
      "grad_norm": 1.2159603834152222,
      "learning_rate": 0.0001767909800520382,
      "loss": 0.5079,
      "step": 670
    },
    {
      "epoch": 0.11793782248623336,
      "grad_norm": 0.7574623227119446,
      "learning_rate": 0.00017644405897658283,
      "loss": 0.5281,
      "step": 680
    },
    {
      "epoch": 0.11967220222867797,
      "grad_norm": 0.792121946811676,
      "learning_rate": 0.0001760971379011275,
      "loss": 0.5008,
      "step": 690
    },
    {
      "epoch": 0.12140658197112257,
      "grad_norm": 0.8064789175987244,
      "learning_rate": 0.00017575021682567215,
      "loss": 0.5316,
      "step": 700
    },
    {
      "epoch": 0.12314096171356718,
      "grad_norm": 0.8532897233963013,
      "learning_rate": 0.00017540329575021683,
      "loss": 0.5311,
      "step": 710
    },
    {
      "epoch": 0.1248753414560118,
      "grad_norm": 0.7916345596313477,
      "learning_rate": 0.0001750563746747615,
      "loss": 0.5307,
      "step": 720
    },
    {
      "epoch": 0.1266097211984564,
      "grad_norm": 0.9886270761489868,
      "learning_rate": 0.00017470945359930617,
      "loss": 0.5523,
      "step": 730
    },
    {
      "epoch": 0.128344100940901,
      "grad_norm": 0.9201990365982056,
      "learning_rate": 0.00017436253252385084,
      "loss": 0.509,
      "step": 740
    },
    {
      "epoch": 0.13007848068334563,
      "grad_norm": 0.7614508271217346,
      "learning_rate": 0.0001740156114483955,
      "loss": 0.6164,
      "step": 750
    },
    {
      "epoch": 0.13181286042579024,
      "grad_norm": 0.7819229960441589,
      "learning_rate": 0.00017366869037294016,
      "loss": 0.529,
      "step": 760
    },
    {
      "epoch": 0.13354724016823483,
      "grad_norm": 0.9719189405441284,
      "learning_rate": 0.00017332176929748484,
      "loss": 0.5313,
      "step": 770
    },
    {
      "epoch": 0.13528161991067944,
      "grad_norm": 0.6589162349700928,
      "learning_rate": 0.0001729748482220295,
      "loss": 0.504,
      "step": 780
    },
    {
      "epoch": 0.13701599965312405,
      "grad_norm": 0.6838045120239258,
      "learning_rate": 0.00017262792714657415,
      "loss": 0.5136,
      "step": 790
    },
    {
      "epoch": 0.13875037939556867,
      "grad_norm": 0.7332388758659363,
      "learning_rate": 0.00017228100607111883,
      "loss": 0.5613,
      "step": 800
    },
    {
      "epoch": 0.14048475913801325,
      "grad_norm": 0.8705220818519592,
      "learning_rate": 0.0001719340849956635,
      "loss": 0.5142,
      "step": 810
    },
    {
      "epoch": 0.14221913888045787,
      "grad_norm": 1.0035444498062134,
      "learning_rate": 0.00017158716392020817,
      "loss": 0.5488,
      "step": 820
    },
    {
      "epoch": 0.14395351862290248,
      "grad_norm": 0.7737194299697876,
      "learning_rate": 0.00017124024284475282,
      "loss": 0.5355,
      "step": 830
    },
    {
      "epoch": 0.1456878983653471,
      "grad_norm": 0.7520883679389954,
      "learning_rate": 0.0001708933217692975,
      "loss": 0.573,
      "step": 840
    },
    {
      "epoch": 0.1474222781077917,
      "grad_norm": 0.8724210262298584,
      "learning_rate": 0.00017054640069384216,
      "loss": 0.5305,
      "step": 850
    },
    {
      "epoch": 0.1491566578502363,
      "grad_norm": 0.8224956393241882,
      "learning_rate": 0.00017019947961838684,
      "loss": 0.5538,
      "step": 860
    },
    {
      "epoch": 0.1508910375926809,
      "grad_norm": 0.8507670760154724,
      "learning_rate": 0.00016985255854293148,
      "loss": 0.5366,
      "step": 870
    },
    {
      "epoch": 0.15262541733512552,
      "grad_norm": 0.8243544697761536,
      "learning_rate": 0.00016950563746747616,
      "loss": 0.5069,
      "step": 880
    },
    {
      "epoch": 0.15435979707757014,
      "grad_norm": 0.71662437915802,
      "learning_rate": 0.00016915871639202083,
      "loss": 0.5319,
      "step": 890
    },
    {
      "epoch": 0.15609417682001475,
      "grad_norm": 0.7661607265472412,
      "learning_rate": 0.00016881179531656547,
      "loss": 0.5335,
      "step": 900
    },
    {
      "epoch": 0.15782855656245934,
      "grad_norm": 0.6027960181236267,
      "learning_rate": 0.00016846487424111015,
      "loss": 0.5156,
      "step": 910
    },
    {
      "epoch": 0.15956293630490395,
      "grad_norm": 0.9688888192176819,
      "learning_rate": 0.00016811795316565482,
      "loss": 0.5643,
      "step": 920
    },
    {
      "epoch": 0.16129731604734857,
      "grad_norm": 0.8623057007789612,
      "learning_rate": 0.0001677710320901995,
      "loss": 0.4927,
      "step": 930
    },
    {
      "epoch": 0.16303169578979318,
      "grad_norm": 0.8581346273422241,
      "learning_rate": 0.00016742411101474417,
      "loss": 0.5247,
      "step": 940
    },
    {
      "epoch": 0.1647660755322378,
      "grad_norm": 0.7837463021278381,
      "learning_rate": 0.00016707718993928884,
      "loss": 0.5767,
      "step": 950
    },
    {
      "epoch": 0.16650045527468238,
      "grad_norm": 0.7025507092475891,
      "learning_rate": 0.00016673026886383348,
      "loss": 0.5687,
      "step": 960
    },
    {
      "epoch": 0.168234835017127,
      "grad_norm": 0.6543734669685364,
      "learning_rate": 0.00016638334778837813,
      "loss": 0.5689,
      "step": 970
    },
    {
      "epoch": 0.1699692147595716,
      "grad_norm": 1.0180604457855225,
      "learning_rate": 0.0001660364267129228,
      "loss": 0.564,
      "step": 980
    },
    {
      "epoch": 0.17170359450201622,
      "grad_norm": 0.9559260606765747,
      "learning_rate": 0.00016568950563746748,
      "loss": 0.5924,
      "step": 990
    },
    {
      "epoch": 0.17343797424446084,
      "grad_norm": 0.6762255430221558,
      "learning_rate": 0.00016534258456201215,
      "loss": 0.5048,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 5765,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8057426804736000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
