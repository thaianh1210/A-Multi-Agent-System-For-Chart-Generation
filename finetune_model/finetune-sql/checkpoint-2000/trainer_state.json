{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.34687594848892167,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0017343797424446083,
      "grad_norm": 2.5550849437713623,
      "learning_rate": 0.00019968777103209019,
      "loss": 3.0523,
      "step": 10
    },
    {
      "epoch": 0.0034687594848892165,
      "grad_norm": 2.0123374462127686,
      "learning_rate": 0.00019934084995663486,
      "loss": 2.3094,
      "step": 20
    },
    {
      "epoch": 0.005203139227333824,
      "grad_norm": 2.694957733154297,
      "learning_rate": 0.00019899392888117953,
      "loss": 1.7411,
      "step": 30
    },
    {
      "epoch": 0.006937518969778433,
      "grad_norm": 1.9005944728851318,
      "learning_rate": 0.0001986470078057242,
      "loss": 1.1989,
      "step": 40
    },
    {
      "epoch": 0.008671898712223042,
      "grad_norm": 1.4874471426010132,
      "learning_rate": 0.00019830008673026888,
      "loss": 1.0164,
      "step": 50
    },
    {
      "epoch": 0.010406278454667649,
      "grad_norm": 2.6214494705200195,
      "learning_rate": 0.00019795316565481355,
      "loss": 0.9067,
      "step": 60
    },
    {
      "epoch": 0.012140658197112257,
      "grad_norm": 1.266445279121399,
      "learning_rate": 0.00019760624457935822,
      "loss": 0.9224,
      "step": 70
    },
    {
      "epoch": 0.013875037939556866,
      "grad_norm": 1.247423529624939,
      "learning_rate": 0.0001972593235039029,
      "loss": 0.8334,
      "step": 80
    },
    {
      "epoch": 0.015609417682001475,
      "grad_norm": 1.0444533824920654,
      "learning_rate": 0.00019691240242844754,
      "loss": 0.8003,
      "step": 90
    },
    {
      "epoch": 0.017343797424446084,
      "grad_norm": 1.0636088848114014,
      "learning_rate": 0.0001965654813529922,
      "loss": 0.8069,
      "step": 100
    },
    {
      "epoch": 0.01907817716689069,
      "grad_norm": 1.1975089311599731,
      "learning_rate": 0.00019621856027753686,
      "loss": 0.8468,
      "step": 110
    },
    {
      "epoch": 0.020812556909335297,
      "grad_norm": 1.125996470451355,
      "learning_rate": 0.00019587163920208153,
      "loss": 0.7739,
      "step": 120
    },
    {
      "epoch": 0.022546936651779908,
      "grad_norm": 1.0888392925262451,
      "learning_rate": 0.0001955247181266262,
      "loss": 0.7945,
      "step": 130
    },
    {
      "epoch": 0.024281316394224515,
      "grad_norm": 1.4818414449691772,
      "learning_rate": 0.00019517779705117088,
      "loss": 0.7209,
      "step": 140
    },
    {
      "epoch": 0.026015696136669125,
      "grad_norm": 1.7991796731948853,
      "learning_rate": 0.00019483087597571555,
      "loss": 0.74,
      "step": 150
    },
    {
      "epoch": 0.027750075879113732,
      "grad_norm": 1.8334739208221436,
      "learning_rate": 0.0001944839549002602,
      "loss": 0.7401,
      "step": 160
    },
    {
      "epoch": 0.02948445562155834,
      "grad_norm": 1.1871280670166016,
      "learning_rate": 0.00019413703382480487,
      "loss": 0.6621,
      "step": 170
    },
    {
      "epoch": 0.03121883536400295,
      "grad_norm": 1.0494414567947388,
      "learning_rate": 0.00019379011274934952,
      "loss": 0.5911,
      "step": 180
    },
    {
      "epoch": 0.03295321510644756,
      "grad_norm": 0.973773181438446,
      "learning_rate": 0.0001934431916738942,
      "loss": 0.6231,
      "step": 190
    },
    {
      "epoch": 0.03468759484889217,
      "grad_norm": 1.3078233003616333,
      "learning_rate": 0.00019309627059843886,
      "loss": 0.5732,
      "step": 200
    },
    {
      "epoch": 0.036421974591336774,
      "grad_norm": 1.2161680459976196,
      "learning_rate": 0.00019274934952298353,
      "loss": 0.6369,
      "step": 210
    },
    {
      "epoch": 0.03815635433378138,
      "grad_norm": 1.6396287679672241,
      "learning_rate": 0.0001924024284475282,
      "loss": 0.625,
      "step": 220
    },
    {
      "epoch": 0.03989073407622599,
      "grad_norm": 1.4116144180297852,
      "learning_rate": 0.00019205550737207288,
      "loss": 0.647,
      "step": 230
    },
    {
      "epoch": 0.041625113818670595,
      "grad_norm": 1.0980229377746582,
      "learning_rate": 0.00019170858629661753,
      "loss": 0.5614,
      "step": 240
    },
    {
      "epoch": 0.04335949356111521,
      "grad_norm": 1.1962525844573975,
      "learning_rate": 0.0001913616652211622,
      "loss": 0.6344,
      "step": 250
    },
    {
      "epoch": 0.045093873303559816,
      "grad_norm": 1.2604451179504395,
      "learning_rate": 0.00019101474414570687,
      "loss": 0.5919,
      "step": 260
    },
    {
      "epoch": 0.04682825304600442,
      "grad_norm": 0.9544632434844971,
      "learning_rate": 0.00019066782307025152,
      "loss": 0.5926,
      "step": 270
    },
    {
      "epoch": 0.04856263278844903,
      "grad_norm": 1.094160795211792,
      "learning_rate": 0.0001903209019947962,
      "loss": 0.5788,
      "step": 280
    },
    {
      "epoch": 0.05029701253089364,
      "grad_norm": 1.0095820426940918,
      "learning_rate": 0.00018997398091934086,
      "loss": 0.6399,
      "step": 290
    },
    {
      "epoch": 0.05203139227333825,
      "grad_norm": 1.3148770332336426,
      "learning_rate": 0.00018962705984388554,
      "loss": 0.5493,
      "step": 300
    },
    {
      "epoch": 0.05376577201578286,
      "grad_norm": 1.2883634567260742,
      "learning_rate": 0.00018928013876843018,
      "loss": 0.5717,
      "step": 310
    },
    {
      "epoch": 0.055500151758227464,
      "grad_norm": 0.8932894468307495,
      "learning_rate": 0.00018893321769297485,
      "loss": 0.5656,
      "step": 320
    },
    {
      "epoch": 0.05723453150067207,
      "grad_norm": 1.0753910541534424,
      "learning_rate": 0.00018858629661751953,
      "loss": 0.594,
      "step": 330
    },
    {
      "epoch": 0.05896891124311668,
      "grad_norm": 0.753275454044342,
      "learning_rate": 0.0001882393755420642,
      "loss": 0.5478,
      "step": 340
    },
    {
      "epoch": 0.060703290985561285,
      "grad_norm": 1.0140023231506348,
      "learning_rate": 0.00018789245446660885,
      "loss": 0.5268,
      "step": 350
    },
    {
      "epoch": 0.0624376707280059,
      "grad_norm": 0.818980872631073,
      "learning_rate": 0.00018754553339115352,
      "loss": 0.566,
      "step": 360
    },
    {
      "epoch": 0.0641720504704505,
      "grad_norm": 1.1010839939117432,
      "learning_rate": 0.0001871986123156982,
      "loss": 0.6807,
      "step": 370
    },
    {
      "epoch": 0.06590643021289512,
      "grad_norm": 1.2425607442855835,
      "learning_rate": 0.00018685169124024284,
      "loss": 0.6306,
      "step": 380
    },
    {
      "epoch": 0.06764080995533972,
      "grad_norm": 1.155747413635254,
      "learning_rate": 0.0001865047701647875,
      "loss": 0.5633,
      "step": 390
    },
    {
      "epoch": 0.06937518969778433,
      "grad_norm": 1.2084158658981323,
      "learning_rate": 0.00018615784908933218,
      "loss": 0.5758,
      "step": 400
    },
    {
      "epoch": 0.07110956944022893,
      "grad_norm": 0.8114025592803955,
      "learning_rate": 0.00018581092801387686,
      "loss": 0.532,
      "step": 410
    },
    {
      "epoch": 0.07284394918267355,
      "grad_norm": 0.8228296637535095,
      "learning_rate": 0.00018546400693842153,
      "loss": 0.5447,
      "step": 420
    },
    {
      "epoch": 0.07457832892511815,
      "grad_norm": 0.9397292733192444,
      "learning_rate": 0.0001851170858629662,
      "loss": 0.5517,
      "step": 430
    },
    {
      "epoch": 0.07631270866756276,
      "grad_norm": 0.8269107937812805,
      "learning_rate": 0.00018477016478751085,
      "loss": 0.5935,
      "step": 440
    },
    {
      "epoch": 0.07804708841000738,
      "grad_norm": 0.958692729473114,
      "learning_rate": 0.00018442324371205552,
      "loss": 0.5456,
      "step": 450
    },
    {
      "epoch": 0.07978146815245198,
      "grad_norm": 0.9679394960403442,
      "learning_rate": 0.00018407632263660017,
      "loss": 0.5506,
      "step": 460
    },
    {
      "epoch": 0.08151584789489659,
      "grad_norm": 0.8142249584197998,
      "learning_rate": 0.00018372940156114484,
      "loss": 0.5826,
      "step": 470
    },
    {
      "epoch": 0.08325022763734119,
      "grad_norm": 1.1221299171447754,
      "learning_rate": 0.0001833824804856895,
      "loss": 0.599,
      "step": 480
    },
    {
      "epoch": 0.0849846073797858,
      "grad_norm": 0.8575493693351746,
      "learning_rate": 0.00018303555941023418,
      "loss": 0.6185,
      "step": 490
    },
    {
      "epoch": 0.08671898712223042,
      "grad_norm": 1.043362021446228,
      "learning_rate": 0.00018268863833477886,
      "loss": 0.5496,
      "step": 500
    },
    {
      "epoch": 0.08845336686467502,
      "grad_norm": 0.776316225528717,
      "learning_rate": 0.00018234171725932353,
      "loss": 0.5821,
      "step": 510
    },
    {
      "epoch": 0.09018774660711963,
      "grad_norm": 0.7581605315208435,
      "learning_rate": 0.00018199479618386818,
      "loss": 0.4986,
      "step": 520
    },
    {
      "epoch": 0.09192212634956423,
      "grad_norm": 1.2291282415390015,
      "learning_rate": 0.00018164787510841282,
      "loss": 0.5699,
      "step": 530
    },
    {
      "epoch": 0.09365650609200885,
      "grad_norm": 0.8632634878158569,
      "learning_rate": 0.0001813009540329575,
      "loss": 0.5643,
      "step": 540
    },
    {
      "epoch": 0.09539088583445346,
      "grad_norm": 1.301264762878418,
      "learning_rate": 0.00018095403295750217,
      "loss": 0.5133,
      "step": 550
    },
    {
      "epoch": 0.09712526557689806,
      "grad_norm": 1.0773627758026123,
      "learning_rate": 0.00018060711188204684,
      "loss": 0.5576,
      "step": 560
    },
    {
      "epoch": 0.09885964531934267,
      "grad_norm": 1.1195361614227295,
      "learning_rate": 0.0001802601908065915,
      "loss": 0.512,
      "step": 570
    },
    {
      "epoch": 0.10059402506178727,
      "grad_norm": 1.0149531364440918,
      "learning_rate": 0.00017991326973113619,
      "loss": 0.5615,
      "step": 580
    },
    {
      "epoch": 0.10232840480423189,
      "grad_norm": 0.7162327170372009,
      "learning_rate": 0.00017956634865568086,
      "loss": 0.5361,
      "step": 590
    },
    {
      "epoch": 0.1040627845466765,
      "grad_norm": 0.9201861023902893,
      "learning_rate": 0.0001792194275802255,
      "loss": 0.5411,
      "step": 600
    },
    {
      "epoch": 0.1057971642891211,
      "grad_norm": 0.9617904424667358,
      "learning_rate": 0.00017887250650477018,
      "loss": 0.5089,
      "step": 610
    },
    {
      "epoch": 0.10753154403156572,
      "grad_norm": 1.022149682044983,
      "learning_rate": 0.00017852558542931482,
      "loss": 0.6215,
      "step": 620
    },
    {
      "epoch": 0.10926592377401032,
      "grad_norm": 0.8238493204116821,
      "learning_rate": 0.0001781786643538595,
      "loss": 0.5451,
      "step": 630
    },
    {
      "epoch": 0.11100030351645493,
      "grad_norm": 0.8939248919487,
      "learning_rate": 0.00017783174327840417,
      "loss": 0.5524,
      "step": 640
    },
    {
      "epoch": 0.11273468325889954,
      "grad_norm": 0.8883143663406372,
      "learning_rate": 0.00017748482220294884,
      "loss": 0.5246,
      "step": 650
    },
    {
      "epoch": 0.11446906300134414,
      "grad_norm": 0.7424055933952332,
      "learning_rate": 0.00017713790112749352,
      "loss": 0.5263,
      "step": 660
    },
    {
      "epoch": 0.11620344274378876,
      "grad_norm": 1.2159603834152222,
      "learning_rate": 0.0001767909800520382,
      "loss": 0.5079,
      "step": 670
    },
    {
      "epoch": 0.11793782248623336,
      "grad_norm": 0.7574623227119446,
      "learning_rate": 0.00017644405897658283,
      "loss": 0.5281,
      "step": 680
    },
    {
      "epoch": 0.11967220222867797,
      "grad_norm": 0.792121946811676,
      "learning_rate": 0.0001760971379011275,
      "loss": 0.5008,
      "step": 690
    },
    {
      "epoch": 0.12140658197112257,
      "grad_norm": 0.8064789175987244,
      "learning_rate": 0.00017575021682567215,
      "loss": 0.5316,
      "step": 700
    },
    {
      "epoch": 0.12314096171356718,
      "grad_norm": 0.8532897233963013,
      "learning_rate": 0.00017540329575021683,
      "loss": 0.5311,
      "step": 710
    },
    {
      "epoch": 0.1248753414560118,
      "grad_norm": 0.7916345596313477,
      "learning_rate": 0.0001750563746747615,
      "loss": 0.5307,
      "step": 720
    },
    {
      "epoch": 0.1266097211984564,
      "grad_norm": 0.9886270761489868,
      "learning_rate": 0.00017470945359930617,
      "loss": 0.5523,
      "step": 730
    },
    {
      "epoch": 0.128344100940901,
      "grad_norm": 0.9201990365982056,
      "learning_rate": 0.00017436253252385084,
      "loss": 0.509,
      "step": 740
    },
    {
      "epoch": 0.13007848068334563,
      "grad_norm": 0.7614508271217346,
      "learning_rate": 0.0001740156114483955,
      "loss": 0.6164,
      "step": 750
    },
    {
      "epoch": 0.13181286042579024,
      "grad_norm": 0.7819229960441589,
      "learning_rate": 0.00017366869037294016,
      "loss": 0.529,
      "step": 760
    },
    {
      "epoch": 0.13354724016823483,
      "grad_norm": 0.9719189405441284,
      "learning_rate": 0.00017332176929748484,
      "loss": 0.5313,
      "step": 770
    },
    {
      "epoch": 0.13528161991067944,
      "grad_norm": 0.6589162349700928,
      "learning_rate": 0.0001729748482220295,
      "loss": 0.504,
      "step": 780
    },
    {
      "epoch": 0.13701599965312405,
      "grad_norm": 0.6838045120239258,
      "learning_rate": 0.00017262792714657415,
      "loss": 0.5136,
      "step": 790
    },
    {
      "epoch": 0.13875037939556867,
      "grad_norm": 0.7332388758659363,
      "learning_rate": 0.00017228100607111883,
      "loss": 0.5613,
      "step": 800
    },
    {
      "epoch": 0.14048475913801325,
      "grad_norm": 0.8705220818519592,
      "learning_rate": 0.0001719340849956635,
      "loss": 0.5142,
      "step": 810
    },
    {
      "epoch": 0.14221913888045787,
      "grad_norm": 1.0035444498062134,
      "learning_rate": 0.00017158716392020817,
      "loss": 0.5488,
      "step": 820
    },
    {
      "epoch": 0.14395351862290248,
      "grad_norm": 0.7737194299697876,
      "learning_rate": 0.00017124024284475282,
      "loss": 0.5355,
      "step": 830
    },
    {
      "epoch": 0.1456878983653471,
      "grad_norm": 0.7520883679389954,
      "learning_rate": 0.0001708933217692975,
      "loss": 0.573,
      "step": 840
    },
    {
      "epoch": 0.1474222781077917,
      "grad_norm": 0.8724210262298584,
      "learning_rate": 0.00017054640069384216,
      "loss": 0.5305,
      "step": 850
    },
    {
      "epoch": 0.1491566578502363,
      "grad_norm": 0.8224956393241882,
      "learning_rate": 0.00017019947961838684,
      "loss": 0.5538,
      "step": 860
    },
    {
      "epoch": 0.1508910375926809,
      "grad_norm": 0.8507670760154724,
      "learning_rate": 0.00016985255854293148,
      "loss": 0.5366,
      "step": 870
    },
    {
      "epoch": 0.15262541733512552,
      "grad_norm": 0.8243544697761536,
      "learning_rate": 0.00016950563746747616,
      "loss": 0.5069,
      "step": 880
    },
    {
      "epoch": 0.15435979707757014,
      "grad_norm": 0.71662437915802,
      "learning_rate": 0.00016915871639202083,
      "loss": 0.5319,
      "step": 890
    },
    {
      "epoch": 0.15609417682001475,
      "grad_norm": 0.7661607265472412,
      "learning_rate": 0.00016881179531656547,
      "loss": 0.5335,
      "step": 900
    },
    {
      "epoch": 0.15782855656245934,
      "grad_norm": 0.6027960181236267,
      "learning_rate": 0.00016846487424111015,
      "loss": 0.5156,
      "step": 910
    },
    {
      "epoch": 0.15956293630490395,
      "grad_norm": 0.9688888192176819,
      "learning_rate": 0.00016811795316565482,
      "loss": 0.5643,
      "step": 920
    },
    {
      "epoch": 0.16129731604734857,
      "grad_norm": 0.8623057007789612,
      "learning_rate": 0.0001677710320901995,
      "loss": 0.4927,
      "step": 930
    },
    {
      "epoch": 0.16303169578979318,
      "grad_norm": 0.8581346273422241,
      "learning_rate": 0.00016742411101474417,
      "loss": 0.5247,
      "step": 940
    },
    {
      "epoch": 0.1647660755322378,
      "grad_norm": 0.7837463021278381,
      "learning_rate": 0.00016707718993928884,
      "loss": 0.5767,
      "step": 950
    },
    {
      "epoch": 0.16650045527468238,
      "grad_norm": 0.7025507092475891,
      "learning_rate": 0.00016673026886383348,
      "loss": 0.5687,
      "step": 960
    },
    {
      "epoch": 0.168234835017127,
      "grad_norm": 0.6543734669685364,
      "learning_rate": 0.00016638334778837813,
      "loss": 0.5689,
      "step": 970
    },
    {
      "epoch": 0.1699692147595716,
      "grad_norm": 1.0180604457855225,
      "learning_rate": 0.0001660364267129228,
      "loss": 0.564,
      "step": 980
    },
    {
      "epoch": 0.17170359450201622,
      "grad_norm": 0.9559260606765747,
      "learning_rate": 0.00016568950563746748,
      "loss": 0.5924,
      "step": 990
    },
    {
      "epoch": 0.17343797424446084,
      "grad_norm": 0.6762255430221558,
      "learning_rate": 0.00016534258456201215,
      "loss": 0.5048,
      "step": 1000
    },
    {
      "epoch": 0.17517235398690542,
      "grad_norm": 0.8638314008712769,
      "learning_rate": 0.00016499566348655682,
      "loss": 0.5841,
      "step": 1010
    },
    {
      "epoch": 0.17690673372935004,
      "grad_norm": 0.9742166996002197,
      "learning_rate": 0.0001646487424111015,
      "loss": 0.5587,
      "step": 1020
    },
    {
      "epoch": 0.17864111347179465,
      "grad_norm": 0.9518930912017822,
      "learning_rate": 0.00016430182133564617,
      "loss": 0.5526,
      "step": 1030
    },
    {
      "epoch": 0.18037549321423926,
      "grad_norm": 1.1513153314590454,
      "learning_rate": 0.0001639549002601908,
      "loss": 0.5499,
      "step": 1040
    },
    {
      "epoch": 0.18210987295668388,
      "grad_norm": 0.8407158255577087,
      "learning_rate": 0.00016360797918473546,
      "loss": 0.5786,
      "step": 1050
    },
    {
      "epoch": 0.18384425269912846,
      "grad_norm": 0.8604046106338501,
      "learning_rate": 0.00016326105810928013,
      "loss": 0.5106,
      "step": 1060
    },
    {
      "epoch": 0.18557863244157308,
      "grad_norm": 0.6312768459320068,
      "learning_rate": 0.0001629141370338248,
      "loss": 0.5601,
      "step": 1070
    },
    {
      "epoch": 0.1873130121840177,
      "grad_norm": 0.8525477051734924,
      "learning_rate": 0.00016256721595836948,
      "loss": 0.5702,
      "step": 1080
    },
    {
      "epoch": 0.1890473919264623,
      "grad_norm": 0.8499556183815002,
      "learning_rate": 0.00016222029488291415,
      "loss": 0.5689,
      "step": 1090
    },
    {
      "epoch": 0.19078177166890692,
      "grad_norm": 0.9467098116874695,
      "learning_rate": 0.00016187337380745882,
      "loss": 0.5168,
      "step": 1100
    },
    {
      "epoch": 0.1925161514113515,
      "grad_norm": 0.8677939772605896,
      "learning_rate": 0.0001615264527320035,
      "loss": 0.5056,
      "step": 1110
    },
    {
      "epoch": 0.19425053115379612,
      "grad_norm": 0.8191261291503906,
      "learning_rate": 0.00016117953165654814,
      "loss": 0.5172,
      "step": 1120
    },
    {
      "epoch": 0.19598491089624073,
      "grad_norm": 0.9394680857658386,
      "learning_rate": 0.00016083261058109281,
      "loss": 0.5182,
      "step": 1130
    },
    {
      "epoch": 0.19771929063868535,
      "grad_norm": 0.9703214764595032,
      "learning_rate": 0.00016048568950563746,
      "loss": 0.4981,
      "step": 1140
    },
    {
      "epoch": 0.19945367038112996,
      "grad_norm": 0.7619735598564148,
      "learning_rate": 0.00016013876843018213,
      "loss": 0.5763,
      "step": 1150
    },
    {
      "epoch": 0.20118805012357455,
      "grad_norm": 1.0625637769699097,
      "learning_rate": 0.0001597918473547268,
      "loss": 0.5037,
      "step": 1160
    },
    {
      "epoch": 0.20292242986601916,
      "grad_norm": 0.7585030794143677,
      "learning_rate": 0.00015944492627927148,
      "loss": 0.5171,
      "step": 1170
    },
    {
      "epoch": 0.20465680960846377,
      "grad_norm": 0.7726263403892517,
      "learning_rate": 0.00015909800520381615,
      "loss": 0.54,
      "step": 1180
    },
    {
      "epoch": 0.2063911893509084,
      "grad_norm": 0.7178537249565125,
      "learning_rate": 0.00015875108412836082,
      "loss": 0.5195,
      "step": 1190
    },
    {
      "epoch": 0.208125569093353,
      "grad_norm": 0.9159913063049316,
      "learning_rate": 0.00015840416305290547,
      "loss": 0.5859,
      "step": 1200
    },
    {
      "epoch": 0.2098599488357976,
      "grad_norm": 0.8675339818000793,
      "learning_rate": 0.00015805724197745014,
      "loss": 0.5552,
      "step": 1210
    },
    {
      "epoch": 0.2115943285782422,
      "grad_norm": 0.8014780879020691,
      "learning_rate": 0.0001577103209019948,
      "loss": 0.4815,
      "step": 1220
    },
    {
      "epoch": 0.21332870832068682,
      "grad_norm": 0.8599198460578918,
      "learning_rate": 0.00015736339982653946,
      "loss": 0.5037,
      "step": 1230
    },
    {
      "epoch": 0.21506308806313143,
      "grad_norm": 0.775314211845398,
      "learning_rate": 0.00015701647875108413,
      "loss": 0.5641,
      "step": 1240
    },
    {
      "epoch": 0.21679746780557604,
      "grad_norm": 1.3312227725982666,
      "learning_rate": 0.0001566695576756288,
      "loss": 0.5015,
      "step": 1250
    },
    {
      "epoch": 0.21853184754802063,
      "grad_norm": 1.013002634048462,
      "learning_rate": 0.00015632263660017348,
      "loss": 0.5295,
      "step": 1260
    },
    {
      "epoch": 0.22026622729046524,
      "grad_norm": 0.618144154548645,
      "learning_rate": 0.00015597571552471813,
      "loss": 0.5206,
      "step": 1270
    },
    {
      "epoch": 0.22200060703290986,
      "grad_norm": 0.7005691528320312,
      "learning_rate": 0.0001556287944492628,
      "loss": 0.507,
      "step": 1280
    },
    {
      "epoch": 0.22373498677535447,
      "grad_norm": 0.9774540662765503,
      "learning_rate": 0.00015528187337380747,
      "loss": 0.5415,
      "step": 1290
    },
    {
      "epoch": 0.22546936651779909,
      "grad_norm": 0.8326419591903687,
      "learning_rate": 0.00015493495229835214,
      "loss": 0.5859,
      "step": 1300
    },
    {
      "epoch": 0.22720374626024367,
      "grad_norm": 0.7086894512176514,
      "learning_rate": 0.0001545880312228968,
      "loss": 0.4951,
      "step": 1310
    },
    {
      "epoch": 0.22893812600268829,
      "grad_norm": 0.8754698038101196,
      "learning_rate": 0.00015424111014744146,
      "loss": 0.5692,
      "step": 1320
    },
    {
      "epoch": 0.2306725057451329,
      "grad_norm": 0.7073501348495483,
      "learning_rate": 0.00015389418907198614,
      "loss": 0.5121,
      "step": 1330
    },
    {
      "epoch": 0.2324068854875775,
      "grad_norm": 0.7450106739997864,
      "learning_rate": 0.00015354726799653078,
      "loss": 0.5772,
      "step": 1340
    },
    {
      "epoch": 0.2341412652300221,
      "grad_norm": 1.057621955871582,
      "learning_rate": 0.00015320034692107545,
      "loss": 0.5487,
      "step": 1350
    },
    {
      "epoch": 0.2358756449724667,
      "grad_norm": 0.7074055671691895,
      "learning_rate": 0.00015285342584562013,
      "loss": 0.485,
      "step": 1360
    },
    {
      "epoch": 0.23761002471491133,
      "grad_norm": 0.7234088778495789,
      "learning_rate": 0.0001525065047701648,
      "loss": 0.5442,
      "step": 1370
    },
    {
      "epoch": 0.23934440445735594,
      "grad_norm": 0.8184717893600464,
      "learning_rate": 0.00015215958369470947,
      "loss": 0.5211,
      "step": 1380
    },
    {
      "epoch": 0.24107878419980056,
      "grad_norm": 0.8528016805648804,
      "learning_rate": 0.00015181266261925412,
      "loss": 0.5299,
      "step": 1390
    },
    {
      "epoch": 0.24281316394224514,
      "grad_norm": 0.9218398332595825,
      "learning_rate": 0.0001514657415437988,
      "loss": 0.5336,
      "step": 1400
    },
    {
      "epoch": 0.24454754368468976,
      "grad_norm": 0.9450495839118958,
      "learning_rate": 0.00015111882046834346,
      "loss": 0.4913,
      "step": 1410
    },
    {
      "epoch": 0.24628192342713437,
      "grad_norm": 0.8063280582427979,
      "learning_rate": 0.0001507718993928881,
      "loss": 0.4731,
      "step": 1420
    },
    {
      "epoch": 0.24801630316957898,
      "grad_norm": 0.7726646661758423,
      "learning_rate": 0.00015042497831743278,
      "loss": 0.582,
      "step": 1430
    },
    {
      "epoch": 0.2497506829120236,
      "grad_norm": 0.6898558735847473,
      "learning_rate": 0.00015007805724197746,
      "loss": 0.5459,
      "step": 1440
    },
    {
      "epoch": 0.2514850626544682,
      "grad_norm": 0.7302579879760742,
      "learning_rate": 0.00014973113616652213,
      "loss": 0.4509,
      "step": 1450
    },
    {
      "epoch": 0.2532194423969128,
      "grad_norm": 0.8079715967178345,
      "learning_rate": 0.0001493842150910668,
      "loss": 0.4924,
      "step": 1460
    },
    {
      "epoch": 0.2549538221393574,
      "grad_norm": 0.9184403419494629,
      "learning_rate": 0.00014903729401561148,
      "loss": 0.5242,
      "step": 1470
    },
    {
      "epoch": 0.256688201881802,
      "grad_norm": 0.9540342092514038,
      "learning_rate": 0.00014869037294015612,
      "loss": 0.5108,
      "step": 1480
    },
    {
      "epoch": 0.25842258162424664,
      "grad_norm": 0.662722647190094,
      "learning_rate": 0.00014834345186470077,
      "loss": 0.4707,
      "step": 1490
    },
    {
      "epoch": 0.26015696136669125,
      "grad_norm": 1.0262176990509033,
      "learning_rate": 0.00014799653078924544,
      "loss": 0.5443,
      "step": 1500
    },
    {
      "epoch": 0.26189134110913587,
      "grad_norm": 0.7653674483299255,
      "learning_rate": 0.0001476496097137901,
      "loss": 0.4961,
      "step": 1510
    },
    {
      "epoch": 0.2636257208515805,
      "grad_norm": 0.6642130017280579,
      "learning_rate": 0.00014730268863833479,
      "loss": 0.5682,
      "step": 1520
    },
    {
      "epoch": 0.26536010059402504,
      "grad_norm": 0.7625731825828552,
      "learning_rate": 0.00014695576756287946,
      "loss": 0.5445,
      "step": 1530
    },
    {
      "epoch": 0.26709448033646965,
      "grad_norm": 0.8662495613098145,
      "learning_rate": 0.00014660884648742413,
      "loss": 0.5393,
      "step": 1540
    },
    {
      "epoch": 0.26882886007891427,
      "grad_norm": 0.8010826706886292,
      "learning_rate": 0.0001462619254119688,
      "loss": 0.5276,
      "step": 1550
    },
    {
      "epoch": 0.2705632398213589,
      "grad_norm": 0.697375476360321,
      "learning_rate": 0.00014591500433651345,
      "loss": 0.514,
      "step": 1560
    },
    {
      "epoch": 0.2722976195638035,
      "grad_norm": 0.5982699990272522,
      "learning_rate": 0.0001455680832610581,
      "loss": 0.4839,
      "step": 1570
    },
    {
      "epoch": 0.2740319993062481,
      "grad_norm": 0.7269188165664673,
      "learning_rate": 0.00014522116218560277,
      "loss": 0.4315,
      "step": 1580
    },
    {
      "epoch": 0.2757663790486927,
      "grad_norm": 0.7482186555862427,
      "learning_rate": 0.00014487424111014744,
      "loss": 0.5392,
      "step": 1590
    },
    {
      "epoch": 0.27750075879113734,
      "grad_norm": 0.7533226609230042,
      "learning_rate": 0.00014452732003469211,
      "loss": 0.5058,
      "step": 1600
    },
    {
      "epoch": 0.27923513853358195,
      "grad_norm": 0.7971985340118408,
      "learning_rate": 0.0001441803989592368,
      "loss": 0.5496,
      "step": 1610
    },
    {
      "epoch": 0.2809695182760265,
      "grad_norm": 0.9086535573005676,
      "learning_rate": 0.00014383347788378146,
      "loss": 0.5231,
      "step": 1620
    },
    {
      "epoch": 0.2827038980184711,
      "grad_norm": 0.7624841332435608,
      "learning_rate": 0.00014348655680832613,
      "loss": 0.4796,
      "step": 1630
    },
    {
      "epoch": 0.28443827776091574,
      "grad_norm": 0.7093886137008667,
      "learning_rate": 0.00014313963573287078,
      "loss": 0.4985,
      "step": 1640
    },
    {
      "epoch": 0.28617265750336035,
      "grad_norm": 0.6750412583351135,
      "learning_rate": 0.00014279271465741545,
      "loss": 0.5532,
      "step": 1650
    },
    {
      "epoch": 0.28790703724580496,
      "grad_norm": 0.6476730704307556,
      "learning_rate": 0.0001424457935819601,
      "loss": 0.5441,
      "step": 1660
    },
    {
      "epoch": 0.2896414169882496,
      "grad_norm": 0.6679351925849915,
      "learning_rate": 0.00014209887250650477,
      "loss": 0.5186,
      "step": 1670
    },
    {
      "epoch": 0.2913757967306942,
      "grad_norm": 0.691450297832489,
      "learning_rate": 0.00014175195143104944,
      "loss": 0.4914,
      "step": 1680
    },
    {
      "epoch": 0.2931101764731388,
      "grad_norm": 0.7088834047317505,
      "learning_rate": 0.00014140503035559412,
      "loss": 0.4836,
      "step": 1690
    },
    {
      "epoch": 0.2948445562155834,
      "grad_norm": 0.7763418555259705,
      "learning_rate": 0.0001410581092801388,
      "loss": 0.5297,
      "step": 1700
    },
    {
      "epoch": 0.29657893595802803,
      "grad_norm": 0.9440441727638245,
      "learning_rate": 0.00014071118820468343,
      "loss": 0.5052,
      "step": 1710
    },
    {
      "epoch": 0.2983133157004726,
      "grad_norm": 0.9379427433013916,
      "learning_rate": 0.0001403642671292281,
      "loss": 0.5633,
      "step": 1720
    },
    {
      "epoch": 0.3000476954429172,
      "grad_norm": 0.6732260584831238,
      "learning_rate": 0.00014001734605377278,
      "loss": 0.5528,
      "step": 1730
    },
    {
      "epoch": 0.3017820751853618,
      "grad_norm": 0.7572551965713501,
      "learning_rate": 0.00013967042497831743,
      "loss": 0.5284,
      "step": 1740
    },
    {
      "epoch": 0.30351645492780643,
      "grad_norm": 0.8330501914024353,
      "learning_rate": 0.0001393235039028621,
      "loss": 0.5277,
      "step": 1750
    },
    {
      "epoch": 0.30525083467025105,
      "grad_norm": 0.7279008626937866,
      "learning_rate": 0.00013897658282740677,
      "loss": 0.4847,
      "step": 1760
    },
    {
      "epoch": 0.30698521441269566,
      "grad_norm": 0.9518227577209473,
      "learning_rate": 0.00013862966175195144,
      "loss": 0.501,
      "step": 1770
    },
    {
      "epoch": 0.3087195941551403,
      "grad_norm": 0.589831531047821,
      "learning_rate": 0.00013828274067649612,
      "loss": 0.4676,
      "step": 1780
    },
    {
      "epoch": 0.3104539738975849,
      "grad_norm": 0.5695339441299438,
      "learning_rate": 0.00013793581960104076,
      "loss": 0.5027,
      "step": 1790
    },
    {
      "epoch": 0.3121883536400295,
      "grad_norm": 0.6939374208450317,
      "learning_rate": 0.00013758889852558544,
      "loss": 0.5157,
      "step": 1800
    },
    {
      "epoch": 0.3139227333824741,
      "grad_norm": 0.7632051706314087,
      "learning_rate": 0.0001372419774501301,
      "loss": 0.5211,
      "step": 1810
    },
    {
      "epoch": 0.3156571131249187,
      "grad_norm": 0.7699695229530334,
      "learning_rate": 0.00013689505637467478,
      "loss": 0.4907,
      "step": 1820
    },
    {
      "epoch": 0.3173914928673633,
      "grad_norm": 0.7416660189628601,
      "learning_rate": 0.00013654813529921943,
      "loss": 0.5369,
      "step": 1830
    },
    {
      "epoch": 0.3191258726098079,
      "grad_norm": 0.7927618026733398,
      "learning_rate": 0.0001362012142237641,
      "loss": 0.5615,
      "step": 1840
    },
    {
      "epoch": 0.3208602523522525,
      "grad_norm": 0.808673083782196,
      "learning_rate": 0.00013585429314830877,
      "loss": 0.5227,
      "step": 1850
    },
    {
      "epoch": 0.32259463209469713,
      "grad_norm": 0.6342969536781311,
      "learning_rate": 0.00013550737207285342,
      "loss": 0.5363,
      "step": 1860
    },
    {
      "epoch": 0.32432901183714175,
      "grad_norm": 0.891576886177063,
      "learning_rate": 0.0001351604509973981,
      "loss": 0.5428,
      "step": 1870
    },
    {
      "epoch": 0.32606339157958636,
      "grad_norm": 0.9472002983093262,
      "learning_rate": 0.00013481352992194276,
      "loss": 0.558,
      "step": 1880
    },
    {
      "epoch": 0.327797771322031,
      "grad_norm": 0.724912703037262,
      "learning_rate": 0.00013446660884648744,
      "loss": 0.5343,
      "step": 1890
    },
    {
      "epoch": 0.3295321510644756,
      "grad_norm": 0.6124851107597351,
      "learning_rate": 0.0001341196877710321,
      "loss": 0.5214,
      "step": 1900
    },
    {
      "epoch": 0.3312665308069202,
      "grad_norm": 0.6415812969207764,
      "learning_rate": 0.00013377276669557676,
      "loss": 0.5232,
      "step": 1910
    },
    {
      "epoch": 0.33300091054936476,
      "grad_norm": 0.8251940608024597,
      "learning_rate": 0.00013342584562012143,
      "loss": 0.5193,
      "step": 1920
    },
    {
      "epoch": 0.3347352902918094,
      "grad_norm": 0.7508525848388672,
      "learning_rate": 0.0001330789245446661,
      "loss": 0.5051,
      "step": 1930
    },
    {
      "epoch": 0.336469670034254,
      "grad_norm": 1.266374111175537,
      "learning_rate": 0.00013273200346921075,
      "loss": 0.5421,
      "step": 1940
    },
    {
      "epoch": 0.3382040497766986,
      "grad_norm": 0.7218808531761169,
      "learning_rate": 0.00013238508239375542,
      "loss": 0.4701,
      "step": 1950
    },
    {
      "epoch": 0.3399384295191432,
      "grad_norm": 0.776216447353363,
      "learning_rate": 0.0001320381613183001,
      "loss": 0.4833,
      "step": 1960
    },
    {
      "epoch": 0.34167280926158783,
      "grad_norm": 0.6702186465263367,
      "learning_rate": 0.00013169124024284477,
      "loss": 0.5338,
      "step": 1970
    },
    {
      "epoch": 0.34340718900403244,
      "grad_norm": 0.6971157193183899,
      "learning_rate": 0.00013134431916738944,
      "loss": 0.5141,
      "step": 1980
    },
    {
      "epoch": 0.34514156874647706,
      "grad_norm": 1.0345960855484009,
      "learning_rate": 0.0001309973980919341,
      "loss": 0.515,
      "step": 1990
    },
    {
      "epoch": 0.34687594848892167,
      "grad_norm": 0.7123037576675415,
      "learning_rate": 0.00013065047701647876,
      "loss": 0.5251,
      "step": 2000
    }
  ],
  "logging_steps": 10,
  "max_steps": 5765,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6114853609472e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
