{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.6070329098556129,
  "eval_steps": 500,
  "global_step": 3500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0017343797424446083,
      "grad_norm": 2.5550849437713623,
      "learning_rate": 0.00019968777103209019,
      "loss": 3.0523,
      "step": 10
    },
    {
      "epoch": 0.0034687594848892165,
      "grad_norm": 2.0123374462127686,
      "learning_rate": 0.00019934084995663486,
      "loss": 2.3094,
      "step": 20
    },
    {
      "epoch": 0.005203139227333824,
      "grad_norm": 2.694957733154297,
      "learning_rate": 0.00019899392888117953,
      "loss": 1.7411,
      "step": 30
    },
    {
      "epoch": 0.006937518969778433,
      "grad_norm": 1.9005944728851318,
      "learning_rate": 0.0001986470078057242,
      "loss": 1.1989,
      "step": 40
    },
    {
      "epoch": 0.008671898712223042,
      "grad_norm": 1.4874471426010132,
      "learning_rate": 0.00019830008673026888,
      "loss": 1.0164,
      "step": 50
    },
    {
      "epoch": 0.010406278454667649,
      "grad_norm": 2.6214494705200195,
      "learning_rate": 0.00019795316565481355,
      "loss": 0.9067,
      "step": 60
    },
    {
      "epoch": 0.012140658197112257,
      "grad_norm": 1.266445279121399,
      "learning_rate": 0.00019760624457935822,
      "loss": 0.9224,
      "step": 70
    },
    {
      "epoch": 0.013875037939556866,
      "grad_norm": 1.247423529624939,
      "learning_rate": 0.0001972593235039029,
      "loss": 0.8334,
      "step": 80
    },
    {
      "epoch": 0.015609417682001475,
      "grad_norm": 1.0444533824920654,
      "learning_rate": 0.00019691240242844754,
      "loss": 0.8003,
      "step": 90
    },
    {
      "epoch": 0.017343797424446084,
      "grad_norm": 1.0636088848114014,
      "learning_rate": 0.0001965654813529922,
      "loss": 0.8069,
      "step": 100
    },
    {
      "epoch": 0.01907817716689069,
      "grad_norm": 1.1975089311599731,
      "learning_rate": 0.00019621856027753686,
      "loss": 0.8468,
      "step": 110
    },
    {
      "epoch": 0.020812556909335297,
      "grad_norm": 1.125996470451355,
      "learning_rate": 0.00019587163920208153,
      "loss": 0.7739,
      "step": 120
    },
    {
      "epoch": 0.022546936651779908,
      "grad_norm": 1.0888392925262451,
      "learning_rate": 0.0001955247181266262,
      "loss": 0.7945,
      "step": 130
    },
    {
      "epoch": 0.024281316394224515,
      "grad_norm": 1.4818414449691772,
      "learning_rate": 0.00019517779705117088,
      "loss": 0.7209,
      "step": 140
    },
    {
      "epoch": 0.026015696136669125,
      "grad_norm": 1.7991796731948853,
      "learning_rate": 0.00019483087597571555,
      "loss": 0.74,
      "step": 150
    },
    {
      "epoch": 0.027750075879113732,
      "grad_norm": 1.8334739208221436,
      "learning_rate": 0.0001944839549002602,
      "loss": 0.7401,
      "step": 160
    },
    {
      "epoch": 0.02948445562155834,
      "grad_norm": 1.1871280670166016,
      "learning_rate": 0.00019413703382480487,
      "loss": 0.6621,
      "step": 170
    },
    {
      "epoch": 0.03121883536400295,
      "grad_norm": 1.0494414567947388,
      "learning_rate": 0.00019379011274934952,
      "loss": 0.5911,
      "step": 180
    },
    {
      "epoch": 0.03295321510644756,
      "grad_norm": 0.973773181438446,
      "learning_rate": 0.0001934431916738942,
      "loss": 0.6231,
      "step": 190
    },
    {
      "epoch": 0.03468759484889217,
      "grad_norm": 1.3078233003616333,
      "learning_rate": 0.00019309627059843886,
      "loss": 0.5732,
      "step": 200
    },
    {
      "epoch": 0.036421974591336774,
      "grad_norm": 1.2161680459976196,
      "learning_rate": 0.00019274934952298353,
      "loss": 0.6369,
      "step": 210
    },
    {
      "epoch": 0.03815635433378138,
      "grad_norm": 1.6396287679672241,
      "learning_rate": 0.0001924024284475282,
      "loss": 0.625,
      "step": 220
    },
    {
      "epoch": 0.03989073407622599,
      "grad_norm": 1.4116144180297852,
      "learning_rate": 0.00019205550737207288,
      "loss": 0.647,
      "step": 230
    },
    {
      "epoch": 0.041625113818670595,
      "grad_norm": 1.0980229377746582,
      "learning_rate": 0.00019170858629661753,
      "loss": 0.5614,
      "step": 240
    },
    {
      "epoch": 0.04335949356111521,
      "grad_norm": 1.1962525844573975,
      "learning_rate": 0.0001913616652211622,
      "loss": 0.6344,
      "step": 250
    },
    {
      "epoch": 0.045093873303559816,
      "grad_norm": 1.2604451179504395,
      "learning_rate": 0.00019101474414570687,
      "loss": 0.5919,
      "step": 260
    },
    {
      "epoch": 0.04682825304600442,
      "grad_norm": 0.9544632434844971,
      "learning_rate": 0.00019066782307025152,
      "loss": 0.5926,
      "step": 270
    },
    {
      "epoch": 0.04856263278844903,
      "grad_norm": 1.094160795211792,
      "learning_rate": 0.0001903209019947962,
      "loss": 0.5788,
      "step": 280
    },
    {
      "epoch": 0.05029701253089364,
      "grad_norm": 1.0095820426940918,
      "learning_rate": 0.00018997398091934086,
      "loss": 0.6399,
      "step": 290
    },
    {
      "epoch": 0.05203139227333825,
      "grad_norm": 1.3148770332336426,
      "learning_rate": 0.00018962705984388554,
      "loss": 0.5493,
      "step": 300
    },
    {
      "epoch": 0.05376577201578286,
      "grad_norm": 1.2883634567260742,
      "learning_rate": 0.00018928013876843018,
      "loss": 0.5717,
      "step": 310
    },
    {
      "epoch": 0.055500151758227464,
      "grad_norm": 0.8932894468307495,
      "learning_rate": 0.00018893321769297485,
      "loss": 0.5656,
      "step": 320
    },
    {
      "epoch": 0.05723453150067207,
      "grad_norm": 1.0753910541534424,
      "learning_rate": 0.00018858629661751953,
      "loss": 0.594,
      "step": 330
    },
    {
      "epoch": 0.05896891124311668,
      "grad_norm": 0.753275454044342,
      "learning_rate": 0.0001882393755420642,
      "loss": 0.5478,
      "step": 340
    },
    {
      "epoch": 0.060703290985561285,
      "grad_norm": 1.0140023231506348,
      "learning_rate": 0.00018789245446660885,
      "loss": 0.5268,
      "step": 350
    },
    {
      "epoch": 0.0624376707280059,
      "grad_norm": 0.818980872631073,
      "learning_rate": 0.00018754553339115352,
      "loss": 0.566,
      "step": 360
    },
    {
      "epoch": 0.0641720504704505,
      "grad_norm": 1.1010839939117432,
      "learning_rate": 0.0001871986123156982,
      "loss": 0.6807,
      "step": 370
    },
    {
      "epoch": 0.06590643021289512,
      "grad_norm": 1.2425607442855835,
      "learning_rate": 0.00018685169124024284,
      "loss": 0.6306,
      "step": 380
    },
    {
      "epoch": 0.06764080995533972,
      "grad_norm": 1.155747413635254,
      "learning_rate": 0.0001865047701647875,
      "loss": 0.5633,
      "step": 390
    },
    {
      "epoch": 0.06937518969778433,
      "grad_norm": 1.2084158658981323,
      "learning_rate": 0.00018615784908933218,
      "loss": 0.5758,
      "step": 400
    },
    {
      "epoch": 0.07110956944022893,
      "grad_norm": 0.8114025592803955,
      "learning_rate": 0.00018581092801387686,
      "loss": 0.532,
      "step": 410
    },
    {
      "epoch": 0.07284394918267355,
      "grad_norm": 0.8228296637535095,
      "learning_rate": 0.00018546400693842153,
      "loss": 0.5447,
      "step": 420
    },
    {
      "epoch": 0.07457832892511815,
      "grad_norm": 0.9397292733192444,
      "learning_rate": 0.0001851170858629662,
      "loss": 0.5517,
      "step": 430
    },
    {
      "epoch": 0.07631270866756276,
      "grad_norm": 0.8269107937812805,
      "learning_rate": 0.00018477016478751085,
      "loss": 0.5935,
      "step": 440
    },
    {
      "epoch": 0.07804708841000738,
      "grad_norm": 0.958692729473114,
      "learning_rate": 0.00018442324371205552,
      "loss": 0.5456,
      "step": 450
    },
    {
      "epoch": 0.07978146815245198,
      "grad_norm": 0.9679394960403442,
      "learning_rate": 0.00018407632263660017,
      "loss": 0.5506,
      "step": 460
    },
    {
      "epoch": 0.08151584789489659,
      "grad_norm": 0.8142249584197998,
      "learning_rate": 0.00018372940156114484,
      "loss": 0.5826,
      "step": 470
    },
    {
      "epoch": 0.08325022763734119,
      "grad_norm": 1.1221299171447754,
      "learning_rate": 0.0001833824804856895,
      "loss": 0.599,
      "step": 480
    },
    {
      "epoch": 0.0849846073797858,
      "grad_norm": 0.8575493693351746,
      "learning_rate": 0.00018303555941023418,
      "loss": 0.6185,
      "step": 490
    },
    {
      "epoch": 0.08671898712223042,
      "grad_norm": 1.043362021446228,
      "learning_rate": 0.00018268863833477886,
      "loss": 0.5496,
      "step": 500
    },
    {
      "epoch": 0.08845336686467502,
      "grad_norm": 0.776316225528717,
      "learning_rate": 0.00018234171725932353,
      "loss": 0.5821,
      "step": 510
    },
    {
      "epoch": 0.09018774660711963,
      "grad_norm": 0.7581605315208435,
      "learning_rate": 0.00018199479618386818,
      "loss": 0.4986,
      "step": 520
    },
    {
      "epoch": 0.09192212634956423,
      "grad_norm": 1.2291282415390015,
      "learning_rate": 0.00018164787510841282,
      "loss": 0.5699,
      "step": 530
    },
    {
      "epoch": 0.09365650609200885,
      "grad_norm": 0.8632634878158569,
      "learning_rate": 0.0001813009540329575,
      "loss": 0.5643,
      "step": 540
    },
    {
      "epoch": 0.09539088583445346,
      "grad_norm": 1.301264762878418,
      "learning_rate": 0.00018095403295750217,
      "loss": 0.5133,
      "step": 550
    },
    {
      "epoch": 0.09712526557689806,
      "grad_norm": 1.0773627758026123,
      "learning_rate": 0.00018060711188204684,
      "loss": 0.5576,
      "step": 560
    },
    {
      "epoch": 0.09885964531934267,
      "grad_norm": 1.1195361614227295,
      "learning_rate": 0.0001802601908065915,
      "loss": 0.512,
      "step": 570
    },
    {
      "epoch": 0.10059402506178727,
      "grad_norm": 1.0149531364440918,
      "learning_rate": 0.00017991326973113619,
      "loss": 0.5615,
      "step": 580
    },
    {
      "epoch": 0.10232840480423189,
      "grad_norm": 0.7162327170372009,
      "learning_rate": 0.00017956634865568086,
      "loss": 0.5361,
      "step": 590
    },
    {
      "epoch": 0.1040627845466765,
      "grad_norm": 0.9201861023902893,
      "learning_rate": 0.0001792194275802255,
      "loss": 0.5411,
      "step": 600
    },
    {
      "epoch": 0.1057971642891211,
      "grad_norm": 0.9617904424667358,
      "learning_rate": 0.00017887250650477018,
      "loss": 0.5089,
      "step": 610
    },
    {
      "epoch": 0.10753154403156572,
      "grad_norm": 1.022149682044983,
      "learning_rate": 0.00017852558542931482,
      "loss": 0.6215,
      "step": 620
    },
    {
      "epoch": 0.10926592377401032,
      "grad_norm": 0.8238493204116821,
      "learning_rate": 0.0001781786643538595,
      "loss": 0.5451,
      "step": 630
    },
    {
      "epoch": 0.11100030351645493,
      "grad_norm": 0.8939248919487,
      "learning_rate": 0.00017783174327840417,
      "loss": 0.5524,
      "step": 640
    },
    {
      "epoch": 0.11273468325889954,
      "grad_norm": 0.8883143663406372,
      "learning_rate": 0.00017748482220294884,
      "loss": 0.5246,
      "step": 650
    },
    {
      "epoch": 0.11446906300134414,
      "grad_norm": 0.7424055933952332,
      "learning_rate": 0.00017713790112749352,
      "loss": 0.5263,
      "step": 660
    },
    {
      "epoch": 0.11620344274378876,
      "grad_norm": 1.2159603834152222,
      "learning_rate": 0.0001767909800520382,
      "loss": 0.5079,
      "step": 670
    },
    {
      "epoch": 0.11793782248623336,
      "grad_norm": 0.7574623227119446,
      "learning_rate": 0.00017644405897658283,
      "loss": 0.5281,
      "step": 680
    },
    {
      "epoch": 0.11967220222867797,
      "grad_norm": 0.792121946811676,
      "learning_rate": 0.0001760971379011275,
      "loss": 0.5008,
      "step": 690
    },
    {
      "epoch": 0.12140658197112257,
      "grad_norm": 0.8064789175987244,
      "learning_rate": 0.00017575021682567215,
      "loss": 0.5316,
      "step": 700
    },
    {
      "epoch": 0.12314096171356718,
      "grad_norm": 0.8532897233963013,
      "learning_rate": 0.00017540329575021683,
      "loss": 0.5311,
      "step": 710
    },
    {
      "epoch": 0.1248753414560118,
      "grad_norm": 0.7916345596313477,
      "learning_rate": 0.0001750563746747615,
      "loss": 0.5307,
      "step": 720
    },
    {
      "epoch": 0.1266097211984564,
      "grad_norm": 0.9886270761489868,
      "learning_rate": 0.00017470945359930617,
      "loss": 0.5523,
      "step": 730
    },
    {
      "epoch": 0.128344100940901,
      "grad_norm": 0.9201990365982056,
      "learning_rate": 0.00017436253252385084,
      "loss": 0.509,
      "step": 740
    },
    {
      "epoch": 0.13007848068334563,
      "grad_norm": 0.7614508271217346,
      "learning_rate": 0.0001740156114483955,
      "loss": 0.6164,
      "step": 750
    },
    {
      "epoch": 0.13181286042579024,
      "grad_norm": 0.7819229960441589,
      "learning_rate": 0.00017366869037294016,
      "loss": 0.529,
      "step": 760
    },
    {
      "epoch": 0.13354724016823483,
      "grad_norm": 0.9719189405441284,
      "learning_rate": 0.00017332176929748484,
      "loss": 0.5313,
      "step": 770
    },
    {
      "epoch": 0.13528161991067944,
      "grad_norm": 0.6589162349700928,
      "learning_rate": 0.0001729748482220295,
      "loss": 0.504,
      "step": 780
    },
    {
      "epoch": 0.13701599965312405,
      "grad_norm": 0.6838045120239258,
      "learning_rate": 0.00017262792714657415,
      "loss": 0.5136,
      "step": 790
    },
    {
      "epoch": 0.13875037939556867,
      "grad_norm": 0.7332388758659363,
      "learning_rate": 0.00017228100607111883,
      "loss": 0.5613,
      "step": 800
    },
    {
      "epoch": 0.14048475913801325,
      "grad_norm": 0.8705220818519592,
      "learning_rate": 0.0001719340849956635,
      "loss": 0.5142,
      "step": 810
    },
    {
      "epoch": 0.14221913888045787,
      "grad_norm": 1.0035444498062134,
      "learning_rate": 0.00017158716392020817,
      "loss": 0.5488,
      "step": 820
    },
    {
      "epoch": 0.14395351862290248,
      "grad_norm": 0.7737194299697876,
      "learning_rate": 0.00017124024284475282,
      "loss": 0.5355,
      "step": 830
    },
    {
      "epoch": 0.1456878983653471,
      "grad_norm": 0.7520883679389954,
      "learning_rate": 0.0001708933217692975,
      "loss": 0.573,
      "step": 840
    },
    {
      "epoch": 0.1474222781077917,
      "grad_norm": 0.8724210262298584,
      "learning_rate": 0.00017054640069384216,
      "loss": 0.5305,
      "step": 850
    },
    {
      "epoch": 0.1491566578502363,
      "grad_norm": 0.8224956393241882,
      "learning_rate": 0.00017019947961838684,
      "loss": 0.5538,
      "step": 860
    },
    {
      "epoch": 0.1508910375926809,
      "grad_norm": 0.8507670760154724,
      "learning_rate": 0.00016985255854293148,
      "loss": 0.5366,
      "step": 870
    },
    {
      "epoch": 0.15262541733512552,
      "grad_norm": 0.8243544697761536,
      "learning_rate": 0.00016950563746747616,
      "loss": 0.5069,
      "step": 880
    },
    {
      "epoch": 0.15435979707757014,
      "grad_norm": 0.71662437915802,
      "learning_rate": 0.00016915871639202083,
      "loss": 0.5319,
      "step": 890
    },
    {
      "epoch": 0.15609417682001475,
      "grad_norm": 0.7661607265472412,
      "learning_rate": 0.00016881179531656547,
      "loss": 0.5335,
      "step": 900
    },
    {
      "epoch": 0.15782855656245934,
      "grad_norm": 0.6027960181236267,
      "learning_rate": 0.00016846487424111015,
      "loss": 0.5156,
      "step": 910
    },
    {
      "epoch": 0.15956293630490395,
      "grad_norm": 0.9688888192176819,
      "learning_rate": 0.00016811795316565482,
      "loss": 0.5643,
      "step": 920
    },
    {
      "epoch": 0.16129731604734857,
      "grad_norm": 0.8623057007789612,
      "learning_rate": 0.0001677710320901995,
      "loss": 0.4927,
      "step": 930
    },
    {
      "epoch": 0.16303169578979318,
      "grad_norm": 0.8581346273422241,
      "learning_rate": 0.00016742411101474417,
      "loss": 0.5247,
      "step": 940
    },
    {
      "epoch": 0.1647660755322378,
      "grad_norm": 0.7837463021278381,
      "learning_rate": 0.00016707718993928884,
      "loss": 0.5767,
      "step": 950
    },
    {
      "epoch": 0.16650045527468238,
      "grad_norm": 0.7025507092475891,
      "learning_rate": 0.00016673026886383348,
      "loss": 0.5687,
      "step": 960
    },
    {
      "epoch": 0.168234835017127,
      "grad_norm": 0.6543734669685364,
      "learning_rate": 0.00016638334778837813,
      "loss": 0.5689,
      "step": 970
    },
    {
      "epoch": 0.1699692147595716,
      "grad_norm": 1.0180604457855225,
      "learning_rate": 0.0001660364267129228,
      "loss": 0.564,
      "step": 980
    },
    {
      "epoch": 0.17170359450201622,
      "grad_norm": 0.9559260606765747,
      "learning_rate": 0.00016568950563746748,
      "loss": 0.5924,
      "step": 990
    },
    {
      "epoch": 0.17343797424446084,
      "grad_norm": 0.6762255430221558,
      "learning_rate": 0.00016534258456201215,
      "loss": 0.5048,
      "step": 1000
    },
    {
      "epoch": 0.17517235398690542,
      "grad_norm": 0.8638314008712769,
      "learning_rate": 0.00016499566348655682,
      "loss": 0.5841,
      "step": 1010
    },
    {
      "epoch": 0.17690673372935004,
      "grad_norm": 0.9742166996002197,
      "learning_rate": 0.0001646487424111015,
      "loss": 0.5587,
      "step": 1020
    },
    {
      "epoch": 0.17864111347179465,
      "grad_norm": 0.9518930912017822,
      "learning_rate": 0.00016430182133564617,
      "loss": 0.5526,
      "step": 1030
    },
    {
      "epoch": 0.18037549321423926,
      "grad_norm": 1.1513153314590454,
      "learning_rate": 0.0001639549002601908,
      "loss": 0.5499,
      "step": 1040
    },
    {
      "epoch": 0.18210987295668388,
      "grad_norm": 0.8407158255577087,
      "learning_rate": 0.00016360797918473546,
      "loss": 0.5786,
      "step": 1050
    },
    {
      "epoch": 0.18384425269912846,
      "grad_norm": 0.8604046106338501,
      "learning_rate": 0.00016326105810928013,
      "loss": 0.5106,
      "step": 1060
    },
    {
      "epoch": 0.18557863244157308,
      "grad_norm": 0.6312768459320068,
      "learning_rate": 0.0001629141370338248,
      "loss": 0.5601,
      "step": 1070
    },
    {
      "epoch": 0.1873130121840177,
      "grad_norm": 0.8525477051734924,
      "learning_rate": 0.00016256721595836948,
      "loss": 0.5702,
      "step": 1080
    },
    {
      "epoch": 0.1890473919264623,
      "grad_norm": 0.8499556183815002,
      "learning_rate": 0.00016222029488291415,
      "loss": 0.5689,
      "step": 1090
    },
    {
      "epoch": 0.19078177166890692,
      "grad_norm": 0.9467098116874695,
      "learning_rate": 0.00016187337380745882,
      "loss": 0.5168,
      "step": 1100
    },
    {
      "epoch": 0.1925161514113515,
      "grad_norm": 0.8677939772605896,
      "learning_rate": 0.0001615264527320035,
      "loss": 0.5056,
      "step": 1110
    },
    {
      "epoch": 0.19425053115379612,
      "grad_norm": 0.8191261291503906,
      "learning_rate": 0.00016117953165654814,
      "loss": 0.5172,
      "step": 1120
    },
    {
      "epoch": 0.19598491089624073,
      "grad_norm": 0.9394680857658386,
      "learning_rate": 0.00016083261058109281,
      "loss": 0.5182,
      "step": 1130
    },
    {
      "epoch": 0.19771929063868535,
      "grad_norm": 0.9703214764595032,
      "learning_rate": 0.00016048568950563746,
      "loss": 0.4981,
      "step": 1140
    },
    {
      "epoch": 0.19945367038112996,
      "grad_norm": 0.7619735598564148,
      "learning_rate": 0.00016013876843018213,
      "loss": 0.5763,
      "step": 1150
    },
    {
      "epoch": 0.20118805012357455,
      "grad_norm": 1.0625637769699097,
      "learning_rate": 0.0001597918473547268,
      "loss": 0.5037,
      "step": 1160
    },
    {
      "epoch": 0.20292242986601916,
      "grad_norm": 0.7585030794143677,
      "learning_rate": 0.00015944492627927148,
      "loss": 0.5171,
      "step": 1170
    },
    {
      "epoch": 0.20465680960846377,
      "grad_norm": 0.7726263403892517,
      "learning_rate": 0.00015909800520381615,
      "loss": 0.54,
      "step": 1180
    },
    {
      "epoch": 0.2063911893509084,
      "grad_norm": 0.7178537249565125,
      "learning_rate": 0.00015875108412836082,
      "loss": 0.5195,
      "step": 1190
    },
    {
      "epoch": 0.208125569093353,
      "grad_norm": 0.9159913063049316,
      "learning_rate": 0.00015840416305290547,
      "loss": 0.5859,
      "step": 1200
    },
    {
      "epoch": 0.2098599488357976,
      "grad_norm": 0.8675339818000793,
      "learning_rate": 0.00015805724197745014,
      "loss": 0.5552,
      "step": 1210
    },
    {
      "epoch": 0.2115943285782422,
      "grad_norm": 0.8014780879020691,
      "learning_rate": 0.0001577103209019948,
      "loss": 0.4815,
      "step": 1220
    },
    {
      "epoch": 0.21332870832068682,
      "grad_norm": 0.8599198460578918,
      "learning_rate": 0.00015736339982653946,
      "loss": 0.5037,
      "step": 1230
    },
    {
      "epoch": 0.21506308806313143,
      "grad_norm": 0.775314211845398,
      "learning_rate": 0.00015701647875108413,
      "loss": 0.5641,
      "step": 1240
    },
    {
      "epoch": 0.21679746780557604,
      "grad_norm": 1.3312227725982666,
      "learning_rate": 0.0001566695576756288,
      "loss": 0.5015,
      "step": 1250
    },
    {
      "epoch": 0.21853184754802063,
      "grad_norm": 1.013002634048462,
      "learning_rate": 0.00015632263660017348,
      "loss": 0.5295,
      "step": 1260
    },
    {
      "epoch": 0.22026622729046524,
      "grad_norm": 0.618144154548645,
      "learning_rate": 0.00015597571552471813,
      "loss": 0.5206,
      "step": 1270
    },
    {
      "epoch": 0.22200060703290986,
      "grad_norm": 0.7005691528320312,
      "learning_rate": 0.0001556287944492628,
      "loss": 0.507,
      "step": 1280
    },
    {
      "epoch": 0.22373498677535447,
      "grad_norm": 0.9774540662765503,
      "learning_rate": 0.00015528187337380747,
      "loss": 0.5415,
      "step": 1290
    },
    {
      "epoch": 0.22546936651779909,
      "grad_norm": 0.8326419591903687,
      "learning_rate": 0.00015493495229835214,
      "loss": 0.5859,
      "step": 1300
    },
    {
      "epoch": 0.22720374626024367,
      "grad_norm": 0.7086894512176514,
      "learning_rate": 0.0001545880312228968,
      "loss": 0.4951,
      "step": 1310
    },
    {
      "epoch": 0.22893812600268829,
      "grad_norm": 0.8754698038101196,
      "learning_rate": 0.00015424111014744146,
      "loss": 0.5692,
      "step": 1320
    },
    {
      "epoch": 0.2306725057451329,
      "grad_norm": 0.7073501348495483,
      "learning_rate": 0.00015389418907198614,
      "loss": 0.5121,
      "step": 1330
    },
    {
      "epoch": 0.2324068854875775,
      "grad_norm": 0.7450106739997864,
      "learning_rate": 0.00015354726799653078,
      "loss": 0.5772,
      "step": 1340
    },
    {
      "epoch": 0.2341412652300221,
      "grad_norm": 1.057621955871582,
      "learning_rate": 0.00015320034692107545,
      "loss": 0.5487,
      "step": 1350
    },
    {
      "epoch": 0.2358756449724667,
      "grad_norm": 0.7074055671691895,
      "learning_rate": 0.00015285342584562013,
      "loss": 0.485,
      "step": 1360
    },
    {
      "epoch": 0.23761002471491133,
      "grad_norm": 0.7234088778495789,
      "learning_rate": 0.0001525065047701648,
      "loss": 0.5442,
      "step": 1370
    },
    {
      "epoch": 0.23934440445735594,
      "grad_norm": 0.8184717893600464,
      "learning_rate": 0.00015215958369470947,
      "loss": 0.5211,
      "step": 1380
    },
    {
      "epoch": 0.24107878419980056,
      "grad_norm": 0.8528016805648804,
      "learning_rate": 0.00015181266261925412,
      "loss": 0.5299,
      "step": 1390
    },
    {
      "epoch": 0.24281316394224514,
      "grad_norm": 0.9218398332595825,
      "learning_rate": 0.0001514657415437988,
      "loss": 0.5336,
      "step": 1400
    },
    {
      "epoch": 0.24454754368468976,
      "grad_norm": 0.9450495839118958,
      "learning_rate": 0.00015111882046834346,
      "loss": 0.4913,
      "step": 1410
    },
    {
      "epoch": 0.24628192342713437,
      "grad_norm": 0.8063280582427979,
      "learning_rate": 0.0001507718993928881,
      "loss": 0.4731,
      "step": 1420
    },
    {
      "epoch": 0.24801630316957898,
      "grad_norm": 0.7726646661758423,
      "learning_rate": 0.00015042497831743278,
      "loss": 0.582,
      "step": 1430
    },
    {
      "epoch": 0.2497506829120236,
      "grad_norm": 0.6898558735847473,
      "learning_rate": 0.00015007805724197746,
      "loss": 0.5459,
      "step": 1440
    },
    {
      "epoch": 0.2514850626544682,
      "grad_norm": 0.7302579879760742,
      "learning_rate": 0.00014973113616652213,
      "loss": 0.4509,
      "step": 1450
    },
    {
      "epoch": 0.2532194423969128,
      "grad_norm": 0.8079715967178345,
      "learning_rate": 0.0001493842150910668,
      "loss": 0.4924,
      "step": 1460
    },
    {
      "epoch": 0.2549538221393574,
      "grad_norm": 0.9184403419494629,
      "learning_rate": 0.00014903729401561148,
      "loss": 0.5242,
      "step": 1470
    },
    {
      "epoch": 0.256688201881802,
      "grad_norm": 0.9540342092514038,
      "learning_rate": 0.00014869037294015612,
      "loss": 0.5108,
      "step": 1480
    },
    {
      "epoch": 0.25842258162424664,
      "grad_norm": 0.662722647190094,
      "learning_rate": 0.00014834345186470077,
      "loss": 0.4707,
      "step": 1490
    },
    {
      "epoch": 0.26015696136669125,
      "grad_norm": 1.0262176990509033,
      "learning_rate": 0.00014799653078924544,
      "loss": 0.5443,
      "step": 1500
    },
    {
      "epoch": 0.26189134110913587,
      "grad_norm": 0.7653674483299255,
      "learning_rate": 0.0001476496097137901,
      "loss": 0.4961,
      "step": 1510
    },
    {
      "epoch": 0.2636257208515805,
      "grad_norm": 0.6642130017280579,
      "learning_rate": 0.00014730268863833479,
      "loss": 0.5682,
      "step": 1520
    },
    {
      "epoch": 0.26536010059402504,
      "grad_norm": 0.7625731825828552,
      "learning_rate": 0.00014695576756287946,
      "loss": 0.5445,
      "step": 1530
    },
    {
      "epoch": 0.26709448033646965,
      "grad_norm": 0.8662495613098145,
      "learning_rate": 0.00014660884648742413,
      "loss": 0.5393,
      "step": 1540
    },
    {
      "epoch": 0.26882886007891427,
      "grad_norm": 0.8010826706886292,
      "learning_rate": 0.0001462619254119688,
      "loss": 0.5276,
      "step": 1550
    },
    {
      "epoch": 0.2705632398213589,
      "grad_norm": 0.697375476360321,
      "learning_rate": 0.00014591500433651345,
      "loss": 0.514,
      "step": 1560
    },
    {
      "epoch": 0.2722976195638035,
      "grad_norm": 0.5982699990272522,
      "learning_rate": 0.0001455680832610581,
      "loss": 0.4839,
      "step": 1570
    },
    {
      "epoch": 0.2740319993062481,
      "grad_norm": 0.7269188165664673,
      "learning_rate": 0.00014522116218560277,
      "loss": 0.4315,
      "step": 1580
    },
    {
      "epoch": 0.2757663790486927,
      "grad_norm": 0.7482186555862427,
      "learning_rate": 0.00014487424111014744,
      "loss": 0.5392,
      "step": 1590
    },
    {
      "epoch": 0.27750075879113734,
      "grad_norm": 0.7533226609230042,
      "learning_rate": 0.00014452732003469211,
      "loss": 0.5058,
      "step": 1600
    },
    {
      "epoch": 0.27923513853358195,
      "grad_norm": 0.7971985340118408,
      "learning_rate": 0.0001441803989592368,
      "loss": 0.5496,
      "step": 1610
    },
    {
      "epoch": 0.2809695182760265,
      "grad_norm": 0.9086535573005676,
      "learning_rate": 0.00014383347788378146,
      "loss": 0.5231,
      "step": 1620
    },
    {
      "epoch": 0.2827038980184711,
      "grad_norm": 0.7624841332435608,
      "learning_rate": 0.00014348655680832613,
      "loss": 0.4796,
      "step": 1630
    },
    {
      "epoch": 0.28443827776091574,
      "grad_norm": 0.7093886137008667,
      "learning_rate": 0.00014313963573287078,
      "loss": 0.4985,
      "step": 1640
    },
    {
      "epoch": 0.28617265750336035,
      "grad_norm": 0.6750412583351135,
      "learning_rate": 0.00014279271465741545,
      "loss": 0.5532,
      "step": 1650
    },
    {
      "epoch": 0.28790703724580496,
      "grad_norm": 0.6476730704307556,
      "learning_rate": 0.0001424457935819601,
      "loss": 0.5441,
      "step": 1660
    },
    {
      "epoch": 0.2896414169882496,
      "grad_norm": 0.6679351925849915,
      "learning_rate": 0.00014209887250650477,
      "loss": 0.5186,
      "step": 1670
    },
    {
      "epoch": 0.2913757967306942,
      "grad_norm": 0.691450297832489,
      "learning_rate": 0.00014175195143104944,
      "loss": 0.4914,
      "step": 1680
    },
    {
      "epoch": 0.2931101764731388,
      "grad_norm": 0.7088834047317505,
      "learning_rate": 0.00014140503035559412,
      "loss": 0.4836,
      "step": 1690
    },
    {
      "epoch": 0.2948445562155834,
      "grad_norm": 0.7763418555259705,
      "learning_rate": 0.0001410581092801388,
      "loss": 0.5297,
      "step": 1700
    },
    {
      "epoch": 0.29657893595802803,
      "grad_norm": 0.9440441727638245,
      "learning_rate": 0.00014071118820468343,
      "loss": 0.5052,
      "step": 1710
    },
    {
      "epoch": 0.2983133157004726,
      "grad_norm": 0.9379427433013916,
      "learning_rate": 0.0001403642671292281,
      "loss": 0.5633,
      "step": 1720
    },
    {
      "epoch": 0.3000476954429172,
      "grad_norm": 0.6732260584831238,
      "learning_rate": 0.00014001734605377278,
      "loss": 0.5528,
      "step": 1730
    },
    {
      "epoch": 0.3017820751853618,
      "grad_norm": 0.7572551965713501,
      "learning_rate": 0.00013967042497831743,
      "loss": 0.5284,
      "step": 1740
    },
    {
      "epoch": 0.30351645492780643,
      "grad_norm": 0.8330501914024353,
      "learning_rate": 0.0001393235039028621,
      "loss": 0.5277,
      "step": 1750
    },
    {
      "epoch": 0.30525083467025105,
      "grad_norm": 0.7279008626937866,
      "learning_rate": 0.00013897658282740677,
      "loss": 0.4847,
      "step": 1760
    },
    {
      "epoch": 0.30698521441269566,
      "grad_norm": 0.9518227577209473,
      "learning_rate": 0.00013862966175195144,
      "loss": 0.501,
      "step": 1770
    },
    {
      "epoch": 0.3087195941551403,
      "grad_norm": 0.589831531047821,
      "learning_rate": 0.00013828274067649612,
      "loss": 0.4676,
      "step": 1780
    },
    {
      "epoch": 0.3104539738975849,
      "grad_norm": 0.5695339441299438,
      "learning_rate": 0.00013793581960104076,
      "loss": 0.5027,
      "step": 1790
    },
    {
      "epoch": 0.3121883536400295,
      "grad_norm": 0.6939374208450317,
      "learning_rate": 0.00013758889852558544,
      "loss": 0.5157,
      "step": 1800
    },
    {
      "epoch": 0.3139227333824741,
      "grad_norm": 0.7632051706314087,
      "learning_rate": 0.0001372419774501301,
      "loss": 0.5211,
      "step": 1810
    },
    {
      "epoch": 0.3156571131249187,
      "grad_norm": 0.7699695229530334,
      "learning_rate": 0.00013689505637467478,
      "loss": 0.4907,
      "step": 1820
    },
    {
      "epoch": 0.3173914928673633,
      "grad_norm": 0.7416660189628601,
      "learning_rate": 0.00013654813529921943,
      "loss": 0.5369,
      "step": 1830
    },
    {
      "epoch": 0.3191258726098079,
      "grad_norm": 0.7927618026733398,
      "learning_rate": 0.0001362012142237641,
      "loss": 0.5615,
      "step": 1840
    },
    {
      "epoch": 0.3208602523522525,
      "grad_norm": 0.808673083782196,
      "learning_rate": 0.00013585429314830877,
      "loss": 0.5227,
      "step": 1850
    },
    {
      "epoch": 0.32259463209469713,
      "grad_norm": 0.6342969536781311,
      "learning_rate": 0.00013550737207285342,
      "loss": 0.5363,
      "step": 1860
    },
    {
      "epoch": 0.32432901183714175,
      "grad_norm": 0.891576886177063,
      "learning_rate": 0.0001351604509973981,
      "loss": 0.5428,
      "step": 1870
    },
    {
      "epoch": 0.32606339157958636,
      "grad_norm": 0.9472002983093262,
      "learning_rate": 0.00013481352992194276,
      "loss": 0.558,
      "step": 1880
    },
    {
      "epoch": 0.327797771322031,
      "grad_norm": 0.724912703037262,
      "learning_rate": 0.00013446660884648744,
      "loss": 0.5343,
      "step": 1890
    },
    {
      "epoch": 0.3295321510644756,
      "grad_norm": 0.6124851107597351,
      "learning_rate": 0.0001341196877710321,
      "loss": 0.5214,
      "step": 1900
    },
    {
      "epoch": 0.3312665308069202,
      "grad_norm": 0.6415812969207764,
      "learning_rate": 0.00013377276669557676,
      "loss": 0.5232,
      "step": 1910
    },
    {
      "epoch": 0.33300091054936476,
      "grad_norm": 0.8251940608024597,
      "learning_rate": 0.00013342584562012143,
      "loss": 0.5193,
      "step": 1920
    },
    {
      "epoch": 0.3347352902918094,
      "grad_norm": 0.7508525848388672,
      "learning_rate": 0.0001330789245446661,
      "loss": 0.5051,
      "step": 1930
    },
    {
      "epoch": 0.336469670034254,
      "grad_norm": 1.266374111175537,
      "learning_rate": 0.00013273200346921075,
      "loss": 0.5421,
      "step": 1940
    },
    {
      "epoch": 0.3382040497766986,
      "grad_norm": 0.7218808531761169,
      "learning_rate": 0.00013238508239375542,
      "loss": 0.4701,
      "step": 1950
    },
    {
      "epoch": 0.3399384295191432,
      "grad_norm": 0.776216447353363,
      "learning_rate": 0.0001320381613183001,
      "loss": 0.4833,
      "step": 1960
    },
    {
      "epoch": 0.34167280926158783,
      "grad_norm": 0.6702186465263367,
      "learning_rate": 0.00013169124024284477,
      "loss": 0.5338,
      "step": 1970
    },
    {
      "epoch": 0.34340718900403244,
      "grad_norm": 0.6971157193183899,
      "learning_rate": 0.00013134431916738944,
      "loss": 0.5141,
      "step": 1980
    },
    {
      "epoch": 0.34514156874647706,
      "grad_norm": 1.0345960855484009,
      "learning_rate": 0.0001309973980919341,
      "loss": 0.515,
      "step": 1990
    },
    {
      "epoch": 0.34687594848892167,
      "grad_norm": 0.7123037576675415,
      "learning_rate": 0.00013065047701647876,
      "loss": 0.5251,
      "step": 2000
    },
    {
      "epoch": 0.3486103282313663,
      "grad_norm": 0.8493272066116333,
      "learning_rate": 0.0001303035559410234,
      "loss": 0.5,
      "step": 2010
    },
    {
      "epoch": 0.35034470797381084,
      "grad_norm": 0.6264200806617737,
      "learning_rate": 0.00012995663486556808,
      "loss": 0.5109,
      "step": 2020
    },
    {
      "epoch": 0.35207908771625546,
      "grad_norm": 0.7789064049720764,
      "learning_rate": 0.00012960971379011275,
      "loss": 0.5265,
      "step": 2030
    },
    {
      "epoch": 0.35381346745870007,
      "grad_norm": 1.215909719467163,
      "learning_rate": 0.00012926279271465742,
      "loss": 0.5071,
      "step": 2040
    },
    {
      "epoch": 0.3555478472011447,
      "grad_norm": 1.1748603582382202,
      "learning_rate": 0.0001289158716392021,
      "loss": 0.5101,
      "step": 2050
    },
    {
      "epoch": 0.3572822269435893,
      "grad_norm": 0.590299129486084,
      "learning_rate": 0.00012856895056374677,
      "loss": 0.4986,
      "step": 2060
    },
    {
      "epoch": 0.3590166066860339,
      "grad_norm": 0.878107488155365,
      "learning_rate": 0.00012822202948829144,
      "loss": 0.5399,
      "step": 2070
    },
    {
      "epoch": 0.3607509864284785,
      "grad_norm": 1.046035885810852,
      "learning_rate": 0.00012787510841283609,
      "loss": 0.5595,
      "step": 2080
    },
    {
      "epoch": 0.36248536617092314,
      "grad_norm": 1.0968817472457886,
      "learning_rate": 0.00012752818733738073,
      "loss": 0.5195,
      "step": 2090
    },
    {
      "epoch": 0.36421974591336775,
      "grad_norm": 0.8078921437263489,
      "learning_rate": 0.0001271812662619254,
      "loss": 0.5398,
      "step": 2100
    },
    {
      "epoch": 0.36595412565581237,
      "grad_norm": 0.6793740391731262,
      "learning_rate": 0.00012683434518647008,
      "loss": 0.5112,
      "step": 2110
    },
    {
      "epoch": 0.3676885053982569,
      "grad_norm": 0.6471167206764221,
      "learning_rate": 0.00012648742411101475,
      "loss": 0.4529,
      "step": 2120
    },
    {
      "epoch": 0.36942288514070154,
      "grad_norm": 1.1910035610198975,
      "learning_rate": 0.00012614050303555942,
      "loss": 0.5065,
      "step": 2130
    },
    {
      "epoch": 0.37115726488314615,
      "grad_norm": 0.8445138931274414,
      "learning_rate": 0.0001257935819601041,
      "loss": 0.5068,
      "step": 2140
    },
    {
      "epoch": 0.37289164462559077,
      "grad_norm": 0.6547709703445435,
      "learning_rate": 0.00012544666088464877,
      "loss": 0.4913,
      "step": 2150
    },
    {
      "epoch": 0.3746260243680354,
      "grad_norm": 0.5325044393539429,
      "learning_rate": 0.00012509973980919341,
      "loss": 0.5387,
      "step": 2160
    },
    {
      "epoch": 0.37636040411048,
      "grad_norm": 0.7426567077636719,
      "learning_rate": 0.0001247528187337381,
      "loss": 0.5446,
      "step": 2170
    },
    {
      "epoch": 0.3780947838529246,
      "grad_norm": 0.6493839025497437,
      "learning_rate": 0.00012440589765828273,
      "loss": 0.4734,
      "step": 2180
    },
    {
      "epoch": 0.3798291635953692,
      "grad_norm": 0.6491600275039673,
      "learning_rate": 0.0001240589765828274,
      "loss": 0.5204,
      "step": 2190
    },
    {
      "epoch": 0.38156354333781384,
      "grad_norm": 0.6509878039360046,
      "learning_rate": 0.00012371205550737208,
      "loss": 0.5059,
      "step": 2200
    },
    {
      "epoch": 0.3832979230802584,
      "grad_norm": 0.8043583035469055,
      "learning_rate": 0.00012336513443191675,
      "loss": 0.513,
      "step": 2210
    },
    {
      "epoch": 0.385032302822703,
      "grad_norm": 0.8371724486351013,
      "learning_rate": 0.00012301821335646143,
      "loss": 0.5115,
      "step": 2220
    },
    {
      "epoch": 0.3867666825651476,
      "grad_norm": 0.757968544960022,
      "learning_rate": 0.00012267129228100607,
      "loss": 0.5394,
      "step": 2230
    },
    {
      "epoch": 0.38850106230759224,
      "grad_norm": 0.8001652956008911,
      "learning_rate": 0.00012232437120555074,
      "loss": 0.5154,
      "step": 2240
    },
    {
      "epoch": 0.39023544205003685,
      "grad_norm": 0.6728013753890991,
      "learning_rate": 0.0001219774501300954,
      "loss": 0.5106,
      "step": 2250
    },
    {
      "epoch": 0.39196982179248147,
      "grad_norm": 0.8274458050727844,
      "learning_rate": 0.00012163052905464008,
      "loss": 0.4988,
      "step": 2260
    },
    {
      "epoch": 0.3937042015349261,
      "grad_norm": 0.8530777096748352,
      "learning_rate": 0.00012128360797918475,
      "loss": 0.5403,
      "step": 2270
    },
    {
      "epoch": 0.3954385812773707,
      "grad_norm": 0.794225811958313,
      "learning_rate": 0.00012093668690372941,
      "loss": 0.5223,
      "step": 2280
    },
    {
      "epoch": 0.3971729610198153,
      "grad_norm": 0.6251499652862549,
      "learning_rate": 0.00012058976582827408,
      "loss": 0.5006,
      "step": 2290
    },
    {
      "epoch": 0.3989073407622599,
      "grad_norm": 0.6972042918205261,
      "learning_rate": 0.00012024284475281873,
      "loss": 0.4906,
      "step": 2300
    },
    {
      "epoch": 0.4006417205047045,
      "grad_norm": 0.6007173657417297,
      "learning_rate": 0.0001198959236773634,
      "loss": 0.4604,
      "step": 2310
    },
    {
      "epoch": 0.4023761002471491,
      "grad_norm": 0.8842028379440308,
      "learning_rate": 0.00011954900260190807,
      "loss": 0.5141,
      "step": 2320
    },
    {
      "epoch": 0.4041104799895937,
      "grad_norm": 0.8369344472885132,
      "learning_rate": 0.00011920208152645273,
      "loss": 0.5325,
      "step": 2330
    },
    {
      "epoch": 0.4058448597320383,
      "grad_norm": 0.829085111618042,
      "learning_rate": 0.0001188551604509974,
      "loss": 0.5402,
      "step": 2340
    },
    {
      "epoch": 0.40757923947448294,
      "grad_norm": 0.8348305225372314,
      "learning_rate": 0.00011850823937554208,
      "loss": 0.5528,
      "step": 2350
    },
    {
      "epoch": 0.40931361921692755,
      "grad_norm": 0.879067599773407,
      "learning_rate": 0.00011816131830008674,
      "loss": 0.478,
      "step": 2360
    },
    {
      "epoch": 0.41104799895937216,
      "grad_norm": 0.6829871535301208,
      "learning_rate": 0.00011781439722463141,
      "loss": 0.5396,
      "step": 2370
    },
    {
      "epoch": 0.4127823787018168,
      "grad_norm": 0.7062913775444031,
      "learning_rate": 0.00011746747614917606,
      "loss": 0.503,
      "step": 2380
    },
    {
      "epoch": 0.4145167584442614,
      "grad_norm": 0.8982194662094116,
      "learning_rate": 0.00011712055507372073,
      "loss": 0.4695,
      "step": 2390
    },
    {
      "epoch": 0.416251138186706,
      "grad_norm": 0.5161424279212952,
      "learning_rate": 0.0001167736339982654,
      "loss": 0.5311,
      "step": 2400
    },
    {
      "epoch": 0.41798551792915056,
      "grad_norm": 0.8519994020462036,
      "learning_rate": 0.00011642671292281006,
      "loss": 0.489,
      "step": 2410
    },
    {
      "epoch": 0.4197198976715952,
      "grad_norm": 0.6411151885986328,
      "learning_rate": 0.00011607979184735473,
      "loss": 0.4943,
      "step": 2420
    },
    {
      "epoch": 0.4214542774140398,
      "grad_norm": 0.571524977684021,
      "learning_rate": 0.0001157328707718994,
      "loss": 0.4747,
      "step": 2430
    },
    {
      "epoch": 0.4231886571564844,
      "grad_norm": 0.7930188775062561,
      "learning_rate": 0.00011538594969644408,
      "loss": 0.5654,
      "step": 2440
    },
    {
      "epoch": 0.424923036898929,
      "grad_norm": 0.6249622106552124,
      "learning_rate": 0.00011503902862098872,
      "loss": 0.5306,
      "step": 2450
    },
    {
      "epoch": 0.42665741664137363,
      "grad_norm": 0.694672167301178,
      "learning_rate": 0.00011469210754553338,
      "loss": 0.5114,
      "step": 2460
    },
    {
      "epoch": 0.42839179638381825,
      "grad_norm": 0.8747013807296753,
      "learning_rate": 0.00011434518647007806,
      "loss": 0.5157,
      "step": 2470
    },
    {
      "epoch": 0.43012617612626286,
      "grad_norm": 0.6652283668518066,
      "learning_rate": 0.00011399826539462273,
      "loss": 0.5061,
      "step": 2480
    },
    {
      "epoch": 0.4318605558687075,
      "grad_norm": 0.7100988030433655,
      "learning_rate": 0.0001136513443191674,
      "loss": 0.4947,
      "step": 2490
    },
    {
      "epoch": 0.4335949356111521,
      "grad_norm": 0.9096623063087463,
      "learning_rate": 0.00011330442324371206,
      "loss": 0.4819,
      "step": 2500
    },
    {
      "epoch": 0.43532931535359665,
      "grad_norm": 0.8663245439529419,
      "learning_rate": 0.00011295750216825673,
      "loss": 0.5197,
      "step": 2510
    },
    {
      "epoch": 0.43706369509604126,
      "grad_norm": 0.8671486377716064,
      "learning_rate": 0.00011261058109280141,
      "loss": 0.5025,
      "step": 2520
    },
    {
      "epoch": 0.4387980748384859,
      "grad_norm": 0.8190407752990723,
      "learning_rate": 0.00011226366001734605,
      "loss": 0.5023,
      "step": 2530
    },
    {
      "epoch": 0.4405324545809305,
      "grad_norm": 0.6053672432899475,
      "learning_rate": 0.00011191673894189073,
      "loss": 0.5745,
      "step": 2540
    },
    {
      "epoch": 0.4422668343233751,
      "grad_norm": 0.8161375522613525,
      "learning_rate": 0.00011156981786643539,
      "loss": 0.4808,
      "step": 2550
    },
    {
      "epoch": 0.4440012140658197,
      "grad_norm": 0.9839289784431458,
      "learning_rate": 0.00011122289679098006,
      "loss": 0.5264,
      "step": 2560
    },
    {
      "epoch": 0.44573559380826433,
      "grad_norm": 0.6492114067077637,
      "learning_rate": 0.00011087597571552473,
      "loss": 0.4893,
      "step": 2570
    },
    {
      "epoch": 0.44746997355070894,
      "grad_norm": 0.8308883309364319,
      "learning_rate": 0.00011052905464006939,
      "loss": 0.5326,
      "step": 2580
    },
    {
      "epoch": 0.44920435329315356,
      "grad_norm": 0.902975857257843,
      "learning_rate": 0.00011018213356461406,
      "loss": 0.4667,
      "step": 2590
    },
    {
      "epoch": 0.45093873303559817,
      "grad_norm": 0.6316888928413391,
      "learning_rate": 0.00010983521248915871,
      "loss": 0.5458,
      "step": 2600
    },
    {
      "epoch": 0.45267311277804273,
      "grad_norm": 0.6769781112670898,
      "learning_rate": 0.00010948829141370338,
      "loss": 0.4667,
      "step": 2610
    },
    {
      "epoch": 0.45440749252048734,
      "grad_norm": 0.7274680733680725,
      "learning_rate": 0.00010914137033824806,
      "loss": 0.476,
      "step": 2620
    },
    {
      "epoch": 0.45614187226293196,
      "grad_norm": 0.507493257522583,
      "learning_rate": 0.00010879444926279271,
      "loss": 0.5139,
      "step": 2630
    },
    {
      "epoch": 0.45787625200537657,
      "grad_norm": 0.8098742961883545,
      "learning_rate": 0.00010844752818733739,
      "loss": 0.4519,
      "step": 2640
    },
    {
      "epoch": 0.4596106317478212,
      "grad_norm": 0.869736909866333,
      "learning_rate": 0.00010810060711188206,
      "loss": 0.4937,
      "step": 2650
    },
    {
      "epoch": 0.4613450114902658,
      "grad_norm": 0.6634869575500488,
      "learning_rate": 0.00010775368603642673,
      "loss": 0.492,
      "step": 2660
    },
    {
      "epoch": 0.4630793912327104,
      "grad_norm": 0.8101575374603271,
      "learning_rate": 0.00010740676496097138,
      "loss": 0.5323,
      "step": 2670
    },
    {
      "epoch": 0.464813770975155,
      "grad_norm": 0.6901437640190125,
      "learning_rate": 0.00010705984388551604,
      "loss": 0.47,
      "step": 2680
    },
    {
      "epoch": 0.46654815071759964,
      "grad_norm": 0.8503714203834534,
      "learning_rate": 0.00010671292281006071,
      "loss": 0.4867,
      "step": 2690
    },
    {
      "epoch": 0.4682825304600442,
      "grad_norm": 0.6553505659103394,
      "learning_rate": 0.00010636600173460538,
      "loss": 0.4924,
      "step": 2700
    },
    {
      "epoch": 0.4700169102024888,
      "grad_norm": 0.9692922830581665,
      "learning_rate": 0.00010601908065915004,
      "loss": 0.5204,
      "step": 2710
    },
    {
      "epoch": 0.4717512899449334,
      "grad_norm": 0.6416645050048828,
      "learning_rate": 0.00010567215958369472,
      "loss": 0.4651,
      "step": 2720
    },
    {
      "epoch": 0.47348566968737804,
      "grad_norm": 0.710706353187561,
      "learning_rate": 0.00010532523850823939,
      "loss": 0.482,
      "step": 2730
    },
    {
      "epoch": 0.47522004942982266,
      "grad_norm": 1.0412850379943848,
      "learning_rate": 0.00010497831743278406,
      "loss": 0.5152,
      "step": 2740
    },
    {
      "epoch": 0.47695442917226727,
      "grad_norm": 0.8035675883293152,
      "learning_rate": 0.00010463139635732871,
      "loss": 0.5317,
      "step": 2750
    },
    {
      "epoch": 0.4786888089147119,
      "grad_norm": 0.7659491300582886,
      "learning_rate": 0.00010428447528187337,
      "loss": 0.4673,
      "step": 2760
    },
    {
      "epoch": 0.4804231886571565,
      "grad_norm": 0.5645246505737305,
      "learning_rate": 0.00010393755420641804,
      "loss": 0.4743,
      "step": 2770
    },
    {
      "epoch": 0.4821575683996011,
      "grad_norm": 0.7067350149154663,
      "learning_rate": 0.00010359063313096271,
      "loss": 0.4874,
      "step": 2780
    },
    {
      "epoch": 0.4838919481420457,
      "grad_norm": 0.7320488095283508,
      "learning_rate": 0.00010324371205550739,
      "loss": 0.5201,
      "step": 2790
    },
    {
      "epoch": 0.4856263278844903,
      "grad_norm": 0.7882155179977417,
      "learning_rate": 0.00010289679098005204,
      "loss": 0.5101,
      "step": 2800
    },
    {
      "epoch": 0.4873607076269349,
      "grad_norm": 0.6543313264846802,
      "learning_rate": 0.00010254986990459672,
      "loss": 0.5313,
      "step": 2810
    },
    {
      "epoch": 0.4890950873693795,
      "grad_norm": 0.829975962638855,
      "learning_rate": 0.00010220294882914136,
      "loss": 0.4912,
      "step": 2820
    },
    {
      "epoch": 0.4908294671118241,
      "grad_norm": 0.7112954258918762,
      "learning_rate": 0.00010185602775368604,
      "loss": 0.5376,
      "step": 2830
    },
    {
      "epoch": 0.49256384685426874,
      "grad_norm": 0.7884327173233032,
      "learning_rate": 0.00010150910667823071,
      "loss": 0.4683,
      "step": 2840
    },
    {
      "epoch": 0.49429822659671335,
      "grad_norm": 0.6694056391716003,
      "learning_rate": 0.00010116218560277537,
      "loss": 0.4668,
      "step": 2850
    },
    {
      "epoch": 0.49603260633915797,
      "grad_norm": 0.7282833456993103,
      "learning_rate": 0.00010081526452732004,
      "loss": 0.5249,
      "step": 2860
    },
    {
      "epoch": 0.4977669860816026,
      "grad_norm": 1.0143007040023804,
      "learning_rate": 0.00010046834345186471,
      "loss": 0.5686,
      "step": 2870
    },
    {
      "epoch": 0.4995013658240472,
      "grad_norm": 0.6885628700256348,
      "learning_rate": 0.00010012142237640937,
      "loss": 0.509,
      "step": 2880
    },
    {
      "epoch": 0.5012357455664918,
      "grad_norm": 0.8896617293357849,
      "learning_rate": 9.977450130095403e-05,
      "loss": 0.4785,
      "step": 2890
    },
    {
      "epoch": 0.5029701253089364,
      "grad_norm": 0.6138171553611755,
      "learning_rate": 9.94275802254987e-05,
      "loss": 0.4617,
      "step": 2900
    },
    {
      "epoch": 0.504704505051381,
      "grad_norm": 0.6854972243309021,
      "learning_rate": 9.908065915004336e-05,
      "loss": 0.4987,
      "step": 2910
    },
    {
      "epoch": 0.5064388847938256,
      "grad_norm": 1.0716378688812256,
      "learning_rate": 9.873373807458804e-05,
      "loss": 0.54,
      "step": 2920
    },
    {
      "epoch": 0.5081732645362702,
      "grad_norm": 0.6891657114028931,
      "learning_rate": 9.83868169991327e-05,
      "loss": 0.5035,
      "step": 2930
    },
    {
      "epoch": 0.5099076442787148,
      "grad_norm": 0.792335569858551,
      "learning_rate": 9.803989592367737e-05,
      "loss": 0.5259,
      "step": 2940
    },
    {
      "epoch": 0.5116420240211594,
      "grad_norm": 0.6851321458816528,
      "learning_rate": 9.769297484822203e-05,
      "loss": 0.462,
      "step": 2950
    },
    {
      "epoch": 0.513376403763604,
      "grad_norm": 0.8769698739051819,
      "learning_rate": 9.73460537727667e-05,
      "loss": 0.4767,
      "step": 2960
    },
    {
      "epoch": 0.5151107835060487,
      "grad_norm": 0.8270881175994873,
      "learning_rate": 9.699913269731136e-05,
      "loss": 0.5827,
      "step": 2970
    },
    {
      "epoch": 0.5168451632484933,
      "grad_norm": 0.5648444890975952,
      "learning_rate": 9.665221162185603e-05,
      "loss": 0.4999,
      "step": 2980
    },
    {
      "epoch": 0.5185795429909379,
      "grad_norm": 0.7827973961830139,
      "learning_rate": 9.63052905464007e-05,
      "loss": 0.5013,
      "step": 2990
    },
    {
      "epoch": 0.5203139227333825,
      "grad_norm": 0.8935242891311646,
      "learning_rate": 9.595836947094537e-05,
      "loss": 0.5112,
      "step": 3000
    },
    {
      "epoch": 0.5220483024758271,
      "grad_norm": 1.0347651243209839,
      "learning_rate": 9.561144839549004e-05,
      "loss": 0.4857,
      "step": 3010
    },
    {
      "epoch": 0.5237826822182717,
      "grad_norm": 0.9614189863204956,
      "learning_rate": 9.52645273200347e-05,
      "loss": 0.5088,
      "step": 3020
    },
    {
      "epoch": 0.5255170619607163,
      "grad_norm": 0.5860079526901245,
      "learning_rate": 9.491760624457936e-05,
      "loss": 0.485,
      "step": 3030
    },
    {
      "epoch": 0.527251441703161,
      "grad_norm": 0.8865611553192139,
      "learning_rate": 9.457068516912403e-05,
      "loss": 0.5322,
      "step": 3040
    },
    {
      "epoch": 0.5289858214456055,
      "grad_norm": 0.6524754762649536,
      "learning_rate": 9.42237640936687e-05,
      "loss": 0.5097,
      "step": 3050
    },
    {
      "epoch": 0.5307202011880501,
      "grad_norm": 0.7257674336433411,
      "learning_rate": 9.387684301821336e-05,
      "loss": 0.4597,
      "step": 3060
    },
    {
      "epoch": 0.5324545809304947,
      "grad_norm": 0.7783998250961304,
      "learning_rate": 9.352992194275802e-05,
      "loss": 0.486,
      "step": 3070
    },
    {
      "epoch": 0.5341889606729393,
      "grad_norm": 0.7916003465652466,
      "learning_rate": 9.31830008673027e-05,
      "loss": 0.4829,
      "step": 3080
    },
    {
      "epoch": 0.5359233404153839,
      "grad_norm": 0.7303819060325623,
      "learning_rate": 9.283607979184737e-05,
      "loss": 0.5031,
      "step": 3090
    },
    {
      "epoch": 0.5376577201578285,
      "grad_norm": 0.679979145526886,
      "learning_rate": 9.248915871639203e-05,
      "loss": 0.5228,
      "step": 3100
    },
    {
      "epoch": 0.5393920999002731,
      "grad_norm": 0.7654536366462708,
      "learning_rate": 9.214223764093669e-05,
      "loss": 0.542,
      "step": 3110
    },
    {
      "epoch": 0.5411264796427178,
      "grad_norm": 0.7964155077934265,
      "learning_rate": 9.179531656548136e-05,
      "loss": 0.5284,
      "step": 3120
    },
    {
      "epoch": 0.5428608593851624,
      "grad_norm": 0.6652113795280457,
      "learning_rate": 9.144839549002603e-05,
      "loss": 0.4537,
      "step": 3130
    },
    {
      "epoch": 0.544595239127607,
      "grad_norm": 0.802629828453064,
      "learning_rate": 9.110147441457069e-05,
      "loss": 0.5507,
      "step": 3140
    },
    {
      "epoch": 0.5463296188700516,
      "grad_norm": 0.6851889491081238,
      "learning_rate": 9.075455333911535e-05,
      "loss": 0.4433,
      "step": 3150
    },
    {
      "epoch": 0.5480639986124962,
      "grad_norm": 0.9624145030975342,
      "learning_rate": 9.040763226366002e-05,
      "loss": 0.5036,
      "step": 3160
    },
    {
      "epoch": 0.5497983783549408,
      "grad_norm": 0.660849928855896,
      "learning_rate": 9.006071118820468e-05,
      "loss": 0.4688,
      "step": 3170
    },
    {
      "epoch": 0.5515327580973854,
      "grad_norm": 0.6898482441902161,
      "learning_rate": 8.971379011274936e-05,
      "loss": 0.4603,
      "step": 3180
    },
    {
      "epoch": 0.5532671378398301,
      "grad_norm": 0.9072810411453247,
      "learning_rate": 8.936686903729402e-05,
      "loss": 0.5262,
      "step": 3190
    },
    {
      "epoch": 0.5550015175822747,
      "grad_norm": 0.6928591132164001,
      "learning_rate": 8.901994796183869e-05,
      "loss": 0.5256,
      "step": 3200
    },
    {
      "epoch": 0.5567358973247193,
      "grad_norm": 0.8535751104354858,
      "learning_rate": 8.867302688638335e-05,
      "loss": 0.5588,
      "step": 3210
    },
    {
      "epoch": 0.5584702770671639,
      "grad_norm": 0.6776735186576843,
      "learning_rate": 8.832610581092802e-05,
      "loss": 0.4665,
      "step": 3220
    },
    {
      "epoch": 0.5602046568096085,
      "grad_norm": 0.7577744126319885,
      "learning_rate": 8.797918473547268e-05,
      "loss": 0.4907,
      "step": 3230
    },
    {
      "epoch": 0.561939036552053,
      "grad_norm": 0.7384517192840576,
      "learning_rate": 8.763226366001735e-05,
      "loss": 0.5017,
      "step": 3240
    },
    {
      "epoch": 0.5636734162944976,
      "grad_norm": 0.7364180684089661,
      "learning_rate": 8.728534258456201e-05,
      "loss": 0.503,
      "step": 3250
    },
    {
      "epoch": 0.5654077960369422,
      "grad_norm": 0.8019437789916992,
      "learning_rate": 8.693842150910668e-05,
      "loss": 0.5039,
      "step": 3260
    },
    {
      "epoch": 0.5671421757793869,
      "grad_norm": 0.6784868240356445,
      "learning_rate": 8.659150043365136e-05,
      "loss": 0.4862,
      "step": 3270
    },
    {
      "epoch": 0.5688765555218315,
      "grad_norm": 0.5539730191230774,
      "learning_rate": 8.6244579358196e-05,
      "loss": 0.4288,
      "step": 3280
    },
    {
      "epoch": 0.5706109352642761,
      "grad_norm": 0.6690143346786499,
      "learning_rate": 8.589765828274068e-05,
      "loss": 0.4956,
      "step": 3290
    },
    {
      "epoch": 0.5723453150067207,
      "grad_norm": 0.7114009857177734,
      "learning_rate": 8.555073720728535e-05,
      "loss": 0.5398,
      "step": 3300
    },
    {
      "epoch": 0.5740796947491653,
      "grad_norm": 0.8970579504966736,
      "learning_rate": 8.520381613183002e-05,
      "loss": 0.5371,
      "step": 3310
    },
    {
      "epoch": 0.5758140744916099,
      "grad_norm": 0.6885536909103394,
      "learning_rate": 8.485689505637468e-05,
      "loss": 0.4832,
      "step": 3320
    },
    {
      "epoch": 0.5775484542340545,
      "grad_norm": 0.7274964451789856,
      "learning_rate": 8.450997398091934e-05,
      "loss": 0.4888,
      "step": 3330
    },
    {
      "epoch": 0.5792828339764992,
      "grad_norm": 0.6426466703414917,
      "learning_rate": 8.416305290546401e-05,
      "loss": 0.4969,
      "step": 3340
    },
    {
      "epoch": 0.5810172137189438,
      "grad_norm": 0.8844702243804932,
      "learning_rate": 8.381613183000869e-05,
      "loss": 0.4728,
      "step": 3350
    },
    {
      "epoch": 0.5827515934613884,
      "grad_norm": 0.6469940543174744,
      "learning_rate": 8.346921075455335e-05,
      "loss": 0.4892,
      "step": 3360
    },
    {
      "epoch": 0.584485973203833,
      "grad_norm": 0.7298924922943115,
      "learning_rate": 8.3122289679098e-05,
      "loss": 0.4837,
      "step": 3370
    },
    {
      "epoch": 0.5862203529462776,
      "grad_norm": 0.7729203701019287,
      "learning_rate": 8.277536860364268e-05,
      "loss": 0.5287,
      "step": 3380
    },
    {
      "epoch": 0.5879547326887222,
      "grad_norm": 0.7402820587158203,
      "learning_rate": 8.242844752818734e-05,
      "loss": 0.4836,
      "step": 3390
    },
    {
      "epoch": 0.5896891124311668,
      "grad_norm": 0.767391562461853,
      "learning_rate": 8.208152645273201e-05,
      "loss": 0.5284,
      "step": 3400
    },
    {
      "epoch": 0.5914234921736115,
      "grad_norm": 0.6335683465003967,
      "learning_rate": 8.173460537727667e-05,
      "loss": 0.4823,
      "step": 3410
    },
    {
      "epoch": 0.5931578719160561,
      "grad_norm": 0.9344466328620911,
      "learning_rate": 8.138768430182134e-05,
      "loss": 0.5406,
      "step": 3420
    },
    {
      "epoch": 0.5948922516585007,
      "grad_norm": 0.7541022300720215,
      "learning_rate": 8.1040763226366e-05,
      "loss": 0.4465,
      "step": 3430
    },
    {
      "epoch": 0.5966266314009452,
      "grad_norm": 0.7589234709739685,
      "learning_rate": 8.069384215091067e-05,
      "loss": 0.5058,
      "step": 3440
    },
    {
      "epoch": 0.5983610111433898,
      "grad_norm": 0.9194411039352417,
      "learning_rate": 8.034692107545533e-05,
      "loss": 0.5536,
      "step": 3450
    },
    {
      "epoch": 0.6000953908858344,
      "grad_norm": 0.8817410469055176,
      "learning_rate": 8e-05,
      "loss": 0.4784,
      "step": 3460
    },
    {
      "epoch": 0.601829770628279,
      "grad_norm": 0.6843740940093994,
      "learning_rate": 7.965307892454467e-05,
      "loss": 0.5116,
      "step": 3470
    },
    {
      "epoch": 0.6035641503707236,
      "grad_norm": 0.9029726386070251,
      "learning_rate": 7.930615784908934e-05,
      "loss": 0.5525,
      "step": 3480
    },
    {
      "epoch": 0.6052985301131683,
      "grad_norm": 0.732714831829071,
      "learning_rate": 7.8959236773634e-05,
      "loss": 0.4897,
      "step": 3490
    },
    {
      "epoch": 0.6070329098556129,
      "grad_norm": 0.6723242402076721,
      "learning_rate": 7.861231569817867e-05,
      "loss": 0.5058,
      "step": 3500
    }
  ],
  "logging_steps": 10,
  "max_steps": 5765,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.8200993816576e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
