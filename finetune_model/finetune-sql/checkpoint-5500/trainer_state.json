{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9539088583445345,
  "eval_steps": 500,
  "global_step": 5500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0017343797424446083,
      "grad_norm": 2.5550849437713623,
      "learning_rate": 0.00019968777103209019,
      "loss": 3.0523,
      "step": 10
    },
    {
      "epoch": 0.0034687594848892165,
      "grad_norm": 2.0123374462127686,
      "learning_rate": 0.00019934084995663486,
      "loss": 2.3094,
      "step": 20
    },
    {
      "epoch": 0.005203139227333824,
      "grad_norm": 2.694957733154297,
      "learning_rate": 0.00019899392888117953,
      "loss": 1.7411,
      "step": 30
    },
    {
      "epoch": 0.006937518969778433,
      "grad_norm": 1.9005944728851318,
      "learning_rate": 0.0001986470078057242,
      "loss": 1.1989,
      "step": 40
    },
    {
      "epoch": 0.008671898712223042,
      "grad_norm": 1.4874471426010132,
      "learning_rate": 0.00019830008673026888,
      "loss": 1.0164,
      "step": 50
    },
    {
      "epoch": 0.010406278454667649,
      "grad_norm": 2.6214494705200195,
      "learning_rate": 0.00019795316565481355,
      "loss": 0.9067,
      "step": 60
    },
    {
      "epoch": 0.012140658197112257,
      "grad_norm": 1.266445279121399,
      "learning_rate": 0.00019760624457935822,
      "loss": 0.9224,
      "step": 70
    },
    {
      "epoch": 0.013875037939556866,
      "grad_norm": 1.247423529624939,
      "learning_rate": 0.0001972593235039029,
      "loss": 0.8334,
      "step": 80
    },
    {
      "epoch": 0.015609417682001475,
      "grad_norm": 1.0444533824920654,
      "learning_rate": 0.00019691240242844754,
      "loss": 0.8003,
      "step": 90
    },
    {
      "epoch": 0.017343797424446084,
      "grad_norm": 1.0636088848114014,
      "learning_rate": 0.0001965654813529922,
      "loss": 0.8069,
      "step": 100
    },
    {
      "epoch": 0.01907817716689069,
      "grad_norm": 1.1975089311599731,
      "learning_rate": 0.00019621856027753686,
      "loss": 0.8468,
      "step": 110
    },
    {
      "epoch": 0.020812556909335297,
      "grad_norm": 1.125996470451355,
      "learning_rate": 0.00019587163920208153,
      "loss": 0.7739,
      "step": 120
    },
    {
      "epoch": 0.022546936651779908,
      "grad_norm": 1.0888392925262451,
      "learning_rate": 0.0001955247181266262,
      "loss": 0.7945,
      "step": 130
    },
    {
      "epoch": 0.024281316394224515,
      "grad_norm": 1.4818414449691772,
      "learning_rate": 0.00019517779705117088,
      "loss": 0.7209,
      "step": 140
    },
    {
      "epoch": 0.026015696136669125,
      "grad_norm": 1.7991796731948853,
      "learning_rate": 0.00019483087597571555,
      "loss": 0.74,
      "step": 150
    },
    {
      "epoch": 0.027750075879113732,
      "grad_norm": 1.8334739208221436,
      "learning_rate": 0.0001944839549002602,
      "loss": 0.7401,
      "step": 160
    },
    {
      "epoch": 0.02948445562155834,
      "grad_norm": 1.1871280670166016,
      "learning_rate": 0.00019413703382480487,
      "loss": 0.6621,
      "step": 170
    },
    {
      "epoch": 0.03121883536400295,
      "grad_norm": 1.0494414567947388,
      "learning_rate": 0.00019379011274934952,
      "loss": 0.5911,
      "step": 180
    },
    {
      "epoch": 0.03295321510644756,
      "grad_norm": 0.973773181438446,
      "learning_rate": 0.0001934431916738942,
      "loss": 0.6231,
      "step": 190
    },
    {
      "epoch": 0.03468759484889217,
      "grad_norm": 1.3078233003616333,
      "learning_rate": 0.00019309627059843886,
      "loss": 0.5732,
      "step": 200
    },
    {
      "epoch": 0.036421974591336774,
      "grad_norm": 1.2161680459976196,
      "learning_rate": 0.00019274934952298353,
      "loss": 0.6369,
      "step": 210
    },
    {
      "epoch": 0.03815635433378138,
      "grad_norm": 1.6396287679672241,
      "learning_rate": 0.0001924024284475282,
      "loss": 0.625,
      "step": 220
    },
    {
      "epoch": 0.03989073407622599,
      "grad_norm": 1.4116144180297852,
      "learning_rate": 0.00019205550737207288,
      "loss": 0.647,
      "step": 230
    },
    {
      "epoch": 0.041625113818670595,
      "grad_norm": 1.0980229377746582,
      "learning_rate": 0.00019170858629661753,
      "loss": 0.5614,
      "step": 240
    },
    {
      "epoch": 0.04335949356111521,
      "grad_norm": 1.1962525844573975,
      "learning_rate": 0.0001913616652211622,
      "loss": 0.6344,
      "step": 250
    },
    {
      "epoch": 0.045093873303559816,
      "grad_norm": 1.2604451179504395,
      "learning_rate": 0.00019101474414570687,
      "loss": 0.5919,
      "step": 260
    },
    {
      "epoch": 0.04682825304600442,
      "grad_norm": 0.9544632434844971,
      "learning_rate": 0.00019066782307025152,
      "loss": 0.5926,
      "step": 270
    },
    {
      "epoch": 0.04856263278844903,
      "grad_norm": 1.094160795211792,
      "learning_rate": 0.0001903209019947962,
      "loss": 0.5788,
      "step": 280
    },
    {
      "epoch": 0.05029701253089364,
      "grad_norm": 1.0095820426940918,
      "learning_rate": 0.00018997398091934086,
      "loss": 0.6399,
      "step": 290
    },
    {
      "epoch": 0.05203139227333825,
      "grad_norm": 1.3148770332336426,
      "learning_rate": 0.00018962705984388554,
      "loss": 0.5493,
      "step": 300
    },
    {
      "epoch": 0.05376577201578286,
      "grad_norm": 1.2883634567260742,
      "learning_rate": 0.00018928013876843018,
      "loss": 0.5717,
      "step": 310
    },
    {
      "epoch": 0.055500151758227464,
      "grad_norm": 0.8932894468307495,
      "learning_rate": 0.00018893321769297485,
      "loss": 0.5656,
      "step": 320
    },
    {
      "epoch": 0.05723453150067207,
      "grad_norm": 1.0753910541534424,
      "learning_rate": 0.00018858629661751953,
      "loss": 0.594,
      "step": 330
    },
    {
      "epoch": 0.05896891124311668,
      "grad_norm": 0.753275454044342,
      "learning_rate": 0.0001882393755420642,
      "loss": 0.5478,
      "step": 340
    },
    {
      "epoch": 0.060703290985561285,
      "grad_norm": 1.0140023231506348,
      "learning_rate": 0.00018789245446660885,
      "loss": 0.5268,
      "step": 350
    },
    {
      "epoch": 0.0624376707280059,
      "grad_norm": 0.818980872631073,
      "learning_rate": 0.00018754553339115352,
      "loss": 0.566,
      "step": 360
    },
    {
      "epoch": 0.0641720504704505,
      "grad_norm": 1.1010839939117432,
      "learning_rate": 0.0001871986123156982,
      "loss": 0.6807,
      "step": 370
    },
    {
      "epoch": 0.06590643021289512,
      "grad_norm": 1.2425607442855835,
      "learning_rate": 0.00018685169124024284,
      "loss": 0.6306,
      "step": 380
    },
    {
      "epoch": 0.06764080995533972,
      "grad_norm": 1.155747413635254,
      "learning_rate": 0.0001865047701647875,
      "loss": 0.5633,
      "step": 390
    },
    {
      "epoch": 0.06937518969778433,
      "grad_norm": 1.2084158658981323,
      "learning_rate": 0.00018615784908933218,
      "loss": 0.5758,
      "step": 400
    },
    {
      "epoch": 0.07110956944022893,
      "grad_norm": 0.8114025592803955,
      "learning_rate": 0.00018581092801387686,
      "loss": 0.532,
      "step": 410
    },
    {
      "epoch": 0.07284394918267355,
      "grad_norm": 0.8228296637535095,
      "learning_rate": 0.00018546400693842153,
      "loss": 0.5447,
      "step": 420
    },
    {
      "epoch": 0.07457832892511815,
      "grad_norm": 0.9397292733192444,
      "learning_rate": 0.0001851170858629662,
      "loss": 0.5517,
      "step": 430
    },
    {
      "epoch": 0.07631270866756276,
      "grad_norm": 0.8269107937812805,
      "learning_rate": 0.00018477016478751085,
      "loss": 0.5935,
      "step": 440
    },
    {
      "epoch": 0.07804708841000738,
      "grad_norm": 0.958692729473114,
      "learning_rate": 0.00018442324371205552,
      "loss": 0.5456,
      "step": 450
    },
    {
      "epoch": 0.07978146815245198,
      "grad_norm": 0.9679394960403442,
      "learning_rate": 0.00018407632263660017,
      "loss": 0.5506,
      "step": 460
    },
    {
      "epoch": 0.08151584789489659,
      "grad_norm": 0.8142249584197998,
      "learning_rate": 0.00018372940156114484,
      "loss": 0.5826,
      "step": 470
    },
    {
      "epoch": 0.08325022763734119,
      "grad_norm": 1.1221299171447754,
      "learning_rate": 0.0001833824804856895,
      "loss": 0.599,
      "step": 480
    },
    {
      "epoch": 0.0849846073797858,
      "grad_norm": 0.8575493693351746,
      "learning_rate": 0.00018303555941023418,
      "loss": 0.6185,
      "step": 490
    },
    {
      "epoch": 0.08671898712223042,
      "grad_norm": 1.043362021446228,
      "learning_rate": 0.00018268863833477886,
      "loss": 0.5496,
      "step": 500
    },
    {
      "epoch": 0.08845336686467502,
      "grad_norm": 0.776316225528717,
      "learning_rate": 0.00018234171725932353,
      "loss": 0.5821,
      "step": 510
    },
    {
      "epoch": 0.09018774660711963,
      "grad_norm": 0.7581605315208435,
      "learning_rate": 0.00018199479618386818,
      "loss": 0.4986,
      "step": 520
    },
    {
      "epoch": 0.09192212634956423,
      "grad_norm": 1.2291282415390015,
      "learning_rate": 0.00018164787510841282,
      "loss": 0.5699,
      "step": 530
    },
    {
      "epoch": 0.09365650609200885,
      "grad_norm": 0.8632634878158569,
      "learning_rate": 0.0001813009540329575,
      "loss": 0.5643,
      "step": 540
    },
    {
      "epoch": 0.09539088583445346,
      "grad_norm": 1.301264762878418,
      "learning_rate": 0.00018095403295750217,
      "loss": 0.5133,
      "step": 550
    },
    {
      "epoch": 0.09712526557689806,
      "grad_norm": 1.0773627758026123,
      "learning_rate": 0.00018060711188204684,
      "loss": 0.5576,
      "step": 560
    },
    {
      "epoch": 0.09885964531934267,
      "grad_norm": 1.1195361614227295,
      "learning_rate": 0.0001802601908065915,
      "loss": 0.512,
      "step": 570
    },
    {
      "epoch": 0.10059402506178727,
      "grad_norm": 1.0149531364440918,
      "learning_rate": 0.00017991326973113619,
      "loss": 0.5615,
      "step": 580
    },
    {
      "epoch": 0.10232840480423189,
      "grad_norm": 0.7162327170372009,
      "learning_rate": 0.00017956634865568086,
      "loss": 0.5361,
      "step": 590
    },
    {
      "epoch": 0.1040627845466765,
      "grad_norm": 0.9201861023902893,
      "learning_rate": 0.0001792194275802255,
      "loss": 0.5411,
      "step": 600
    },
    {
      "epoch": 0.1057971642891211,
      "grad_norm": 0.9617904424667358,
      "learning_rate": 0.00017887250650477018,
      "loss": 0.5089,
      "step": 610
    },
    {
      "epoch": 0.10753154403156572,
      "grad_norm": 1.022149682044983,
      "learning_rate": 0.00017852558542931482,
      "loss": 0.6215,
      "step": 620
    },
    {
      "epoch": 0.10926592377401032,
      "grad_norm": 0.8238493204116821,
      "learning_rate": 0.0001781786643538595,
      "loss": 0.5451,
      "step": 630
    },
    {
      "epoch": 0.11100030351645493,
      "grad_norm": 0.8939248919487,
      "learning_rate": 0.00017783174327840417,
      "loss": 0.5524,
      "step": 640
    },
    {
      "epoch": 0.11273468325889954,
      "grad_norm": 0.8883143663406372,
      "learning_rate": 0.00017748482220294884,
      "loss": 0.5246,
      "step": 650
    },
    {
      "epoch": 0.11446906300134414,
      "grad_norm": 0.7424055933952332,
      "learning_rate": 0.00017713790112749352,
      "loss": 0.5263,
      "step": 660
    },
    {
      "epoch": 0.11620344274378876,
      "grad_norm": 1.2159603834152222,
      "learning_rate": 0.0001767909800520382,
      "loss": 0.5079,
      "step": 670
    },
    {
      "epoch": 0.11793782248623336,
      "grad_norm": 0.7574623227119446,
      "learning_rate": 0.00017644405897658283,
      "loss": 0.5281,
      "step": 680
    },
    {
      "epoch": 0.11967220222867797,
      "grad_norm": 0.792121946811676,
      "learning_rate": 0.0001760971379011275,
      "loss": 0.5008,
      "step": 690
    },
    {
      "epoch": 0.12140658197112257,
      "grad_norm": 0.8064789175987244,
      "learning_rate": 0.00017575021682567215,
      "loss": 0.5316,
      "step": 700
    },
    {
      "epoch": 0.12314096171356718,
      "grad_norm": 0.8532897233963013,
      "learning_rate": 0.00017540329575021683,
      "loss": 0.5311,
      "step": 710
    },
    {
      "epoch": 0.1248753414560118,
      "grad_norm": 0.7916345596313477,
      "learning_rate": 0.0001750563746747615,
      "loss": 0.5307,
      "step": 720
    },
    {
      "epoch": 0.1266097211984564,
      "grad_norm": 0.9886270761489868,
      "learning_rate": 0.00017470945359930617,
      "loss": 0.5523,
      "step": 730
    },
    {
      "epoch": 0.128344100940901,
      "grad_norm": 0.9201990365982056,
      "learning_rate": 0.00017436253252385084,
      "loss": 0.509,
      "step": 740
    },
    {
      "epoch": 0.13007848068334563,
      "grad_norm": 0.7614508271217346,
      "learning_rate": 0.0001740156114483955,
      "loss": 0.6164,
      "step": 750
    },
    {
      "epoch": 0.13181286042579024,
      "grad_norm": 0.7819229960441589,
      "learning_rate": 0.00017366869037294016,
      "loss": 0.529,
      "step": 760
    },
    {
      "epoch": 0.13354724016823483,
      "grad_norm": 0.9719189405441284,
      "learning_rate": 0.00017332176929748484,
      "loss": 0.5313,
      "step": 770
    },
    {
      "epoch": 0.13528161991067944,
      "grad_norm": 0.6589162349700928,
      "learning_rate": 0.0001729748482220295,
      "loss": 0.504,
      "step": 780
    },
    {
      "epoch": 0.13701599965312405,
      "grad_norm": 0.6838045120239258,
      "learning_rate": 0.00017262792714657415,
      "loss": 0.5136,
      "step": 790
    },
    {
      "epoch": 0.13875037939556867,
      "grad_norm": 0.7332388758659363,
      "learning_rate": 0.00017228100607111883,
      "loss": 0.5613,
      "step": 800
    },
    {
      "epoch": 0.14048475913801325,
      "grad_norm": 0.8705220818519592,
      "learning_rate": 0.0001719340849956635,
      "loss": 0.5142,
      "step": 810
    },
    {
      "epoch": 0.14221913888045787,
      "grad_norm": 1.0035444498062134,
      "learning_rate": 0.00017158716392020817,
      "loss": 0.5488,
      "step": 820
    },
    {
      "epoch": 0.14395351862290248,
      "grad_norm": 0.7737194299697876,
      "learning_rate": 0.00017124024284475282,
      "loss": 0.5355,
      "step": 830
    },
    {
      "epoch": 0.1456878983653471,
      "grad_norm": 0.7520883679389954,
      "learning_rate": 0.0001708933217692975,
      "loss": 0.573,
      "step": 840
    },
    {
      "epoch": 0.1474222781077917,
      "grad_norm": 0.8724210262298584,
      "learning_rate": 0.00017054640069384216,
      "loss": 0.5305,
      "step": 850
    },
    {
      "epoch": 0.1491566578502363,
      "grad_norm": 0.8224956393241882,
      "learning_rate": 0.00017019947961838684,
      "loss": 0.5538,
      "step": 860
    },
    {
      "epoch": 0.1508910375926809,
      "grad_norm": 0.8507670760154724,
      "learning_rate": 0.00016985255854293148,
      "loss": 0.5366,
      "step": 870
    },
    {
      "epoch": 0.15262541733512552,
      "grad_norm": 0.8243544697761536,
      "learning_rate": 0.00016950563746747616,
      "loss": 0.5069,
      "step": 880
    },
    {
      "epoch": 0.15435979707757014,
      "grad_norm": 0.71662437915802,
      "learning_rate": 0.00016915871639202083,
      "loss": 0.5319,
      "step": 890
    },
    {
      "epoch": 0.15609417682001475,
      "grad_norm": 0.7661607265472412,
      "learning_rate": 0.00016881179531656547,
      "loss": 0.5335,
      "step": 900
    },
    {
      "epoch": 0.15782855656245934,
      "grad_norm": 0.6027960181236267,
      "learning_rate": 0.00016846487424111015,
      "loss": 0.5156,
      "step": 910
    },
    {
      "epoch": 0.15956293630490395,
      "grad_norm": 0.9688888192176819,
      "learning_rate": 0.00016811795316565482,
      "loss": 0.5643,
      "step": 920
    },
    {
      "epoch": 0.16129731604734857,
      "grad_norm": 0.8623057007789612,
      "learning_rate": 0.0001677710320901995,
      "loss": 0.4927,
      "step": 930
    },
    {
      "epoch": 0.16303169578979318,
      "grad_norm": 0.8581346273422241,
      "learning_rate": 0.00016742411101474417,
      "loss": 0.5247,
      "step": 940
    },
    {
      "epoch": 0.1647660755322378,
      "grad_norm": 0.7837463021278381,
      "learning_rate": 0.00016707718993928884,
      "loss": 0.5767,
      "step": 950
    },
    {
      "epoch": 0.16650045527468238,
      "grad_norm": 0.7025507092475891,
      "learning_rate": 0.00016673026886383348,
      "loss": 0.5687,
      "step": 960
    },
    {
      "epoch": 0.168234835017127,
      "grad_norm": 0.6543734669685364,
      "learning_rate": 0.00016638334778837813,
      "loss": 0.5689,
      "step": 970
    },
    {
      "epoch": 0.1699692147595716,
      "grad_norm": 1.0180604457855225,
      "learning_rate": 0.0001660364267129228,
      "loss": 0.564,
      "step": 980
    },
    {
      "epoch": 0.17170359450201622,
      "grad_norm": 0.9559260606765747,
      "learning_rate": 0.00016568950563746748,
      "loss": 0.5924,
      "step": 990
    },
    {
      "epoch": 0.17343797424446084,
      "grad_norm": 0.6762255430221558,
      "learning_rate": 0.00016534258456201215,
      "loss": 0.5048,
      "step": 1000
    },
    {
      "epoch": 0.17517235398690542,
      "grad_norm": 0.8638314008712769,
      "learning_rate": 0.00016499566348655682,
      "loss": 0.5841,
      "step": 1010
    },
    {
      "epoch": 0.17690673372935004,
      "grad_norm": 0.9742166996002197,
      "learning_rate": 0.0001646487424111015,
      "loss": 0.5587,
      "step": 1020
    },
    {
      "epoch": 0.17864111347179465,
      "grad_norm": 0.9518930912017822,
      "learning_rate": 0.00016430182133564617,
      "loss": 0.5526,
      "step": 1030
    },
    {
      "epoch": 0.18037549321423926,
      "grad_norm": 1.1513153314590454,
      "learning_rate": 0.0001639549002601908,
      "loss": 0.5499,
      "step": 1040
    },
    {
      "epoch": 0.18210987295668388,
      "grad_norm": 0.8407158255577087,
      "learning_rate": 0.00016360797918473546,
      "loss": 0.5786,
      "step": 1050
    },
    {
      "epoch": 0.18384425269912846,
      "grad_norm": 0.8604046106338501,
      "learning_rate": 0.00016326105810928013,
      "loss": 0.5106,
      "step": 1060
    },
    {
      "epoch": 0.18557863244157308,
      "grad_norm": 0.6312768459320068,
      "learning_rate": 0.0001629141370338248,
      "loss": 0.5601,
      "step": 1070
    },
    {
      "epoch": 0.1873130121840177,
      "grad_norm": 0.8525477051734924,
      "learning_rate": 0.00016256721595836948,
      "loss": 0.5702,
      "step": 1080
    },
    {
      "epoch": 0.1890473919264623,
      "grad_norm": 0.8499556183815002,
      "learning_rate": 0.00016222029488291415,
      "loss": 0.5689,
      "step": 1090
    },
    {
      "epoch": 0.19078177166890692,
      "grad_norm": 0.9467098116874695,
      "learning_rate": 0.00016187337380745882,
      "loss": 0.5168,
      "step": 1100
    },
    {
      "epoch": 0.1925161514113515,
      "grad_norm": 0.8677939772605896,
      "learning_rate": 0.0001615264527320035,
      "loss": 0.5056,
      "step": 1110
    },
    {
      "epoch": 0.19425053115379612,
      "grad_norm": 0.8191261291503906,
      "learning_rate": 0.00016117953165654814,
      "loss": 0.5172,
      "step": 1120
    },
    {
      "epoch": 0.19598491089624073,
      "grad_norm": 0.9394680857658386,
      "learning_rate": 0.00016083261058109281,
      "loss": 0.5182,
      "step": 1130
    },
    {
      "epoch": 0.19771929063868535,
      "grad_norm": 0.9703214764595032,
      "learning_rate": 0.00016048568950563746,
      "loss": 0.4981,
      "step": 1140
    },
    {
      "epoch": 0.19945367038112996,
      "grad_norm": 0.7619735598564148,
      "learning_rate": 0.00016013876843018213,
      "loss": 0.5763,
      "step": 1150
    },
    {
      "epoch": 0.20118805012357455,
      "grad_norm": 1.0625637769699097,
      "learning_rate": 0.0001597918473547268,
      "loss": 0.5037,
      "step": 1160
    },
    {
      "epoch": 0.20292242986601916,
      "grad_norm": 0.7585030794143677,
      "learning_rate": 0.00015944492627927148,
      "loss": 0.5171,
      "step": 1170
    },
    {
      "epoch": 0.20465680960846377,
      "grad_norm": 0.7726263403892517,
      "learning_rate": 0.00015909800520381615,
      "loss": 0.54,
      "step": 1180
    },
    {
      "epoch": 0.2063911893509084,
      "grad_norm": 0.7178537249565125,
      "learning_rate": 0.00015875108412836082,
      "loss": 0.5195,
      "step": 1190
    },
    {
      "epoch": 0.208125569093353,
      "grad_norm": 0.9159913063049316,
      "learning_rate": 0.00015840416305290547,
      "loss": 0.5859,
      "step": 1200
    },
    {
      "epoch": 0.2098599488357976,
      "grad_norm": 0.8675339818000793,
      "learning_rate": 0.00015805724197745014,
      "loss": 0.5552,
      "step": 1210
    },
    {
      "epoch": 0.2115943285782422,
      "grad_norm": 0.8014780879020691,
      "learning_rate": 0.0001577103209019948,
      "loss": 0.4815,
      "step": 1220
    },
    {
      "epoch": 0.21332870832068682,
      "grad_norm": 0.8599198460578918,
      "learning_rate": 0.00015736339982653946,
      "loss": 0.5037,
      "step": 1230
    },
    {
      "epoch": 0.21506308806313143,
      "grad_norm": 0.775314211845398,
      "learning_rate": 0.00015701647875108413,
      "loss": 0.5641,
      "step": 1240
    },
    {
      "epoch": 0.21679746780557604,
      "grad_norm": 1.3312227725982666,
      "learning_rate": 0.0001566695576756288,
      "loss": 0.5015,
      "step": 1250
    },
    {
      "epoch": 0.21853184754802063,
      "grad_norm": 1.013002634048462,
      "learning_rate": 0.00015632263660017348,
      "loss": 0.5295,
      "step": 1260
    },
    {
      "epoch": 0.22026622729046524,
      "grad_norm": 0.618144154548645,
      "learning_rate": 0.00015597571552471813,
      "loss": 0.5206,
      "step": 1270
    },
    {
      "epoch": 0.22200060703290986,
      "grad_norm": 0.7005691528320312,
      "learning_rate": 0.0001556287944492628,
      "loss": 0.507,
      "step": 1280
    },
    {
      "epoch": 0.22373498677535447,
      "grad_norm": 0.9774540662765503,
      "learning_rate": 0.00015528187337380747,
      "loss": 0.5415,
      "step": 1290
    },
    {
      "epoch": 0.22546936651779909,
      "grad_norm": 0.8326419591903687,
      "learning_rate": 0.00015493495229835214,
      "loss": 0.5859,
      "step": 1300
    },
    {
      "epoch": 0.22720374626024367,
      "grad_norm": 0.7086894512176514,
      "learning_rate": 0.0001545880312228968,
      "loss": 0.4951,
      "step": 1310
    },
    {
      "epoch": 0.22893812600268829,
      "grad_norm": 0.8754698038101196,
      "learning_rate": 0.00015424111014744146,
      "loss": 0.5692,
      "step": 1320
    },
    {
      "epoch": 0.2306725057451329,
      "grad_norm": 0.7073501348495483,
      "learning_rate": 0.00015389418907198614,
      "loss": 0.5121,
      "step": 1330
    },
    {
      "epoch": 0.2324068854875775,
      "grad_norm": 0.7450106739997864,
      "learning_rate": 0.00015354726799653078,
      "loss": 0.5772,
      "step": 1340
    },
    {
      "epoch": 0.2341412652300221,
      "grad_norm": 1.057621955871582,
      "learning_rate": 0.00015320034692107545,
      "loss": 0.5487,
      "step": 1350
    },
    {
      "epoch": 0.2358756449724667,
      "grad_norm": 0.7074055671691895,
      "learning_rate": 0.00015285342584562013,
      "loss": 0.485,
      "step": 1360
    },
    {
      "epoch": 0.23761002471491133,
      "grad_norm": 0.7234088778495789,
      "learning_rate": 0.0001525065047701648,
      "loss": 0.5442,
      "step": 1370
    },
    {
      "epoch": 0.23934440445735594,
      "grad_norm": 0.8184717893600464,
      "learning_rate": 0.00015215958369470947,
      "loss": 0.5211,
      "step": 1380
    },
    {
      "epoch": 0.24107878419980056,
      "grad_norm": 0.8528016805648804,
      "learning_rate": 0.00015181266261925412,
      "loss": 0.5299,
      "step": 1390
    },
    {
      "epoch": 0.24281316394224514,
      "grad_norm": 0.9218398332595825,
      "learning_rate": 0.0001514657415437988,
      "loss": 0.5336,
      "step": 1400
    },
    {
      "epoch": 0.24454754368468976,
      "grad_norm": 0.9450495839118958,
      "learning_rate": 0.00015111882046834346,
      "loss": 0.4913,
      "step": 1410
    },
    {
      "epoch": 0.24628192342713437,
      "grad_norm": 0.8063280582427979,
      "learning_rate": 0.0001507718993928881,
      "loss": 0.4731,
      "step": 1420
    },
    {
      "epoch": 0.24801630316957898,
      "grad_norm": 0.7726646661758423,
      "learning_rate": 0.00015042497831743278,
      "loss": 0.582,
      "step": 1430
    },
    {
      "epoch": 0.2497506829120236,
      "grad_norm": 0.6898558735847473,
      "learning_rate": 0.00015007805724197746,
      "loss": 0.5459,
      "step": 1440
    },
    {
      "epoch": 0.2514850626544682,
      "grad_norm": 0.7302579879760742,
      "learning_rate": 0.00014973113616652213,
      "loss": 0.4509,
      "step": 1450
    },
    {
      "epoch": 0.2532194423969128,
      "grad_norm": 0.8079715967178345,
      "learning_rate": 0.0001493842150910668,
      "loss": 0.4924,
      "step": 1460
    },
    {
      "epoch": 0.2549538221393574,
      "grad_norm": 0.9184403419494629,
      "learning_rate": 0.00014903729401561148,
      "loss": 0.5242,
      "step": 1470
    },
    {
      "epoch": 0.256688201881802,
      "grad_norm": 0.9540342092514038,
      "learning_rate": 0.00014869037294015612,
      "loss": 0.5108,
      "step": 1480
    },
    {
      "epoch": 0.25842258162424664,
      "grad_norm": 0.662722647190094,
      "learning_rate": 0.00014834345186470077,
      "loss": 0.4707,
      "step": 1490
    },
    {
      "epoch": 0.26015696136669125,
      "grad_norm": 1.0262176990509033,
      "learning_rate": 0.00014799653078924544,
      "loss": 0.5443,
      "step": 1500
    },
    {
      "epoch": 0.26189134110913587,
      "grad_norm": 0.7653674483299255,
      "learning_rate": 0.0001476496097137901,
      "loss": 0.4961,
      "step": 1510
    },
    {
      "epoch": 0.2636257208515805,
      "grad_norm": 0.6642130017280579,
      "learning_rate": 0.00014730268863833479,
      "loss": 0.5682,
      "step": 1520
    },
    {
      "epoch": 0.26536010059402504,
      "grad_norm": 0.7625731825828552,
      "learning_rate": 0.00014695576756287946,
      "loss": 0.5445,
      "step": 1530
    },
    {
      "epoch": 0.26709448033646965,
      "grad_norm": 0.8662495613098145,
      "learning_rate": 0.00014660884648742413,
      "loss": 0.5393,
      "step": 1540
    },
    {
      "epoch": 0.26882886007891427,
      "grad_norm": 0.8010826706886292,
      "learning_rate": 0.0001462619254119688,
      "loss": 0.5276,
      "step": 1550
    },
    {
      "epoch": 0.2705632398213589,
      "grad_norm": 0.697375476360321,
      "learning_rate": 0.00014591500433651345,
      "loss": 0.514,
      "step": 1560
    },
    {
      "epoch": 0.2722976195638035,
      "grad_norm": 0.5982699990272522,
      "learning_rate": 0.0001455680832610581,
      "loss": 0.4839,
      "step": 1570
    },
    {
      "epoch": 0.2740319993062481,
      "grad_norm": 0.7269188165664673,
      "learning_rate": 0.00014522116218560277,
      "loss": 0.4315,
      "step": 1580
    },
    {
      "epoch": 0.2757663790486927,
      "grad_norm": 0.7482186555862427,
      "learning_rate": 0.00014487424111014744,
      "loss": 0.5392,
      "step": 1590
    },
    {
      "epoch": 0.27750075879113734,
      "grad_norm": 0.7533226609230042,
      "learning_rate": 0.00014452732003469211,
      "loss": 0.5058,
      "step": 1600
    },
    {
      "epoch": 0.27923513853358195,
      "grad_norm": 0.7971985340118408,
      "learning_rate": 0.0001441803989592368,
      "loss": 0.5496,
      "step": 1610
    },
    {
      "epoch": 0.2809695182760265,
      "grad_norm": 0.9086535573005676,
      "learning_rate": 0.00014383347788378146,
      "loss": 0.5231,
      "step": 1620
    },
    {
      "epoch": 0.2827038980184711,
      "grad_norm": 0.7624841332435608,
      "learning_rate": 0.00014348655680832613,
      "loss": 0.4796,
      "step": 1630
    },
    {
      "epoch": 0.28443827776091574,
      "grad_norm": 0.7093886137008667,
      "learning_rate": 0.00014313963573287078,
      "loss": 0.4985,
      "step": 1640
    },
    {
      "epoch": 0.28617265750336035,
      "grad_norm": 0.6750412583351135,
      "learning_rate": 0.00014279271465741545,
      "loss": 0.5532,
      "step": 1650
    },
    {
      "epoch": 0.28790703724580496,
      "grad_norm": 0.6476730704307556,
      "learning_rate": 0.0001424457935819601,
      "loss": 0.5441,
      "step": 1660
    },
    {
      "epoch": 0.2896414169882496,
      "grad_norm": 0.6679351925849915,
      "learning_rate": 0.00014209887250650477,
      "loss": 0.5186,
      "step": 1670
    },
    {
      "epoch": 0.2913757967306942,
      "grad_norm": 0.691450297832489,
      "learning_rate": 0.00014175195143104944,
      "loss": 0.4914,
      "step": 1680
    },
    {
      "epoch": 0.2931101764731388,
      "grad_norm": 0.7088834047317505,
      "learning_rate": 0.00014140503035559412,
      "loss": 0.4836,
      "step": 1690
    },
    {
      "epoch": 0.2948445562155834,
      "grad_norm": 0.7763418555259705,
      "learning_rate": 0.0001410581092801388,
      "loss": 0.5297,
      "step": 1700
    },
    {
      "epoch": 0.29657893595802803,
      "grad_norm": 0.9440441727638245,
      "learning_rate": 0.00014071118820468343,
      "loss": 0.5052,
      "step": 1710
    },
    {
      "epoch": 0.2983133157004726,
      "grad_norm": 0.9379427433013916,
      "learning_rate": 0.0001403642671292281,
      "loss": 0.5633,
      "step": 1720
    },
    {
      "epoch": 0.3000476954429172,
      "grad_norm": 0.6732260584831238,
      "learning_rate": 0.00014001734605377278,
      "loss": 0.5528,
      "step": 1730
    },
    {
      "epoch": 0.3017820751853618,
      "grad_norm": 0.7572551965713501,
      "learning_rate": 0.00013967042497831743,
      "loss": 0.5284,
      "step": 1740
    },
    {
      "epoch": 0.30351645492780643,
      "grad_norm": 0.8330501914024353,
      "learning_rate": 0.0001393235039028621,
      "loss": 0.5277,
      "step": 1750
    },
    {
      "epoch": 0.30525083467025105,
      "grad_norm": 0.7279008626937866,
      "learning_rate": 0.00013897658282740677,
      "loss": 0.4847,
      "step": 1760
    },
    {
      "epoch": 0.30698521441269566,
      "grad_norm": 0.9518227577209473,
      "learning_rate": 0.00013862966175195144,
      "loss": 0.501,
      "step": 1770
    },
    {
      "epoch": 0.3087195941551403,
      "grad_norm": 0.589831531047821,
      "learning_rate": 0.00013828274067649612,
      "loss": 0.4676,
      "step": 1780
    },
    {
      "epoch": 0.3104539738975849,
      "grad_norm": 0.5695339441299438,
      "learning_rate": 0.00013793581960104076,
      "loss": 0.5027,
      "step": 1790
    },
    {
      "epoch": 0.3121883536400295,
      "grad_norm": 0.6939374208450317,
      "learning_rate": 0.00013758889852558544,
      "loss": 0.5157,
      "step": 1800
    },
    {
      "epoch": 0.3139227333824741,
      "grad_norm": 0.7632051706314087,
      "learning_rate": 0.0001372419774501301,
      "loss": 0.5211,
      "step": 1810
    },
    {
      "epoch": 0.3156571131249187,
      "grad_norm": 0.7699695229530334,
      "learning_rate": 0.00013689505637467478,
      "loss": 0.4907,
      "step": 1820
    },
    {
      "epoch": 0.3173914928673633,
      "grad_norm": 0.7416660189628601,
      "learning_rate": 0.00013654813529921943,
      "loss": 0.5369,
      "step": 1830
    },
    {
      "epoch": 0.3191258726098079,
      "grad_norm": 0.7927618026733398,
      "learning_rate": 0.0001362012142237641,
      "loss": 0.5615,
      "step": 1840
    },
    {
      "epoch": 0.3208602523522525,
      "grad_norm": 0.808673083782196,
      "learning_rate": 0.00013585429314830877,
      "loss": 0.5227,
      "step": 1850
    },
    {
      "epoch": 0.32259463209469713,
      "grad_norm": 0.6342969536781311,
      "learning_rate": 0.00013550737207285342,
      "loss": 0.5363,
      "step": 1860
    },
    {
      "epoch": 0.32432901183714175,
      "grad_norm": 0.891576886177063,
      "learning_rate": 0.0001351604509973981,
      "loss": 0.5428,
      "step": 1870
    },
    {
      "epoch": 0.32606339157958636,
      "grad_norm": 0.9472002983093262,
      "learning_rate": 0.00013481352992194276,
      "loss": 0.558,
      "step": 1880
    },
    {
      "epoch": 0.327797771322031,
      "grad_norm": 0.724912703037262,
      "learning_rate": 0.00013446660884648744,
      "loss": 0.5343,
      "step": 1890
    },
    {
      "epoch": 0.3295321510644756,
      "grad_norm": 0.6124851107597351,
      "learning_rate": 0.0001341196877710321,
      "loss": 0.5214,
      "step": 1900
    },
    {
      "epoch": 0.3312665308069202,
      "grad_norm": 0.6415812969207764,
      "learning_rate": 0.00013377276669557676,
      "loss": 0.5232,
      "step": 1910
    },
    {
      "epoch": 0.33300091054936476,
      "grad_norm": 0.8251940608024597,
      "learning_rate": 0.00013342584562012143,
      "loss": 0.5193,
      "step": 1920
    },
    {
      "epoch": 0.3347352902918094,
      "grad_norm": 0.7508525848388672,
      "learning_rate": 0.0001330789245446661,
      "loss": 0.5051,
      "step": 1930
    },
    {
      "epoch": 0.336469670034254,
      "grad_norm": 1.266374111175537,
      "learning_rate": 0.00013273200346921075,
      "loss": 0.5421,
      "step": 1940
    },
    {
      "epoch": 0.3382040497766986,
      "grad_norm": 0.7218808531761169,
      "learning_rate": 0.00013238508239375542,
      "loss": 0.4701,
      "step": 1950
    },
    {
      "epoch": 0.3399384295191432,
      "grad_norm": 0.776216447353363,
      "learning_rate": 0.0001320381613183001,
      "loss": 0.4833,
      "step": 1960
    },
    {
      "epoch": 0.34167280926158783,
      "grad_norm": 0.6702186465263367,
      "learning_rate": 0.00013169124024284477,
      "loss": 0.5338,
      "step": 1970
    },
    {
      "epoch": 0.34340718900403244,
      "grad_norm": 0.6971157193183899,
      "learning_rate": 0.00013134431916738944,
      "loss": 0.5141,
      "step": 1980
    },
    {
      "epoch": 0.34514156874647706,
      "grad_norm": 1.0345960855484009,
      "learning_rate": 0.0001309973980919341,
      "loss": 0.515,
      "step": 1990
    },
    {
      "epoch": 0.34687594848892167,
      "grad_norm": 0.7123037576675415,
      "learning_rate": 0.00013065047701647876,
      "loss": 0.5251,
      "step": 2000
    },
    {
      "epoch": 0.3486103282313663,
      "grad_norm": 0.8493272066116333,
      "learning_rate": 0.0001303035559410234,
      "loss": 0.5,
      "step": 2010
    },
    {
      "epoch": 0.35034470797381084,
      "grad_norm": 0.6264200806617737,
      "learning_rate": 0.00012995663486556808,
      "loss": 0.5109,
      "step": 2020
    },
    {
      "epoch": 0.35207908771625546,
      "grad_norm": 0.7789064049720764,
      "learning_rate": 0.00012960971379011275,
      "loss": 0.5265,
      "step": 2030
    },
    {
      "epoch": 0.35381346745870007,
      "grad_norm": 1.215909719467163,
      "learning_rate": 0.00012926279271465742,
      "loss": 0.5071,
      "step": 2040
    },
    {
      "epoch": 0.3555478472011447,
      "grad_norm": 1.1748603582382202,
      "learning_rate": 0.0001289158716392021,
      "loss": 0.5101,
      "step": 2050
    },
    {
      "epoch": 0.3572822269435893,
      "grad_norm": 0.590299129486084,
      "learning_rate": 0.00012856895056374677,
      "loss": 0.4986,
      "step": 2060
    },
    {
      "epoch": 0.3590166066860339,
      "grad_norm": 0.878107488155365,
      "learning_rate": 0.00012822202948829144,
      "loss": 0.5399,
      "step": 2070
    },
    {
      "epoch": 0.3607509864284785,
      "grad_norm": 1.046035885810852,
      "learning_rate": 0.00012787510841283609,
      "loss": 0.5595,
      "step": 2080
    },
    {
      "epoch": 0.36248536617092314,
      "grad_norm": 1.0968817472457886,
      "learning_rate": 0.00012752818733738073,
      "loss": 0.5195,
      "step": 2090
    },
    {
      "epoch": 0.36421974591336775,
      "grad_norm": 0.8078921437263489,
      "learning_rate": 0.0001271812662619254,
      "loss": 0.5398,
      "step": 2100
    },
    {
      "epoch": 0.36595412565581237,
      "grad_norm": 0.6793740391731262,
      "learning_rate": 0.00012683434518647008,
      "loss": 0.5112,
      "step": 2110
    },
    {
      "epoch": 0.3676885053982569,
      "grad_norm": 0.6471167206764221,
      "learning_rate": 0.00012648742411101475,
      "loss": 0.4529,
      "step": 2120
    },
    {
      "epoch": 0.36942288514070154,
      "grad_norm": 1.1910035610198975,
      "learning_rate": 0.00012614050303555942,
      "loss": 0.5065,
      "step": 2130
    },
    {
      "epoch": 0.37115726488314615,
      "grad_norm": 0.8445138931274414,
      "learning_rate": 0.0001257935819601041,
      "loss": 0.5068,
      "step": 2140
    },
    {
      "epoch": 0.37289164462559077,
      "grad_norm": 0.6547709703445435,
      "learning_rate": 0.00012544666088464877,
      "loss": 0.4913,
      "step": 2150
    },
    {
      "epoch": 0.3746260243680354,
      "grad_norm": 0.5325044393539429,
      "learning_rate": 0.00012509973980919341,
      "loss": 0.5387,
      "step": 2160
    },
    {
      "epoch": 0.37636040411048,
      "grad_norm": 0.7426567077636719,
      "learning_rate": 0.0001247528187337381,
      "loss": 0.5446,
      "step": 2170
    },
    {
      "epoch": 0.3780947838529246,
      "grad_norm": 0.6493839025497437,
      "learning_rate": 0.00012440589765828273,
      "loss": 0.4734,
      "step": 2180
    },
    {
      "epoch": 0.3798291635953692,
      "grad_norm": 0.6491600275039673,
      "learning_rate": 0.0001240589765828274,
      "loss": 0.5204,
      "step": 2190
    },
    {
      "epoch": 0.38156354333781384,
      "grad_norm": 0.6509878039360046,
      "learning_rate": 0.00012371205550737208,
      "loss": 0.5059,
      "step": 2200
    },
    {
      "epoch": 0.3832979230802584,
      "grad_norm": 0.8043583035469055,
      "learning_rate": 0.00012336513443191675,
      "loss": 0.513,
      "step": 2210
    },
    {
      "epoch": 0.385032302822703,
      "grad_norm": 0.8371724486351013,
      "learning_rate": 0.00012301821335646143,
      "loss": 0.5115,
      "step": 2220
    },
    {
      "epoch": 0.3867666825651476,
      "grad_norm": 0.757968544960022,
      "learning_rate": 0.00012267129228100607,
      "loss": 0.5394,
      "step": 2230
    },
    {
      "epoch": 0.38850106230759224,
      "grad_norm": 0.8001652956008911,
      "learning_rate": 0.00012232437120555074,
      "loss": 0.5154,
      "step": 2240
    },
    {
      "epoch": 0.39023544205003685,
      "grad_norm": 0.6728013753890991,
      "learning_rate": 0.0001219774501300954,
      "loss": 0.5106,
      "step": 2250
    },
    {
      "epoch": 0.39196982179248147,
      "grad_norm": 0.8274458050727844,
      "learning_rate": 0.00012163052905464008,
      "loss": 0.4988,
      "step": 2260
    },
    {
      "epoch": 0.3937042015349261,
      "grad_norm": 0.8530777096748352,
      "learning_rate": 0.00012128360797918475,
      "loss": 0.5403,
      "step": 2270
    },
    {
      "epoch": 0.3954385812773707,
      "grad_norm": 0.794225811958313,
      "learning_rate": 0.00012093668690372941,
      "loss": 0.5223,
      "step": 2280
    },
    {
      "epoch": 0.3971729610198153,
      "grad_norm": 0.6251499652862549,
      "learning_rate": 0.00012058976582827408,
      "loss": 0.5006,
      "step": 2290
    },
    {
      "epoch": 0.3989073407622599,
      "grad_norm": 0.6972042918205261,
      "learning_rate": 0.00012024284475281873,
      "loss": 0.4906,
      "step": 2300
    },
    {
      "epoch": 0.4006417205047045,
      "grad_norm": 0.6007173657417297,
      "learning_rate": 0.0001198959236773634,
      "loss": 0.4604,
      "step": 2310
    },
    {
      "epoch": 0.4023761002471491,
      "grad_norm": 0.8842028379440308,
      "learning_rate": 0.00011954900260190807,
      "loss": 0.5141,
      "step": 2320
    },
    {
      "epoch": 0.4041104799895937,
      "grad_norm": 0.8369344472885132,
      "learning_rate": 0.00011920208152645273,
      "loss": 0.5325,
      "step": 2330
    },
    {
      "epoch": 0.4058448597320383,
      "grad_norm": 0.829085111618042,
      "learning_rate": 0.0001188551604509974,
      "loss": 0.5402,
      "step": 2340
    },
    {
      "epoch": 0.40757923947448294,
      "grad_norm": 0.8348305225372314,
      "learning_rate": 0.00011850823937554208,
      "loss": 0.5528,
      "step": 2350
    },
    {
      "epoch": 0.40931361921692755,
      "grad_norm": 0.879067599773407,
      "learning_rate": 0.00011816131830008674,
      "loss": 0.478,
      "step": 2360
    },
    {
      "epoch": 0.41104799895937216,
      "grad_norm": 0.6829871535301208,
      "learning_rate": 0.00011781439722463141,
      "loss": 0.5396,
      "step": 2370
    },
    {
      "epoch": 0.4127823787018168,
      "grad_norm": 0.7062913775444031,
      "learning_rate": 0.00011746747614917606,
      "loss": 0.503,
      "step": 2380
    },
    {
      "epoch": 0.4145167584442614,
      "grad_norm": 0.8982194662094116,
      "learning_rate": 0.00011712055507372073,
      "loss": 0.4695,
      "step": 2390
    },
    {
      "epoch": 0.416251138186706,
      "grad_norm": 0.5161424279212952,
      "learning_rate": 0.0001167736339982654,
      "loss": 0.5311,
      "step": 2400
    },
    {
      "epoch": 0.41798551792915056,
      "grad_norm": 0.8519994020462036,
      "learning_rate": 0.00011642671292281006,
      "loss": 0.489,
      "step": 2410
    },
    {
      "epoch": 0.4197198976715952,
      "grad_norm": 0.6411151885986328,
      "learning_rate": 0.00011607979184735473,
      "loss": 0.4943,
      "step": 2420
    },
    {
      "epoch": 0.4214542774140398,
      "grad_norm": 0.571524977684021,
      "learning_rate": 0.0001157328707718994,
      "loss": 0.4747,
      "step": 2430
    },
    {
      "epoch": 0.4231886571564844,
      "grad_norm": 0.7930188775062561,
      "learning_rate": 0.00011538594969644408,
      "loss": 0.5654,
      "step": 2440
    },
    {
      "epoch": 0.424923036898929,
      "grad_norm": 0.6249622106552124,
      "learning_rate": 0.00011503902862098872,
      "loss": 0.5306,
      "step": 2450
    },
    {
      "epoch": 0.42665741664137363,
      "grad_norm": 0.694672167301178,
      "learning_rate": 0.00011469210754553338,
      "loss": 0.5114,
      "step": 2460
    },
    {
      "epoch": 0.42839179638381825,
      "grad_norm": 0.8747013807296753,
      "learning_rate": 0.00011434518647007806,
      "loss": 0.5157,
      "step": 2470
    },
    {
      "epoch": 0.43012617612626286,
      "grad_norm": 0.6652283668518066,
      "learning_rate": 0.00011399826539462273,
      "loss": 0.5061,
      "step": 2480
    },
    {
      "epoch": 0.4318605558687075,
      "grad_norm": 0.7100988030433655,
      "learning_rate": 0.0001136513443191674,
      "loss": 0.4947,
      "step": 2490
    },
    {
      "epoch": 0.4335949356111521,
      "grad_norm": 0.9096623063087463,
      "learning_rate": 0.00011330442324371206,
      "loss": 0.4819,
      "step": 2500
    },
    {
      "epoch": 0.43532931535359665,
      "grad_norm": 0.8663245439529419,
      "learning_rate": 0.00011295750216825673,
      "loss": 0.5197,
      "step": 2510
    },
    {
      "epoch": 0.43706369509604126,
      "grad_norm": 0.8671486377716064,
      "learning_rate": 0.00011261058109280141,
      "loss": 0.5025,
      "step": 2520
    },
    {
      "epoch": 0.4387980748384859,
      "grad_norm": 0.8190407752990723,
      "learning_rate": 0.00011226366001734605,
      "loss": 0.5023,
      "step": 2530
    },
    {
      "epoch": 0.4405324545809305,
      "grad_norm": 0.6053672432899475,
      "learning_rate": 0.00011191673894189073,
      "loss": 0.5745,
      "step": 2540
    },
    {
      "epoch": 0.4422668343233751,
      "grad_norm": 0.8161375522613525,
      "learning_rate": 0.00011156981786643539,
      "loss": 0.4808,
      "step": 2550
    },
    {
      "epoch": 0.4440012140658197,
      "grad_norm": 0.9839289784431458,
      "learning_rate": 0.00011122289679098006,
      "loss": 0.5264,
      "step": 2560
    },
    {
      "epoch": 0.44573559380826433,
      "grad_norm": 0.6492114067077637,
      "learning_rate": 0.00011087597571552473,
      "loss": 0.4893,
      "step": 2570
    },
    {
      "epoch": 0.44746997355070894,
      "grad_norm": 0.8308883309364319,
      "learning_rate": 0.00011052905464006939,
      "loss": 0.5326,
      "step": 2580
    },
    {
      "epoch": 0.44920435329315356,
      "grad_norm": 0.902975857257843,
      "learning_rate": 0.00011018213356461406,
      "loss": 0.4667,
      "step": 2590
    },
    {
      "epoch": 0.45093873303559817,
      "grad_norm": 0.6316888928413391,
      "learning_rate": 0.00010983521248915871,
      "loss": 0.5458,
      "step": 2600
    },
    {
      "epoch": 0.45267311277804273,
      "grad_norm": 0.6769781112670898,
      "learning_rate": 0.00010948829141370338,
      "loss": 0.4667,
      "step": 2610
    },
    {
      "epoch": 0.45440749252048734,
      "grad_norm": 0.7274680733680725,
      "learning_rate": 0.00010914137033824806,
      "loss": 0.476,
      "step": 2620
    },
    {
      "epoch": 0.45614187226293196,
      "grad_norm": 0.507493257522583,
      "learning_rate": 0.00010879444926279271,
      "loss": 0.5139,
      "step": 2630
    },
    {
      "epoch": 0.45787625200537657,
      "grad_norm": 0.8098742961883545,
      "learning_rate": 0.00010844752818733739,
      "loss": 0.4519,
      "step": 2640
    },
    {
      "epoch": 0.4596106317478212,
      "grad_norm": 0.869736909866333,
      "learning_rate": 0.00010810060711188206,
      "loss": 0.4937,
      "step": 2650
    },
    {
      "epoch": 0.4613450114902658,
      "grad_norm": 0.6634869575500488,
      "learning_rate": 0.00010775368603642673,
      "loss": 0.492,
      "step": 2660
    },
    {
      "epoch": 0.4630793912327104,
      "grad_norm": 0.8101575374603271,
      "learning_rate": 0.00010740676496097138,
      "loss": 0.5323,
      "step": 2670
    },
    {
      "epoch": 0.464813770975155,
      "grad_norm": 0.6901437640190125,
      "learning_rate": 0.00010705984388551604,
      "loss": 0.47,
      "step": 2680
    },
    {
      "epoch": 0.46654815071759964,
      "grad_norm": 0.8503714203834534,
      "learning_rate": 0.00010671292281006071,
      "loss": 0.4867,
      "step": 2690
    },
    {
      "epoch": 0.4682825304600442,
      "grad_norm": 0.6553505659103394,
      "learning_rate": 0.00010636600173460538,
      "loss": 0.4924,
      "step": 2700
    },
    {
      "epoch": 0.4700169102024888,
      "grad_norm": 0.9692922830581665,
      "learning_rate": 0.00010601908065915004,
      "loss": 0.5204,
      "step": 2710
    },
    {
      "epoch": 0.4717512899449334,
      "grad_norm": 0.6416645050048828,
      "learning_rate": 0.00010567215958369472,
      "loss": 0.4651,
      "step": 2720
    },
    {
      "epoch": 0.47348566968737804,
      "grad_norm": 0.710706353187561,
      "learning_rate": 0.00010532523850823939,
      "loss": 0.482,
      "step": 2730
    },
    {
      "epoch": 0.47522004942982266,
      "grad_norm": 1.0412850379943848,
      "learning_rate": 0.00010497831743278406,
      "loss": 0.5152,
      "step": 2740
    },
    {
      "epoch": 0.47695442917226727,
      "grad_norm": 0.8035675883293152,
      "learning_rate": 0.00010463139635732871,
      "loss": 0.5317,
      "step": 2750
    },
    {
      "epoch": 0.4786888089147119,
      "grad_norm": 0.7659491300582886,
      "learning_rate": 0.00010428447528187337,
      "loss": 0.4673,
      "step": 2760
    },
    {
      "epoch": 0.4804231886571565,
      "grad_norm": 0.5645246505737305,
      "learning_rate": 0.00010393755420641804,
      "loss": 0.4743,
      "step": 2770
    },
    {
      "epoch": 0.4821575683996011,
      "grad_norm": 0.7067350149154663,
      "learning_rate": 0.00010359063313096271,
      "loss": 0.4874,
      "step": 2780
    },
    {
      "epoch": 0.4838919481420457,
      "grad_norm": 0.7320488095283508,
      "learning_rate": 0.00010324371205550739,
      "loss": 0.5201,
      "step": 2790
    },
    {
      "epoch": 0.4856263278844903,
      "grad_norm": 0.7882155179977417,
      "learning_rate": 0.00010289679098005204,
      "loss": 0.5101,
      "step": 2800
    },
    {
      "epoch": 0.4873607076269349,
      "grad_norm": 0.6543313264846802,
      "learning_rate": 0.00010254986990459672,
      "loss": 0.5313,
      "step": 2810
    },
    {
      "epoch": 0.4890950873693795,
      "grad_norm": 0.829975962638855,
      "learning_rate": 0.00010220294882914136,
      "loss": 0.4912,
      "step": 2820
    },
    {
      "epoch": 0.4908294671118241,
      "grad_norm": 0.7112954258918762,
      "learning_rate": 0.00010185602775368604,
      "loss": 0.5376,
      "step": 2830
    },
    {
      "epoch": 0.49256384685426874,
      "grad_norm": 0.7884327173233032,
      "learning_rate": 0.00010150910667823071,
      "loss": 0.4683,
      "step": 2840
    },
    {
      "epoch": 0.49429822659671335,
      "grad_norm": 0.6694056391716003,
      "learning_rate": 0.00010116218560277537,
      "loss": 0.4668,
      "step": 2850
    },
    {
      "epoch": 0.49603260633915797,
      "grad_norm": 0.7282833456993103,
      "learning_rate": 0.00010081526452732004,
      "loss": 0.5249,
      "step": 2860
    },
    {
      "epoch": 0.4977669860816026,
      "grad_norm": 1.0143007040023804,
      "learning_rate": 0.00010046834345186471,
      "loss": 0.5686,
      "step": 2870
    },
    {
      "epoch": 0.4995013658240472,
      "grad_norm": 0.6885628700256348,
      "learning_rate": 0.00010012142237640937,
      "loss": 0.509,
      "step": 2880
    },
    {
      "epoch": 0.5012357455664918,
      "grad_norm": 0.8896617293357849,
      "learning_rate": 9.977450130095403e-05,
      "loss": 0.4785,
      "step": 2890
    },
    {
      "epoch": 0.5029701253089364,
      "grad_norm": 0.6138171553611755,
      "learning_rate": 9.94275802254987e-05,
      "loss": 0.4617,
      "step": 2900
    },
    {
      "epoch": 0.504704505051381,
      "grad_norm": 0.6854972243309021,
      "learning_rate": 9.908065915004336e-05,
      "loss": 0.4987,
      "step": 2910
    },
    {
      "epoch": 0.5064388847938256,
      "grad_norm": 1.0716378688812256,
      "learning_rate": 9.873373807458804e-05,
      "loss": 0.54,
      "step": 2920
    },
    {
      "epoch": 0.5081732645362702,
      "grad_norm": 0.6891657114028931,
      "learning_rate": 9.83868169991327e-05,
      "loss": 0.5035,
      "step": 2930
    },
    {
      "epoch": 0.5099076442787148,
      "grad_norm": 0.792335569858551,
      "learning_rate": 9.803989592367737e-05,
      "loss": 0.5259,
      "step": 2940
    },
    {
      "epoch": 0.5116420240211594,
      "grad_norm": 0.6851321458816528,
      "learning_rate": 9.769297484822203e-05,
      "loss": 0.462,
      "step": 2950
    },
    {
      "epoch": 0.513376403763604,
      "grad_norm": 0.8769698739051819,
      "learning_rate": 9.73460537727667e-05,
      "loss": 0.4767,
      "step": 2960
    },
    {
      "epoch": 0.5151107835060487,
      "grad_norm": 0.8270881175994873,
      "learning_rate": 9.699913269731136e-05,
      "loss": 0.5827,
      "step": 2970
    },
    {
      "epoch": 0.5168451632484933,
      "grad_norm": 0.5648444890975952,
      "learning_rate": 9.665221162185603e-05,
      "loss": 0.4999,
      "step": 2980
    },
    {
      "epoch": 0.5185795429909379,
      "grad_norm": 0.7827973961830139,
      "learning_rate": 9.63052905464007e-05,
      "loss": 0.5013,
      "step": 2990
    },
    {
      "epoch": 0.5203139227333825,
      "grad_norm": 0.8935242891311646,
      "learning_rate": 9.595836947094537e-05,
      "loss": 0.5112,
      "step": 3000
    },
    {
      "epoch": 0.5220483024758271,
      "grad_norm": 1.0347651243209839,
      "learning_rate": 9.561144839549004e-05,
      "loss": 0.4857,
      "step": 3010
    },
    {
      "epoch": 0.5237826822182717,
      "grad_norm": 0.9614189863204956,
      "learning_rate": 9.52645273200347e-05,
      "loss": 0.5088,
      "step": 3020
    },
    {
      "epoch": 0.5255170619607163,
      "grad_norm": 0.5860079526901245,
      "learning_rate": 9.491760624457936e-05,
      "loss": 0.485,
      "step": 3030
    },
    {
      "epoch": 0.527251441703161,
      "grad_norm": 0.8865611553192139,
      "learning_rate": 9.457068516912403e-05,
      "loss": 0.5322,
      "step": 3040
    },
    {
      "epoch": 0.5289858214456055,
      "grad_norm": 0.6524754762649536,
      "learning_rate": 9.42237640936687e-05,
      "loss": 0.5097,
      "step": 3050
    },
    {
      "epoch": 0.5307202011880501,
      "grad_norm": 0.7257674336433411,
      "learning_rate": 9.387684301821336e-05,
      "loss": 0.4597,
      "step": 3060
    },
    {
      "epoch": 0.5324545809304947,
      "grad_norm": 0.7783998250961304,
      "learning_rate": 9.352992194275802e-05,
      "loss": 0.486,
      "step": 3070
    },
    {
      "epoch": 0.5341889606729393,
      "grad_norm": 0.7916003465652466,
      "learning_rate": 9.31830008673027e-05,
      "loss": 0.4829,
      "step": 3080
    },
    {
      "epoch": 0.5359233404153839,
      "grad_norm": 0.7303819060325623,
      "learning_rate": 9.283607979184737e-05,
      "loss": 0.5031,
      "step": 3090
    },
    {
      "epoch": 0.5376577201578285,
      "grad_norm": 0.679979145526886,
      "learning_rate": 9.248915871639203e-05,
      "loss": 0.5228,
      "step": 3100
    },
    {
      "epoch": 0.5393920999002731,
      "grad_norm": 0.7654536366462708,
      "learning_rate": 9.214223764093669e-05,
      "loss": 0.542,
      "step": 3110
    },
    {
      "epoch": 0.5411264796427178,
      "grad_norm": 0.7964155077934265,
      "learning_rate": 9.179531656548136e-05,
      "loss": 0.5284,
      "step": 3120
    },
    {
      "epoch": 0.5428608593851624,
      "grad_norm": 0.6652113795280457,
      "learning_rate": 9.144839549002603e-05,
      "loss": 0.4537,
      "step": 3130
    },
    {
      "epoch": 0.544595239127607,
      "grad_norm": 0.802629828453064,
      "learning_rate": 9.110147441457069e-05,
      "loss": 0.5507,
      "step": 3140
    },
    {
      "epoch": 0.5463296188700516,
      "grad_norm": 0.6851889491081238,
      "learning_rate": 9.075455333911535e-05,
      "loss": 0.4433,
      "step": 3150
    },
    {
      "epoch": 0.5480639986124962,
      "grad_norm": 0.9624145030975342,
      "learning_rate": 9.040763226366002e-05,
      "loss": 0.5036,
      "step": 3160
    },
    {
      "epoch": 0.5497983783549408,
      "grad_norm": 0.660849928855896,
      "learning_rate": 9.006071118820468e-05,
      "loss": 0.4688,
      "step": 3170
    },
    {
      "epoch": 0.5515327580973854,
      "grad_norm": 0.6898482441902161,
      "learning_rate": 8.971379011274936e-05,
      "loss": 0.4603,
      "step": 3180
    },
    {
      "epoch": 0.5532671378398301,
      "grad_norm": 0.9072810411453247,
      "learning_rate": 8.936686903729402e-05,
      "loss": 0.5262,
      "step": 3190
    },
    {
      "epoch": 0.5550015175822747,
      "grad_norm": 0.6928591132164001,
      "learning_rate": 8.901994796183869e-05,
      "loss": 0.5256,
      "step": 3200
    },
    {
      "epoch": 0.5567358973247193,
      "grad_norm": 0.8535751104354858,
      "learning_rate": 8.867302688638335e-05,
      "loss": 0.5588,
      "step": 3210
    },
    {
      "epoch": 0.5584702770671639,
      "grad_norm": 0.6776735186576843,
      "learning_rate": 8.832610581092802e-05,
      "loss": 0.4665,
      "step": 3220
    },
    {
      "epoch": 0.5602046568096085,
      "grad_norm": 0.7577744126319885,
      "learning_rate": 8.797918473547268e-05,
      "loss": 0.4907,
      "step": 3230
    },
    {
      "epoch": 0.561939036552053,
      "grad_norm": 0.7384517192840576,
      "learning_rate": 8.763226366001735e-05,
      "loss": 0.5017,
      "step": 3240
    },
    {
      "epoch": 0.5636734162944976,
      "grad_norm": 0.7364180684089661,
      "learning_rate": 8.728534258456201e-05,
      "loss": 0.503,
      "step": 3250
    },
    {
      "epoch": 0.5654077960369422,
      "grad_norm": 0.8019437789916992,
      "learning_rate": 8.693842150910668e-05,
      "loss": 0.5039,
      "step": 3260
    },
    {
      "epoch": 0.5671421757793869,
      "grad_norm": 0.6784868240356445,
      "learning_rate": 8.659150043365136e-05,
      "loss": 0.4862,
      "step": 3270
    },
    {
      "epoch": 0.5688765555218315,
      "grad_norm": 0.5539730191230774,
      "learning_rate": 8.6244579358196e-05,
      "loss": 0.4288,
      "step": 3280
    },
    {
      "epoch": 0.5706109352642761,
      "grad_norm": 0.6690143346786499,
      "learning_rate": 8.589765828274068e-05,
      "loss": 0.4956,
      "step": 3290
    },
    {
      "epoch": 0.5723453150067207,
      "grad_norm": 0.7114009857177734,
      "learning_rate": 8.555073720728535e-05,
      "loss": 0.5398,
      "step": 3300
    },
    {
      "epoch": 0.5740796947491653,
      "grad_norm": 0.8970579504966736,
      "learning_rate": 8.520381613183002e-05,
      "loss": 0.5371,
      "step": 3310
    },
    {
      "epoch": 0.5758140744916099,
      "grad_norm": 0.6885536909103394,
      "learning_rate": 8.485689505637468e-05,
      "loss": 0.4832,
      "step": 3320
    },
    {
      "epoch": 0.5775484542340545,
      "grad_norm": 0.7274964451789856,
      "learning_rate": 8.450997398091934e-05,
      "loss": 0.4888,
      "step": 3330
    },
    {
      "epoch": 0.5792828339764992,
      "grad_norm": 0.6426466703414917,
      "learning_rate": 8.416305290546401e-05,
      "loss": 0.4969,
      "step": 3340
    },
    {
      "epoch": 0.5810172137189438,
      "grad_norm": 0.8844702243804932,
      "learning_rate": 8.381613183000869e-05,
      "loss": 0.4728,
      "step": 3350
    },
    {
      "epoch": 0.5827515934613884,
      "grad_norm": 0.6469940543174744,
      "learning_rate": 8.346921075455335e-05,
      "loss": 0.4892,
      "step": 3360
    },
    {
      "epoch": 0.584485973203833,
      "grad_norm": 0.7298924922943115,
      "learning_rate": 8.3122289679098e-05,
      "loss": 0.4837,
      "step": 3370
    },
    {
      "epoch": 0.5862203529462776,
      "grad_norm": 0.7729203701019287,
      "learning_rate": 8.277536860364268e-05,
      "loss": 0.5287,
      "step": 3380
    },
    {
      "epoch": 0.5879547326887222,
      "grad_norm": 0.7402820587158203,
      "learning_rate": 8.242844752818734e-05,
      "loss": 0.4836,
      "step": 3390
    },
    {
      "epoch": 0.5896891124311668,
      "grad_norm": 0.767391562461853,
      "learning_rate": 8.208152645273201e-05,
      "loss": 0.5284,
      "step": 3400
    },
    {
      "epoch": 0.5914234921736115,
      "grad_norm": 0.6335683465003967,
      "learning_rate": 8.173460537727667e-05,
      "loss": 0.4823,
      "step": 3410
    },
    {
      "epoch": 0.5931578719160561,
      "grad_norm": 0.9344466328620911,
      "learning_rate": 8.138768430182134e-05,
      "loss": 0.5406,
      "step": 3420
    },
    {
      "epoch": 0.5948922516585007,
      "grad_norm": 0.7541022300720215,
      "learning_rate": 8.1040763226366e-05,
      "loss": 0.4465,
      "step": 3430
    },
    {
      "epoch": 0.5966266314009452,
      "grad_norm": 0.7589234709739685,
      "learning_rate": 8.069384215091067e-05,
      "loss": 0.5058,
      "step": 3440
    },
    {
      "epoch": 0.5983610111433898,
      "grad_norm": 0.9194411039352417,
      "learning_rate": 8.034692107545533e-05,
      "loss": 0.5536,
      "step": 3450
    },
    {
      "epoch": 0.6000953908858344,
      "grad_norm": 0.8817410469055176,
      "learning_rate": 8e-05,
      "loss": 0.4784,
      "step": 3460
    },
    {
      "epoch": 0.601829770628279,
      "grad_norm": 0.6843740940093994,
      "learning_rate": 7.965307892454467e-05,
      "loss": 0.5116,
      "step": 3470
    },
    {
      "epoch": 0.6035641503707236,
      "grad_norm": 0.9029726386070251,
      "learning_rate": 7.930615784908934e-05,
      "loss": 0.5525,
      "step": 3480
    },
    {
      "epoch": 0.6052985301131683,
      "grad_norm": 0.732714831829071,
      "learning_rate": 7.8959236773634e-05,
      "loss": 0.4897,
      "step": 3490
    },
    {
      "epoch": 0.6070329098556129,
      "grad_norm": 0.6723242402076721,
      "learning_rate": 7.861231569817867e-05,
      "loss": 0.5058,
      "step": 3500
    },
    {
      "epoch": 0.6087672895980575,
      "grad_norm": 0.6994788646697998,
      "learning_rate": 7.826539462272333e-05,
      "loss": 0.4995,
      "step": 3510
    },
    {
      "epoch": 0.6105016693405021,
      "grad_norm": 0.6824237108230591,
      "learning_rate": 7.7918473547268e-05,
      "loss": 0.4717,
      "step": 3520
    },
    {
      "epoch": 0.6122360490829467,
      "grad_norm": 0.843199610710144,
      "learning_rate": 7.757155247181268e-05,
      "loss": 0.5692,
      "step": 3530
    },
    {
      "epoch": 0.6139704288253913,
      "grad_norm": 0.826997697353363,
      "learning_rate": 7.722463139635732e-05,
      "loss": 0.5459,
      "step": 3540
    },
    {
      "epoch": 0.6157048085678359,
      "grad_norm": 0.6666121482849121,
      "learning_rate": 7.6877710320902e-05,
      "loss": 0.5326,
      "step": 3550
    },
    {
      "epoch": 0.6174391883102806,
      "grad_norm": 0.7250948548316956,
      "learning_rate": 7.653078924544667e-05,
      "loss": 0.5243,
      "step": 3560
    },
    {
      "epoch": 0.6191735680527252,
      "grad_norm": 0.7301869988441467,
      "learning_rate": 7.618386816999134e-05,
      "loss": 0.4904,
      "step": 3570
    },
    {
      "epoch": 0.6209079477951698,
      "grad_norm": 0.714796781539917,
      "learning_rate": 7.583694709453599e-05,
      "loss": 0.4903,
      "step": 3580
    },
    {
      "epoch": 0.6226423275376144,
      "grad_norm": 0.626663327217102,
      "learning_rate": 7.549002601908066e-05,
      "loss": 0.5006,
      "step": 3590
    },
    {
      "epoch": 0.624376707280059,
      "grad_norm": 1.0991190671920776,
      "learning_rate": 7.514310494362533e-05,
      "loss": 0.5199,
      "step": 3600
    },
    {
      "epoch": 0.6261110870225036,
      "grad_norm": 0.6549398899078369,
      "learning_rate": 7.479618386817e-05,
      "loss": 0.4907,
      "step": 3610
    },
    {
      "epoch": 0.6278454667649482,
      "grad_norm": 0.9323130249977112,
      "learning_rate": 7.444926279271466e-05,
      "loss": 0.5317,
      "step": 3620
    },
    {
      "epoch": 0.6295798465073928,
      "grad_norm": 0.6142722964286804,
      "learning_rate": 7.410234171725932e-05,
      "loss": 0.4695,
      "step": 3630
    },
    {
      "epoch": 0.6313142262498374,
      "grad_norm": 0.8893114328384399,
      "learning_rate": 7.3755420641804e-05,
      "loss": 0.538,
      "step": 3640
    },
    {
      "epoch": 0.633048605992282,
      "grad_norm": 0.8043056726455688,
      "learning_rate": 7.340849956634866e-05,
      "loss": 0.4934,
      "step": 3650
    },
    {
      "epoch": 0.6347829857347266,
      "grad_norm": 0.8908848166465759,
      "learning_rate": 7.306157849089333e-05,
      "loss": 0.506,
      "step": 3660
    },
    {
      "epoch": 0.6365173654771712,
      "grad_norm": 0.7442833185195923,
      "learning_rate": 7.271465741543799e-05,
      "loss": 0.5162,
      "step": 3670
    },
    {
      "epoch": 0.6382517452196158,
      "grad_norm": 0.951408863067627,
      "learning_rate": 7.236773633998266e-05,
      "loss": 0.4919,
      "step": 3680
    },
    {
      "epoch": 0.6399861249620604,
      "grad_norm": 0.7257530689239502,
      "learning_rate": 7.202081526452732e-05,
      "loss": 0.5115,
      "step": 3690
    },
    {
      "epoch": 0.641720504704505,
      "grad_norm": 0.7162337899208069,
      "learning_rate": 7.167389418907199e-05,
      "loss": 0.5215,
      "step": 3700
    },
    {
      "epoch": 0.6434548844469496,
      "grad_norm": 0.6392345428466797,
      "learning_rate": 7.132697311361665e-05,
      "loss": 0.5226,
      "step": 3710
    },
    {
      "epoch": 0.6451892641893943,
      "grad_norm": 0.9209622144699097,
      "learning_rate": 7.098005203816132e-05,
      "loss": 0.4836,
      "step": 3720
    },
    {
      "epoch": 0.6469236439318389,
      "grad_norm": 0.7000786662101746,
      "learning_rate": 7.063313096270598e-05,
      "loss": 0.486,
      "step": 3730
    },
    {
      "epoch": 0.6486580236742835,
      "grad_norm": 0.7657500505447388,
      "learning_rate": 7.028620988725066e-05,
      "loss": 0.4335,
      "step": 3740
    },
    {
      "epoch": 0.6503924034167281,
      "grad_norm": 0.7919231057167053,
      "learning_rate": 6.993928881179532e-05,
      "loss": 0.5205,
      "step": 3750
    },
    {
      "epoch": 0.6521267831591727,
      "grad_norm": 0.9674562215805054,
      "learning_rate": 6.959236773633998e-05,
      "loss": 0.5208,
      "step": 3760
    },
    {
      "epoch": 0.6538611629016173,
      "grad_norm": 0.6693699955940247,
      "learning_rate": 6.924544666088465e-05,
      "loss": 0.4806,
      "step": 3770
    },
    {
      "epoch": 0.655595542644062,
      "grad_norm": 0.6634300351142883,
      "learning_rate": 6.889852558542932e-05,
      "loss": 0.4639,
      "step": 3780
    },
    {
      "epoch": 0.6573299223865066,
      "grad_norm": 0.8897411823272705,
      "learning_rate": 6.8551604509974e-05,
      "loss": 0.4712,
      "step": 3790
    },
    {
      "epoch": 0.6590643021289512,
      "grad_norm": 0.7494673132896423,
      "learning_rate": 6.820468343451864e-05,
      "loss": 0.5228,
      "step": 3800
    },
    {
      "epoch": 0.6607986818713958,
      "grad_norm": 0.719573974609375,
      "learning_rate": 6.785776235906331e-05,
      "loss": 0.5014,
      "step": 3810
    },
    {
      "epoch": 0.6625330616138404,
      "grad_norm": 0.6425529718399048,
      "learning_rate": 6.751084128360799e-05,
      "loss": 0.5333,
      "step": 3820
    },
    {
      "epoch": 0.6642674413562849,
      "grad_norm": 0.7441994547843933,
      "learning_rate": 6.716392020815266e-05,
      "loss": 0.4919,
      "step": 3830
    },
    {
      "epoch": 0.6660018210987295,
      "grad_norm": 0.691160261631012,
      "learning_rate": 6.68169991326973e-05,
      "loss": 0.4374,
      "step": 3840
    },
    {
      "epoch": 0.6677362008411741,
      "grad_norm": 0.7182109355926514,
      "learning_rate": 6.647007805724198e-05,
      "loss": 0.5502,
      "step": 3850
    },
    {
      "epoch": 0.6694705805836187,
      "grad_norm": 0.8867943286895752,
      "learning_rate": 6.612315698178665e-05,
      "loss": 0.5029,
      "step": 3860
    },
    {
      "epoch": 0.6712049603260634,
      "grad_norm": 0.8977904915809631,
      "learning_rate": 6.577623590633132e-05,
      "loss": 0.4057,
      "step": 3870
    },
    {
      "epoch": 0.672939340068508,
      "grad_norm": 0.7653469443321228,
      "learning_rate": 6.542931483087598e-05,
      "loss": 0.4954,
      "step": 3880
    },
    {
      "epoch": 0.6746737198109526,
      "grad_norm": 0.6987360119819641,
      "learning_rate": 6.508239375542064e-05,
      "loss": 0.4897,
      "step": 3890
    },
    {
      "epoch": 0.6764080995533972,
      "grad_norm": 0.8377981185913086,
      "learning_rate": 6.473547267996531e-05,
      "loss": 0.4968,
      "step": 3900
    },
    {
      "epoch": 0.6781424792958418,
      "grad_norm": 0.709409236907959,
      "learning_rate": 6.438855160450997e-05,
      "loss": 0.4931,
      "step": 3910
    },
    {
      "epoch": 0.6798768590382864,
      "grad_norm": 0.723003625869751,
      "learning_rate": 6.404163052905465e-05,
      "loss": 0.4611,
      "step": 3920
    },
    {
      "epoch": 0.681611238780731,
      "grad_norm": 0.7740470767021179,
      "learning_rate": 6.36947094535993e-05,
      "loss": 0.5326,
      "step": 3930
    },
    {
      "epoch": 0.6833456185231757,
      "grad_norm": 0.7179240584373474,
      "learning_rate": 6.334778837814398e-05,
      "loss": 0.5212,
      "step": 3940
    },
    {
      "epoch": 0.6850799982656203,
      "grad_norm": 0.796444296836853,
      "learning_rate": 6.300086730268864e-05,
      "loss": 0.5382,
      "step": 3950
    },
    {
      "epoch": 0.6868143780080649,
      "grad_norm": 0.7465384602546692,
      "learning_rate": 6.265394622723331e-05,
      "loss": 0.4919,
      "step": 3960
    },
    {
      "epoch": 0.6885487577505095,
      "grad_norm": 0.9967429041862488,
      "learning_rate": 6.230702515177797e-05,
      "loss": 0.523,
      "step": 3970
    },
    {
      "epoch": 0.6902831374929541,
      "grad_norm": 0.9116987586021423,
      "learning_rate": 6.196010407632264e-05,
      "loss": 0.511,
      "step": 3980
    },
    {
      "epoch": 0.6920175172353987,
      "grad_norm": 0.7429752945899963,
      "learning_rate": 6.16131830008673e-05,
      "loss": 0.5199,
      "step": 3990
    },
    {
      "epoch": 0.6937518969778433,
      "grad_norm": 0.9735521078109741,
      "learning_rate": 6.126626192541198e-05,
      "loss": 0.5105,
      "step": 4000
    },
    {
      "epoch": 0.695486276720288,
      "grad_norm": 0.6554557085037231,
      "learning_rate": 6.091934084995664e-05,
      "loss": 0.5075,
      "step": 4010
    },
    {
      "epoch": 0.6972206564627326,
      "grad_norm": 0.5208843946456909,
      "learning_rate": 6.05724197745013e-05,
      "loss": 0.4771,
      "step": 4020
    },
    {
      "epoch": 0.6989550362051771,
      "grad_norm": 0.845792293548584,
      "learning_rate": 6.022549869904597e-05,
      "loss": 0.511,
      "step": 4030
    },
    {
      "epoch": 0.7006894159476217,
      "grad_norm": 0.7309414744377136,
      "learning_rate": 5.987857762359064e-05,
      "loss": 0.478,
      "step": 4040
    },
    {
      "epoch": 0.7024237956900663,
      "grad_norm": 0.5740442276000977,
      "learning_rate": 5.9531656548135306e-05,
      "loss": 0.4849,
      "step": 4050
    },
    {
      "epoch": 0.7041581754325109,
      "grad_norm": 0.8917330503463745,
      "learning_rate": 5.9184735472679965e-05,
      "loss": 0.5024,
      "step": 4060
    },
    {
      "epoch": 0.7058925551749555,
      "grad_norm": 0.6808280348777771,
      "learning_rate": 5.883781439722463e-05,
      "loss": 0.4884,
      "step": 4070
    },
    {
      "epoch": 0.7076269349174001,
      "grad_norm": 0.6114906668663025,
      "learning_rate": 5.8490893321769304e-05,
      "loss": 0.4488,
      "step": 4080
    },
    {
      "epoch": 0.7093613146598448,
      "grad_norm": 0.9022036790847778,
      "learning_rate": 5.814397224631397e-05,
      "loss": 0.4724,
      "step": 4090
    },
    {
      "epoch": 0.7110956944022894,
      "grad_norm": 0.7327365279197693,
      "learning_rate": 5.779705117085863e-05,
      "loss": 0.5151,
      "step": 4100
    },
    {
      "epoch": 0.712830074144734,
      "grad_norm": 0.9359877109527588,
      "learning_rate": 5.7450130095403296e-05,
      "loss": 0.5475,
      "step": 4110
    },
    {
      "epoch": 0.7145644538871786,
      "grad_norm": 0.7511871457099915,
      "learning_rate": 5.710320901994797e-05,
      "loss": 0.5113,
      "step": 4120
    },
    {
      "epoch": 0.7162988336296232,
      "grad_norm": 0.6830766797065735,
      "learning_rate": 5.675628794449263e-05,
      "loss": 0.5255,
      "step": 4130
    },
    {
      "epoch": 0.7180332133720678,
      "grad_norm": 0.6947246193885803,
      "learning_rate": 5.6409366869037294e-05,
      "loss": 0.4736,
      "step": 4140
    },
    {
      "epoch": 0.7197675931145124,
      "grad_norm": 0.6000739932060242,
      "learning_rate": 5.606244579358197e-05,
      "loss": 0.518,
      "step": 4150
    },
    {
      "epoch": 0.721501972856957,
      "grad_norm": 0.656043529510498,
      "learning_rate": 5.571552471812663e-05,
      "loss": 0.502,
      "step": 4160
    },
    {
      "epoch": 0.7232363525994017,
      "grad_norm": 0.7165887355804443,
      "learning_rate": 5.536860364267129e-05,
      "loss": 0.4808,
      "step": 4170
    },
    {
      "epoch": 0.7249707323418463,
      "grad_norm": 0.8682636022567749,
      "learning_rate": 5.502168256721596e-05,
      "loss": 0.4983,
      "step": 4180
    },
    {
      "epoch": 0.7267051120842909,
      "grad_norm": 0.7699916362762451,
      "learning_rate": 5.467476149176063e-05,
      "loss": 0.5014,
      "step": 4190
    },
    {
      "epoch": 0.7284394918267355,
      "grad_norm": 0.6882349848747253,
      "learning_rate": 5.43278404163053e-05,
      "loss": 0.4324,
      "step": 4200
    },
    {
      "epoch": 0.7301738715691801,
      "grad_norm": 0.6152682304382324,
      "learning_rate": 5.3980919340849956e-05,
      "loss": 0.503,
      "step": 4210
    },
    {
      "epoch": 0.7319082513116247,
      "grad_norm": 0.9010840058326721,
      "learning_rate": 5.363399826539462e-05,
      "loss": 0.4764,
      "step": 4220
    },
    {
      "epoch": 0.7336426310540692,
      "grad_norm": 0.7620449066162109,
      "learning_rate": 5.3287077189939295e-05,
      "loss": 0.4883,
      "step": 4230
    },
    {
      "epoch": 0.7353770107965139,
      "grad_norm": 0.8463456034660339,
      "learning_rate": 5.2940156114483955e-05,
      "loss": 0.4884,
      "step": 4240
    },
    {
      "epoch": 0.7371113905389585,
      "grad_norm": 0.61029052734375,
      "learning_rate": 5.259323503902862e-05,
      "loss": 0.5302,
      "step": 4250
    },
    {
      "epoch": 0.7388457702814031,
      "grad_norm": 0.685082197189331,
      "learning_rate": 5.224631396357329e-05,
      "loss": 0.4849,
      "step": 4260
    },
    {
      "epoch": 0.7405801500238477,
      "grad_norm": 0.652250349521637,
      "learning_rate": 5.189939288811796e-05,
      "loss": 0.4393,
      "step": 4270
    },
    {
      "epoch": 0.7423145297662923,
      "grad_norm": 0.8760392069816589,
      "learning_rate": 5.155247181266262e-05,
      "loss": 0.5216,
      "step": 4280
    },
    {
      "epoch": 0.7440489095087369,
      "grad_norm": 0.7406121492385864,
      "learning_rate": 5.1205550737207285e-05,
      "loss": 0.4626,
      "step": 4290
    },
    {
      "epoch": 0.7457832892511815,
      "grad_norm": 0.7644434571266174,
      "learning_rate": 5.085862966175196e-05,
      "loss": 0.5251,
      "step": 4300
    },
    {
      "epoch": 0.7475176689936261,
      "grad_norm": 0.8223152756690979,
      "learning_rate": 5.0511708586296624e-05,
      "loss": 0.5004,
      "step": 4310
    },
    {
      "epoch": 0.7492520487360708,
      "grad_norm": 0.8714511394500732,
      "learning_rate": 5.0164787510841283e-05,
      "loss": 0.5296,
      "step": 4320
    },
    {
      "epoch": 0.7509864284785154,
      "grad_norm": 0.7739254236221313,
      "learning_rate": 4.981786643538595e-05,
      "loss": 0.4823,
      "step": 4330
    },
    {
      "epoch": 0.75272080822096,
      "grad_norm": 0.9742996096611023,
      "learning_rate": 4.947094535993062e-05,
      "loss": 0.4981,
      "step": 4340
    },
    {
      "epoch": 0.7544551879634046,
      "grad_norm": 0.7709847092628479,
      "learning_rate": 4.912402428447528e-05,
      "loss": 0.5057,
      "step": 4350
    },
    {
      "epoch": 0.7561895677058492,
      "grad_norm": 0.6730232834815979,
      "learning_rate": 4.877710320901995e-05,
      "loss": 0.4544,
      "step": 4360
    },
    {
      "epoch": 0.7579239474482938,
      "grad_norm": 1.1857497692108154,
      "learning_rate": 4.8430182133564614e-05,
      "loss": 0.4506,
      "step": 4370
    },
    {
      "epoch": 0.7596583271907384,
      "grad_norm": 0.7638524770736694,
      "learning_rate": 4.808326105810928e-05,
      "loss": 0.5148,
      "step": 4380
    },
    {
      "epoch": 0.7613927069331831,
      "grad_norm": 0.7400924563407898,
      "learning_rate": 4.7736339982653946e-05,
      "loss": 0.501,
      "step": 4390
    },
    {
      "epoch": 0.7631270866756277,
      "grad_norm": 0.7668944001197815,
      "learning_rate": 4.738941890719861e-05,
      "loss": 0.5288,
      "step": 4400
    },
    {
      "epoch": 0.7648614664180723,
      "grad_norm": 0.7910577654838562,
      "learning_rate": 4.7042497831743285e-05,
      "loss": 0.4764,
      "step": 4410
    },
    {
      "epoch": 0.7665958461605168,
      "grad_norm": 0.978735625743866,
      "learning_rate": 4.6695576756287944e-05,
      "loss": 0.5373,
      "step": 4420
    },
    {
      "epoch": 0.7683302259029614,
      "grad_norm": 0.6238415241241455,
      "learning_rate": 4.634865568083262e-05,
      "loss": 0.4838,
      "step": 4430
    },
    {
      "epoch": 0.770064605645406,
      "grad_norm": 0.6120663285255432,
      "learning_rate": 4.6001734605377277e-05,
      "loss": 0.4832,
      "step": 4440
    },
    {
      "epoch": 0.7717989853878506,
      "grad_norm": 0.634356677532196,
      "learning_rate": 4.565481352992195e-05,
      "loss": 0.5559,
      "step": 4450
    },
    {
      "epoch": 0.7735333651302952,
      "grad_norm": 0.8580872416496277,
      "learning_rate": 4.530789245446661e-05,
      "loss": 0.538,
      "step": 4460
    },
    {
      "epoch": 0.7752677448727399,
      "grad_norm": 0.7465367913246155,
      "learning_rate": 4.496097137901128e-05,
      "loss": 0.4753,
      "step": 4470
    },
    {
      "epoch": 0.7770021246151845,
      "grad_norm": 0.9016647934913635,
      "learning_rate": 4.461405030355594e-05,
      "loss": 0.4897,
      "step": 4480
    },
    {
      "epoch": 0.7787365043576291,
      "grad_norm": 0.7122774720191956,
      "learning_rate": 4.426712922810061e-05,
      "loss": 0.5223,
      "step": 4490
    },
    {
      "epoch": 0.7804708841000737,
      "grad_norm": 0.6932346820831299,
      "learning_rate": 4.392020815264527e-05,
      "loss": 0.4775,
      "step": 4500
    },
    {
      "epoch": 0.7822052638425183,
      "grad_norm": 0.737436830997467,
      "learning_rate": 4.357328707718994e-05,
      "loss": 0.484,
      "step": 4510
    },
    {
      "epoch": 0.7839396435849629,
      "grad_norm": 0.6981858015060425,
      "learning_rate": 4.3226366001734605e-05,
      "loss": 0.4631,
      "step": 4520
    },
    {
      "epoch": 0.7856740233274075,
      "grad_norm": 0.7607622146606445,
      "learning_rate": 4.287944492627927e-05,
      "loss": 0.4779,
      "step": 4530
    },
    {
      "epoch": 0.7874084030698522,
      "grad_norm": 0.9364786744117737,
      "learning_rate": 4.2532523850823944e-05,
      "loss": 0.506,
      "step": 4540
    },
    {
      "epoch": 0.7891427828122968,
      "grad_norm": 0.6938275694847107,
      "learning_rate": 4.2185602775368604e-05,
      "loss": 0.4751,
      "step": 4550
    },
    {
      "epoch": 0.7908771625547414,
      "grad_norm": 0.877493143081665,
      "learning_rate": 4.1838681699913276e-05,
      "loss": 0.4801,
      "step": 4560
    },
    {
      "epoch": 0.792611542297186,
      "grad_norm": 0.6949759721755981,
      "learning_rate": 4.1491760624457936e-05,
      "loss": 0.5481,
      "step": 4570
    },
    {
      "epoch": 0.7943459220396306,
      "grad_norm": 0.6622966527938843,
      "learning_rate": 4.114483954900261e-05,
      "loss": 0.5547,
      "step": 4580
    },
    {
      "epoch": 0.7960803017820752,
      "grad_norm": 0.879961371421814,
      "learning_rate": 4.079791847354727e-05,
      "loss": 0.5675,
      "step": 4590
    },
    {
      "epoch": 0.7978146815245198,
      "grad_norm": 0.758886456489563,
      "learning_rate": 4.045099739809194e-05,
      "loss": 0.5088,
      "step": 4600
    },
    {
      "epoch": 0.7995490612669645,
      "grad_norm": 0.6361307501792908,
      "learning_rate": 4.01040763226366e-05,
      "loss": 0.5008,
      "step": 4610
    },
    {
      "epoch": 0.801283441009409,
      "grad_norm": 0.889106035232544,
      "learning_rate": 3.9757155247181266e-05,
      "loss": 0.5041,
      "step": 4620
    },
    {
      "epoch": 0.8030178207518536,
      "grad_norm": 0.7618629336357117,
      "learning_rate": 3.941023417172593e-05,
      "loss": 0.515,
      "step": 4630
    },
    {
      "epoch": 0.8047522004942982,
      "grad_norm": 0.5827241539955139,
      "learning_rate": 3.90633130962706e-05,
      "loss": 0.5252,
      "step": 4640
    },
    {
      "epoch": 0.8064865802367428,
      "grad_norm": 0.9398325681686401,
      "learning_rate": 3.8716392020815264e-05,
      "loss": 0.5315,
      "step": 4650
    },
    {
      "epoch": 0.8082209599791874,
      "grad_norm": 0.6772662997245789,
      "learning_rate": 3.836947094535993e-05,
      "loss": 0.4584,
      "step": 4660
    },
    {
      "epoch": 0.809955339721632,
      "grad_norm": 0.710666835308075,
      "learning_rate": 3.80225498699046e-05,
      "loss": 0.498,
      "step": 4670
    },
    {
      "epoch": 0.8116897194640766,
      "grad_norm": 0.8197211027145386,
      "learning_rate": 3.767562879444926e-05,
      "loss": 0.5383,
      "step": 4680
    },
    {
      "epoch": 0.8134240992065213,
      "grad_norm": 0.6585163474082947,
      "learning_rate": 3.7328707718993936e-05,
      "loss": 0.5015,
      "step": 4690
    },
    {
      "epoch": 0.8151584789489659,
      "grad_norm": 0.796556293964386,
      "learning_rate": 3.6981786643538595e-05,
      "loss": 0.451,
      "step": 4700
    },
    {
      "epoch": 0.8168928586914105,
      "grad_norm": 0.6910567879676819,
      "learning_rate": 3.663486556808327e-05,
      "loss": 0.487,
      "step": 4710
    },
    {
      "epoch": 0.8186272384338551,
      "grad_norm": 0.7569940686225891,
      "learning_rate": 3.628794449262793e-05,
      "loss": 0.4776,
      "step": 4720
    },
    {
      "epoch": 0.8203616181762997,
      "grad_norm": 0.7448247075080872,
      "learning_rate": 3.594102341717259e-05,
      "loss": 0.4867,
      "step": 4730
    },
    {
      "epoch": 0.8220959979187443,
      "grad_norm": 0.9073430299758911,
      "learning_rate": 3.559410234171726e-05,
      "loss": 0.5217,
      "step": 4740
    },
    {
      "epoch": 0.8238303776611889,
      "grad_norm": 0.7423551678657532,
      "learning_rate": 3.5247181266261925e-05,
      "loss": 0.5364,
      "step": 4750
    },
    {
      "epoch": 0.8255647574036336,
      "grad_norm": 0.8295901417732239,
      "learning_rate": 3.490026019080659e-05,
      "loss": 0.4836,
      "step": 4760
    },
    {
      "epoch": 0.8272991371460782,
      "grad_norm": 0.719129204750061,
      "learning_rate": 3.455333911535126e-05,
      "loss": 0.4943,
      "step": 4770
    },
    {
      "epoch": 0.8290335168885228,
      "grad_norm": 0.7513428330421448,
      "learning_rate": 3.4206418039895924e-05,
      "loss": 0.5259,
      "step": 4780
    },
    {
      "epoch": 0.8307678966309674,
      "grad_norm": 0.7472087740898132,
      "learning_rate": 3.385949696444059e-05,
      "loss": 0.5056,
      "step": 4790
    },
    {
      "epoch": 0.832502276373412,
      "grad_norm": 0.7581727504730225,
      "learning_rate": 3.351257588898526e-05,
      "loss": 0.5032,
      "step": 4800
    },
    {
      "epoch": 0.8342366561158565,
      "grad_norm": 0.7418259978294373,
      "learning_rate": 3.316565481352992e-05,
      "loss": 0.4988,
      "step": 4810
    },
    {
      "epoch": 0.8359710358583011,
      "grad_norm": 0.84682297706604,
      "learning_rate": 3.2818733738074595e-05,
      "loss": 0.4785,
      "step": 4820
    },
    {
      "epoch": 0.8377054156007457,
      "grad_norm": 0.7958149909973145,
      "learning_rate": 3.2471812662619254e-05,
      "loss": 0.4969,
      "step": 4830
    },
    {
      "epoch": 0.8394397953431904,
      "grad_norm": 0.7733375430107117,
      "learning_rate": 3.212489158716393e-05,
      "loss": 0.5266,
      "step": 4840
    },
    {
      "epoch": 0.841174175085635,
      "grad_norm": 0.6902115345001221,
      "learning_rate": 3.1777970511708586e-05,
      "loss": 0.494,
      "step": 4850
    },
    {
      "epoch": 0.8429085548280796,
      "grad_norm": 0.6270032525062561,
      "learning_rate": 3.143104943625325e-05,
      "loss": 0.4904,
      "step": 4860
    },
    {
      "epoch": 0.8446429345705242,
      "grad_norm": 0.8523076176643372,
      "learning_rate": 3.108412836079792e-05,
      "loss": 0.5027,
      "step": 4870
    },
    {
      "epoch": 0.8463773143129688,
      "grad_norm": 0.9473335146903992,
      "learning_rate": 3.0737207285342584e-05,
      "loss": 0.5127,
      "step": 4880
    },
    {
      "epoch": 0.8481116940554134,
      "grad_norm": 0.7102193236351013,
      "learning_rate": 3.0390286209887254e-05,
      "loss": 0.474,
      "step": 4890
    },
    {
      "epoch": 0.849846073797858,
      "grad_norm": 0.8753589987754822,
      "learning_rate": 3.0043365134431917e-05,
      "loss": 0.5036,
      "step": 4900
    },
    {
      "epoch": 0.8515804535403027,
      "grad_norm": 0.6434029936790466,
      "learning_rate": 2.9696444058976586e-05,
      "loss": 0.4918,
      "step": 4910
    },
    {
      "epoch": 0.8533148332827473,
      "grad_norm": 0.6911303997039795,
      "learning_rate": 2.934952298352125e-05,
      "loss": 0.4791,
      "step": 4920
    },
    {
      "epoch": 0.8550492130251919,
      "grad_norm": 0.9562534689903259,
      "learning_rate": 2.900260190806592e-05,
      "loss": 0.4829,
      "step": 4930
    },
    {
      "epoch": 0.8567835927676365,
      "grad_norm": 0.8799000978469849,
      "learning_rate": 2.865568083261058e-05,
      "loss": 0.5175,
      "step": 4940
    },
    {
      "epoch": 0.8585179725100811,
      "grad_norm": 0.8138860464096069,
      "learning_rate": 2.830875975715525e-05,
      "loss": 0.4859,
      "step": 4950
    },
    {
      "epoch": 0.8602523522525257,
      "grad_norm": 0.666220486164093,
      "learning_rate": 2.7961838681699913e-05,
      "loss": 0.5066,
      "step": 4960
    },
    {
      "epoch": 0.8619867319949703,
      "grad_norm": 0.7144705057144165,
      "learning_rate": 2.761491760624458e-05,
      "loss": 0.4777,
      "step": 4970
    },
    {
      "epoch": 0.863721111737415,
      "grad_norm": 0.6776838302612305,
      "learning_rate": 2.726799653078925e-05,
      "loss": 0.5083,
      "step": 4980
    },
    {
      "epoch": 0.8654554914798596,
      "grad_norm": 0.6352401971817017,
      "learning_rate": 2.692107545533391e-05,
      "loss": 0.4767,
      "step": 4990
    },
    {
      "epoch": 0.8671898712223042,
      "grad_norm": 0.7197897434234619,
      "learning_rate": 2.657415437987858e-05,
      "loss": 0.4968,
      "step": 5000
    },
    {
      "epoch": 0.8689242509647487,
      "grad_norm": 1.03322172164917,
      "learning_rate": 2.6227233304423244e-05,
      "loss": 0.4955,
      "step": 5010
    },
    {
      "epoch": 0.8706586307071933,
      "grad_norm": 0.7880062460899353,
      "learning_rate": 2.5880312228967913e-05,
      "loss": 0.4831,
      "step": 5020
    },
    {
      "epoch": 0.8723930104496379,
      "grad_norm": 0.7827279567718506,
      "learning_rate": 2.5533391153512576e-05,
      "loss": 0.5092,
      "step": 5030
    },
    {
      "epoch": 0.8741273901920825,
      "grad_norm": 0.6312685608863831,
      "learning_rate": 2.5186470078057245e-05,
      "loss": 0.4923,
      "step": 5040
    },
    {
      "epoch": 0.8758617699345271,
      "grad_norm": 0.8051155805587769,
      "learning_rate": 2.4839549002601908e-05,
      "loss": 0.4857,
      "step": 5050
    },
    {
      "epoch": 0.8775961496769717,
      "grad_norm": 0.6690564155578613,
      "learning_rate": 2.4492627927146574e-05,
      "loss": 0.489,
      "step": 5060
    },
    {
      "epoch": 0.8793305294194164,
      "grad_norm": 0.7216095924377441,
      "learning_rate": 2.414570685169124e-05,
      "loss": 0.4861,
      "step": 5070
    },
    {
      "epoch": 0.881064909161861,
      "grad_norm": 0.6814486384391785,
      "learning_rate": 2.3798785776235906e-05,
      "loss": 0.494,
      "step": 5080
    },
    {
      "epoch": 0.8827992889043056,
      "grad_norm": 0.7476446032524109,
      "learning_rate": 2.3451864700780572e-05,
      "loss": 0.5172,
      "step": 5090
    },
    {
      "epoch": 0.8845336686467502,
      "grad_norm": 0.6653352379798889,
      "learning_rate": 2.310494362532524e-05,
      "loss": 0.4803,
      "step": 5100
    },
    {
      "epoch": 0.8862680483891948,
      "grad_norm": 0.777351975440979,
      "learning_rate": 2.2758022549869908e-05,
      "loss": 0.474,
      "step": 5110
    },
    {
      "epoch": 0.8880024281316394,
      "grad_norm": 0.6398897171020508,
      "learning_rate": 2.2411101474414574e-05,
      "loss": 0.4923,
      "step": 5120
    },
    {
      "epoch": 0.889736807874084,
      "grad_norm": 0.5641688108444214,
      "learning_rate": 2.206418039895924e-05,
      "loss": 0.5313,
      "step": 5130
    },
    {
      "epoch": 0.8914711876165287,
      "grad_norm": 0.9721219539642334,
      "learning_rate": 2.1717259323503906e-05,
      "loss": 0.4729,
      "step": 5140
    },
    {
      "epoch": 0.8932055673589733,
      "grad_norm": 0.7009762525558472,
      "learning_rate": 2.137033824804857e-05,
      "loss": 0.4641,
      "step": 5150
    },
    {
      "epoch": 0.8949399471014179,
      "grad_norm": 0.8539851903915405,
      "learning_rate": 2.1023417172593235e-05,
      "loss": 0.4894,
      "step": 5160
    },
    {
      "epoch": 0.8966743268438625,
      "grad_norm": 0.641577959060669,
      "learning_rate": 2.06764960971379e-05,
      "loss": 0.4719,
      "step": 5170
    },
    {
      "epoch": 0.8984087065863071,
      "grad_norm": 0.7063131928443909,
      "learning_rate": 2.0329575021682567e-05,
      "loss": 0.4989,
      "step": 5180
    },
    {
      "epoch": 0.9001430863287517,
      "grad_norm": 0.8242688775062561,
      "learning_rate": 1.9982653946227233e-05,
      "loss": 0.4866,
      "step": 5190
    },
    {
      "epoch": 0.9018774660711963,
      "grad_norm": 0.6822465062141418,
      "learning_rate": 1.96357328707719e-05,
      "loss": 0.4827,
      "step": 5200
    },
    {
      "epoch": 0.9036118458136408,
      "grad_norm": 0.6827684640884399,
      "learning_rate": 1.9288811795316565e-05,
      "loss": 0.4479,
      "step": 5210
    },
    {
      "epoch": 0.9053462255560855,
      "grad_norm": 0.6922405958175659,
      "learning_rate": 1.894189071986123e-05,
      "loss": 0.4841,
      "step": 5220
    },
    {
      "epoch": 0.9070806052985301,
      "grad_norm": 0.8128798604011536,
      "learning_rate": 1.8594969644405898e-05,
      "loss": 0.4863,
      "step": 5230
    },
    {
      "epoch": 0.9088149850409747,
      "grad_norm": 1.0421836376190186,
      "learning_rate": 1.8248048568950564e-05,
      "loss": 0.544,
      "step": 5240
    },
    {
      "epoch": 0.9105493647834193,
      "grad_norm": 0.8297311663627625,
      "learning_rate": 1.7901127493495233e-05,
      "loss": 0.5002,
      "step": 5250
    },
    {
      "epoch": 0.9122837445258639,
      "grad_norm": 0.7505292892456055,
      "learning_rate": 1.75542064180399e-05,
      "loss": 0.499,
      "step": 5260
    },
    {
      "epoch": 0.9140181242683085,
      "grad_norm": 0.6893332004547119,
      "learning_rate": 1.7207285342584562e-05,
      "loss": 0.4903,
      "step": 5270
    },
    {
      "epoch": 0.9157525040107531,
      "grad_norm": 0.7143068909645081,
      "learning_rate": 1.6860364267129228e-05,
      "loss": 0.4898,
      "step": 5280
    },
    {
      "epoch": 0.9174868837531978,
      "grad_norm": 0.8790411949157715,
      "learning_rate": 1.6513443191673894e-05,
      "loss": 0.5198,
      "step": 5290
    },
    {
      "epoch": 0.9192212634956424,
      "grad_norm": 0.7653092741966248,
      "learning_rate": 1.616652211621856e-05,
      "loss": 0.4688,
      "step": 5300
    },
    {
      "epoch": 0.920955643238087,
      "grad_norm": 0.7367482781410217,
      "learning_rate": 1.5819601040763226e-05,
      "loss": 0.5433,
      "step": 5310
    },
    {
      "epoch": 0.9226900229805316,
      "grad_norm": 0.74312424659729,
      "learning_rate": 1.5472679965307892e-05,
      "loss": 0.5176,
      "step": 5320
    },
    {
      "epoch": 0.9244244027229762,
      "grad_norm": 0.7547156810760498,
      "learning_rate": 1.5125758889852558e-05,
      "loss": 0.5007,
      "step": 5330
    },
    {
      "epoch": 0.9261587824654208,
      "grad_norm": 0.5444682240486145,
      "learning_rate": 1.4778837814397226e-05,
      "loss": 0.5347,
      "step": 5340
    },
    {
      "epoch": 0.9278931622078654,
      "grad_norm": 0.7637814879417419,
      "learning_rate": 1.4431916738941892e-05,
      "loss": 0.4693,
      "step": 5350
    },
    {
      "epoch": 0.92962754195031,
      "grad_norm": 0.7325692176818848,
      "learning_rate": 1.4084995663486558e-05,
      "loss": 0.5195,
      "step": 5360
    },
    {
      "epoch": 0.9313619216927547,
      "grad_norm": 0.6730213165283203,
      "learning_rate": 1.3738074588031225e-05,
      "loss": 0.5259,
      "step": 5370
    },
    {
      "epoch": 0.9330963014351993,
      "grad_norm": 0.7768345475196838,
      "learning_rate": 1.339115351257589e-05,
      "loss": 0.487,
      "step": 5380
    },
    {
      "epoch": 0.9348306811776439,
      "grad_norm": 0.7284306883811951,
      "learning_rate": 1.3044232437120555e-05,
      "loss": 0.4886,
      "step": 5390
    },
    {
      "epoch": 0.9365650609200884,
      "grad_norm": 0.5617222785949707,
      "learning_rate": 1.2697311361665221e-05,
      "loss": 0.4371,
      "step": 5400
    },
    {
      "epoch": 0.938299440662533,
      "grad_norm": 0.6211934685707092,
      "learning_rate": 1.2350390286209889e-05,
      "loss": 0.4711,
      "step": 5410
    },
    {
      "epoch": 0.9400338204049776,
      "grad_norm": 0.8508502244949341,
      "learning_rate": 1.2003469210754555e-05,
      "loss": 0.5058,
      "step": 5420
    },
    {
      "epoch": 0.9417682001474222,
      "grad_norm": 1.0277165174484253,
      "learning_rate": 1.165654813529922e-05,
      "loss": 0.4605,
      "step": 5430
    },
    {
      "epoch": 0.9435025798898669,
      "grad_norm": 0.6579120755195618,
      "learning_rate": 1.1309627059843885e-05,
      "loss": 0.4956,
      "step": 5440
    },
    {
      "epoch": 0.9452369596323115,
      "grad_norm": 0.833388090133667,
      "learning_rate": 1.0962705984388552e-05,
      "loss": 0.4729,
      "step": 5450
    },
    {
      "epoch": 0.9469713393747561,
      "grad_norm": 0.8685356378555298,
      "learning_rate": 1.0615784908933218e-05,
      "loss": 0.5347,
      "step": 5460
    },
    {
      "epoch": 0.9487057191172007,
      "grad_norm": 0.8333202600479126,
      "learning_rate": 1.0268863833477885e-05,
      "loss": 0.506,
      "step": 5470
    },
    {
      "epoch": 0.9504400988596453,
      "grad_norm": 0.7044509053230286,
      "learning_rate": 9.921942758022552e-06,
      "loss": 0.4742,
      "step": 5480
    },
    {
      "epoch": 0.9521744786020899,
      "grad_norm": 0.7053804397583008,
      "learning_rate": 9.575021682567216e-06,
      "loss": 0.4881,
      "step": 5490
    },
    {
      "epoch": 0.9539088583445345,
      "grad_norm": 0.8313674330711365,
      "learning_rate": 9.228100607111882e-06,
      "loss": 0.5,
      "step": 5500
    }
  ],
  "logging_steps": 10,
  "max_steps": 5765,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.4315847426048e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
